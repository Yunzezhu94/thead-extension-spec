[#intrinsics]
=== Intrinsic support for XTheadVector

==== Introduction
The RISC-V `XTheadVector` C intrinsics provide users interfaces in the C language level to directly leverage XTheadVector (<<#xtheadvector>>), with assistance from the compiler in handling instruction scheduling and register allocation.

The `XTheadVector` intrinsics support all RVV intrinsic functions with the restriction of missing support for fractional LMUL values (https://github.com/riscv-non-isa/rvv-intrinsic-doc). Additionally, the `XTheadVector` extension intrinsics API extends the RVV intrinsics API by new load/store functions and integer extract functions (<<#appendix>>).

==== Test macro
The `__riscv_th_v_intrinsic` macro is the C macro to test the compiler's support for the RISC-V `XTheadVector` extension intrinsics. The API extension is optional and targeting the vendor extension `XTheadVector`.

The value of the test macro is defined as its version, which is computed using the following formula. The formula is identical to what is defined in the RISC-V C API specification (https://github.com/riscv-non-isa/rvv-intrinsic-doc) .

----
<MAJOR_VERSION> * 1,000,000 + <MINOR_VERSION> * 1,000 + <REVISION_VERSION>
----

For example, the v1.0 version should define the macro with value `1000000`.

==== Availability

With `<riscv_th_vector.h>` included, availability of intrinsic variants depends on the required architecture of their corresponding xtheadvector instructions. The supported architecture is specified to the compiler using the `-march` option.

We suggest guarding the inclusion with the test macro.

[,c]
----
#if defined(__riscv_xtheadvector) && defined(__riscv_th_v_intrinsic)
#include <riscv_th_vector.h>
#elif defined (__riscv_vector) && defined (__riscv_v_intrinsic)
#include <riscv_vector.h>
#else
#error This file requires either RVV intrinsics or XTheadVector intrinsics
#endif
----

==== Example

The following shows an example with `vlb` intrinsic of `vint32m1_t` output vector type, `vsb` intrinsic of `vint32m1_t` input vector type as well as `vadd` intrinsic of `vint32m1_t` input/output vector type.

[,c]
----
void function (void *in, void *out)
{
    vint32m1_t v = __riscv_th_vlb_v_i32m1 (in, 4);
    vint32m1_t v2 = __riscv_th_vlb_v_i32m1_tu (v, in, 4);
    vint32m1_t v3 = __riscv_vadd_vv_i32m1 (v2, v2, 4);
    vint32m1_t v4 = __riscv_vadd_vv_i32m1_tu (v3, v2, v2, 4);
    __riscv_th_vsb_v_i32m1 (out, v4, 4);
}
----

[#appendix]
==== Additional Intrinsic Functions for XTheadVector

[[xtheadvector-unit-stride-load]]
===== XtheadVector Unit-Stride Load Intrinsics

[,c]
----
vint8m1_t __riscv_th_vlb_v_i8m1 (const int8_t *a, size_t vl);
vint8m2_t __riscv_th_vlb_v_i8m2 (const int8_t *a, size_t vl);
vint8m4_t __riscv_th_vlb_v_i8m4 (const int8_t *a, size_t vl);
vint8m8_t __riscv_th_vlb_v_i8m8 (const int8_t *a, size_t vl);
vint16m1_t __riscv_th_vlb_v_i16m1 (const int16_t *a, size_t vl);
vint16m2_t __riscv_th_vlb_v_i16m2 (const int16_t *a, size_t vl);
vint16m4_t __riscv_th_vlb_v_i16m4 (const int16_t *a, size_t vl);
vint16m8_t __riscv_th_vlb_v_i16m8 (const int16_t *a, size_t vl);
vint32m1_t __riscv_th_vlb_v_i32m1 (const int32_t *a, size_t vl);
vint32m2_t __riscv_th_vlb_v_i32m2 (const int32_t *a, size_t vl);
vint32m4_t __riscv_th_vlb_v_i32m4 (const int32_t *a, size_t vl);
vint32m8_t __riscv_th_vlb_v_i32m8 (const int32_t *a, size_t vl);
vint64m1_t __riscv_th_vlb_v_i64m1 (const int64_t *a, size_t vl);
vint64m2_t __riscv_th_vlb_v_i64m2 (const int64_t *a, size_t vl);
vint64m4_t __riscv_th_vlb_v_i64m4 (const int64_t *a, size_t vl);
vint64m8_t __riscv_th_vlb_v_i64m8 (const int64_t *a, size_t vl);
vint16m1_t __riscv_th_vlh_v_i16m1 (const int16_t *a, size_t vl);
vint16m2_t __riscv_th_vlh_v_i16m2 (const int16_t *a, size_t vl);
vint16m4_t __riscv_th_vlh_v_i16m4 (const int16_t *a, size_t vl);
vint16m8_t __riscv_th_vlh_v_i16m8 (const int16_t *a, size_t vl);
vint32m1_t __riscv_th_vlh_v_i32m1 (const int32_t *a, size_t vl);
vint32m2_t __riscv_th_vlh_v_i32m2 (const int32_t *a, size_t vl);
vint32m4_t __riscv_th_vlh_v_i32m4 (const int32_t *a, size_t vl);
vint32m8_t __riscv_th_vlh_v_i32m8 (const int32_t *a, size_t vl);
vint64m1_t __riscv_th_vlh_v_i64m1 (const int64_t *a, size_t vl);
vint64m2_t __riscv_th_vlh_v_i64m2 (const int64_t *a, size_t vl);
vint64m4_t __riscv_th_vlh_v_i64m4 (const int64_t *a, size_t vl);
vint64m8_t __riscv_th_vlh_v_i64m8 (const int64_t *a, size_t vl);
vint32m1_t __riscv_th_vlw_v_i32m1 (const int32_t *a, size_t vl);
vint32m2_t __riscv_th_vlw_v_i32m2 (const int32_t *a, size_t vl);
vint32m4_t __riscv_th_vlw_v_i32m4 (const int32_t *a, size_t vl);
vint32m8_t __riscv_th_vlw_v_i32m8 (const int32_t *a, size_t vl);
vint64m1_t __riscv_th_vlw_v_i64m1 (const int64_t *a, size_t vl);
vint64m2_t __riscv_th_vlw_v_i64m2 (const int64_t *a, size_t vl);
vint64m4_t __riscv_th_vlw_v_i64m4 (const int64_t *a, size_t vl);
vint64m8_t __riscv_th_vlw_v_i64m8 (const int64_t *a, size_t vl);
vuint8m1_t __riscv_th_vlbu_v_u8m1 (const uint8_t *a, size_t vl);
vuint8m2_t __riscv_th_vlbu_v_u8m2 (const uint8_t *a, size_t vl);
vuint8m4_t __riscv_th_vlbu_v_u8m4 (const uint8_t *a, size_t vl);
vuint8m8_t __riscv_th_vlbu_v_u8m8 (const uint8_t *a, size_t vl);
vuint16m1_t __riscv_th_vlbu_v_u16m1 (const uint16_t *a, size_t vl);
vuint16m2_t __riscv_th_vlbu_v_u16m2 (const uint16_t *a, size_t vl);
vuint16m4_t __riscv_th_vlbu_v_u16m4 (const uint16_t *a, size_t vl);
vuint16m8_t __riscv_th_vlbu_v_u16m8 (const uint16_t *a, size_t vl);
vuint32m1_t __riscv_th_vlbu_v_u32m1 (const uint32_t *a, size_t vl);
vuint32m2_t __riscv_th_vlbu_v_u32m2 (const uint32_t *a, size_t vl);
vuint32m4_t __riscv_th_vlbu_v_u32m4 (const uint32_t *a, size_t vl);
vuint32m8_t __riscv_th_vlbu_v_u32m8 (const uint32_t *a, size_t vl);
vuint64m1_t __riscv_th_vlbu_v_u64m1 (const uint64_t *a, size_t vl);
vuint64m2_t __riscv_th_vlbu_v_u64m2 (const uint64_t *a, size_t vl);
vuint64m4_t __riscv_th_vlbu_v_u64m4 (const uint64_t *a, size_t vl);
vuint64m8_t __riscv_th_vlbu_v_u64m8 (const uint64_t *a, size_t vl);
vuint16m1_t __riscv_th_vlhu_v_u16m1 (const uint16_t *a, size_t vl);
vuint16m2_t __riscv_th_vlhu_v_u16m2 (const uint16_t *a, size_t vl);
vuint16m4_t __riscv_th_vlhu_v_u16m4 (const uint16_t *a, size_t vl);
vuint16m8_t __riscv_th_vlhu_v_u16m8 (const uint16_t *a, size_t vl);
vuint32m1_t __riscv_th_vlhu_v_u32m1 (const uint32_t *a, size_t vl);
vuint32m2_t __riscv_th_vlhu_v_u32m2 (const uint32_t *a, size_t vl);
vuint32m4_t __riscv_th_vlhu_v_u32m4 (const uint32_t *a, size_t vl);
vuint32m8_t __riscv_th_vlhu_v_u32m8 (const uint32_t *a, size_t vl);
vuint64m1_t __riscv_th_vlhu_v_u64m1 (const uint64_t *a, size_t vl);
vuint64m2_t __riscv_th_vlhu_v_u64m2 (const uint64_t *a, size_t vl);
vuint64m4_t __riscv_th_vlhu_v_u64m4 (const uint64_t *a, size_t vl);
vuint64m8_t __riscv_th_vlhu_v_u64m8 (const uint64_t *a, size_t vl);
vuint32m1_t __riscv_th_vlwu_v_u32m1 (const uint32_t *a, size_t vl);
vuint32m2_t __riscv_th_vlwu_v_u32m2 (const uint32_t *a, size_t vl);
vuint32m4_t __riscv_th_vlwu_v_u32m4 (const uint32_t *a, size_t vl);
vuint32m8_t __riscv_th_vlwu_v_u32m8 (const uint32_t *a, size_t vl);
vuint64m1_t __riscv_th_vlwu_v_u64m1 (const uint64_t *a, size_t vl);
vuint64m2_t __riscv_th_vlwu_v_u64m2 (const uint64_t *a, size_t vl);
vuint64m4_t __riscv_th_vlwu_v_u64m4 (const uint64_t *a, size_t vl);
vuint64m8_t __riscv_th_vlwu_v_u64m8 (const uint64_t *a, size_t vl);
// masked functions
vint8m1_t __riscv_th_vlb_v_i8m1_m (vbool8_t mask, const int8_t *a, size_t vl);
vint8m2_t __riscv_th_vlb_v_i8m2_m (vbool4_t mask, const int8_t *a, size_t vl);
vint8m4_t __riscv_th_vlb_v_i8m4_m (vbool2_t mask, const int8_t *a, size_t vl);
vint8m8_t __riscv_th_vlb_v_i8m8_m (vbool1_t mask, const int8_t *a, size_t vl);
vint16m1_t __riscv_th_vlb_v_i16m1_m (vbool16_t mask, const int16_t *a, size_t vl);
vint16m2_t __riscv_th_vlb_v_i16m2_m (vbool8_t mask, const int16_t *a, size_t vl);
vint16m4_t __riscv_th_vlb_v_i16m4_m (vbool4_t mask, const int16_t *a, size_t vl);
vint16m8_t __riscv_th_vlb_v_i16m8_m (vbool2_t mask, const int16_t *a, size_t vl);
vint32m1_t __riscv_th_vlb_v_i32m1_m (vbool32_t mask, const int32_t *a, size_t vl);
vint32m2_t __riscv_th_vlb_v_i32m2_m (vbool16_t mask, const int32_t *a, size_t vl);
vint32m4_t __riscv_th_vlb_v_i32m4_m (vbool8_t mask, const int32_t *a, size_t vl);
vint32m8_t __riscv_th_vlb_v_i32m8_m (vbool4_t mask, const int32_t *a, size_t vl);
vint64m1_t __riscv_th_vlb_v_i64m1_m (vbool64_t mask, const int64_t *a, size_t vl);
vint64m2_t __riscv_th_vlb_v_i64m2_m (vbool32_t mask, const int64_t *a, size_t vl);
vint64m4_t __riscv_th_vlb_v_i64m4_m (vbool16_t mask, const int64_t *a, size_t vl);
vint64m8_t __riscv_th_vlb_v_i64m8_m (vbool8_t mask, const int64_t *a, size_t vl);
vint16m1_t __riscv_th_vlh_v_i16m1_m (vbool16_t mask, const int16_t *a, size_t vl);
vint16m2_t __riscv_th_vlh_v_i16m2_m (vbool8_t mask, const int16_t *a, size_t vl);
vint16m4_t __riscv_th_vlh_v_i16m4_m (vbool4_t mask, const int16_t *a, size_t vl);
vint16m8_t __riscv_th_vlh_v_i16m8_m (vbool2_t mask, const int16_t *a, size_t vl);
vint32m1_t __riscv_th_vlh_v_i32m1_m (vbool32_t mask, const int32_t *a, size_t vl);
vint32m2_t __riscv_th_vlh_v_i32m2_m (vbool16_t mask, const int32_t *a, size_t vl);
vint32m4_t __riscv_th_vlh_v_i32m4_m (vbool8_t mask, const int32_t *a, size_t vl);
vint32m8_t __riscv_th_vlh_v_i32m8_m (vbool4_t mask, const int32_t *a, size_t vl);
vint64m1_t __riscv_th_vlh_v_i64m1_m (vbool64_t mask, const int64_t *a, size_t vl);
vint64m2_t __riscv_th_vlh_v_i64m2_m (vbool32_t mask, const int64_t *a, size_t vl);
vint64m4_t __riscv_th_vlh_v_i64m4_m (vbool16_t mask, const int64_t *a, size_t vl);
vint64m8_t __riscv_th_vlh_v_i64m8_m (vbool8_t mask, const int64_t *a, size_t vl);
vint32m1_t __riscv_th_vlw_v_i32m1_m (vbool32_t mask, const int32_t *a, size_t vl);
vint32m2_t __riscv_th_vlw_v_i32m2_m (vbool16_t mask, const int32_t *a, size_t vl);
vint32m4_t __riscv_th_vlw_v_i32m4_m (vbool8_t mask, const int32_t *a, size_t vl);
vint32m8_t __riscv_th_vlw_v_i32m8_m (vbool4_t mask, const int32_t *a, size_t vl);
vint64m1_t __riscv_th_vlw_v_i64m1_m (vbool64_t mask, const int64_t *a, size_t vl);
vint64m2_t __riscv_th_vlw_v_i64m2_m (vbool32_t mask, const int64_t *a, size_t vl);
vint64m4_t __riscv_th_vlw_v_i64m4_m (vbool16_t mask, const int64_t *a, size_t vl);
vint64m8_t __riscv_th_vlw_v_i64m8_m (vbool8_t mask, const int64_t *a, size_t vl);
vuint8m1_t __riscv_th_vlbu_v_u8m1_m (vbool8_t mask, const uint8_t *a, size_t vl);
vuint8m2_t __riscv_th_vlbu_v_u8m2_m (vbool4_t mask, const uint8_t *a, size_t vl);
vuint8m4_t __riscv_th_vlbu_v_u8m4_m (vbool2_t mask, const uint8_t *a, size_t vl);
vuint8m8_t __riscv_th_vlbu_v_u8m8_m (vbool1_t mask, const uint8_t *a, size_t vl);
vuint16m1_t __riscv_th_vlbu_v_u16m1_m (vbool16_t mask, const uint16_t *a, size_t vl);
vuint16m2_t __riscv_th_vlbu_v_u16m2_m (vbool8_t mask, const uint16_t *a, size_t vl);
vuint16m4_t __riscv_th_vlbu_v_u16m4_m (vbool4_t mask, const uint16_t *a, size_t vl);
vuint16m8_t __riscv_th_vlbu_v_u16m8_m (vbool2_t mask, const uint16_t *a, size_t vl);
vuint32m1_t __riscv_th_vlbu_v_u32m1_m (vbool32_t mask, const uint32_t *a, size_t vl);
vuint32m2_t __riscv_th_vlbu_v_u32m2_m (vbool16_t mask, const uint32_t *a, size_t vl);
vuint32m4_t __riscv_th_vlbu_v_u32m4_m (vbool8_t mask, const uint32_t *a, size_t vl);
vuint32m8_t __riscv_th_vlbu_v_u32m8_m (vbool4_t mask, const uint32_t *a, size_t vl);
vuint64m1_t __riscv_th_vlbu_v_u64m1_m (vbool64_t mask, const uint64_t *a, size_t vl);
vuint64m2_t __riscv_th_vlbu_v_u64m2_m (vbool32_t mask, const uint64_t *a, size_t vl);
vuint64m4_t __riscv_th_vlbu_v_u64m4_m (vbool16_t mask, const uint64_t *a, size_t vl);
vuint64m8_t __riscv_th_vlbu_v_u64m8_m (vbool8_t mask, const uint64_t *a, size_t vl);
vuint16m1_t __riscv_th_vlhu_v_u16m1_m (vbool16_t mask, const uint16_t *a, size_t vl);
vuint16m2_t __riscv_th_vlhu_v_u16m2_m (vbool8_t mask, const uint16_t *a, size_t vl);
vuint16m4_t __riscv_th_vlhu_v_u16m4_m (vbool4_t mask, const uint16_t *a, size_t vl);
vuint16m8_t __riscv_th_vlhu_v_u16m8_m (vbool2_t mask, const uint16_t *a, size_t vl);
vuint32m1_t __riscv_th_vlhu_v_u32m1_m (vbool32_t mask, const uint32_t *a, size_t vl);
vuint32m2_t __riscv_th_vlhu_v_u32m2_m (vbool16_t mask, const uint32_t *a, size_t vl);
vuint32m4_t __riscv_th_vlhu_v_u32m4_m (vbool8_t mask, const uint32_t *a, size_t vl);
vuint32m8_t __riscv_th_vlhu_v_u32m8_m (vbool4_t mask, const uint32_t *a, size_t vl);
vuint64m1_t __riscv_th_vlhu_v_u64m1_m (vbool64_t mask, const uint64_t *a, size_t vl);
vuint64m2_t __riscv_th_vlhu_v_u64m2_m (vbool32_t mask, const uint64_t *a, size_t vl);
vuint64m4_t __riscv_th_vlhu_v_u64m4_m (vbool16_t mask, const uint64_t *a, size_t vl);
vuint64m8_t __riscv_th_vlhu_v_u64m8_m (vbool8_t mask, const uint64_t *a, size_t vl);
vuint32m1_t __riscv_th_vlwu_v_u32m1_m (vbool32_t mask, const uint32_t *a, size_t vl);
vuint32m2_t __riscv_th_vlwu_v_u32m2_m (vbool16_t mask, const uint32_t *a, size_t vl);
vuint32m4_t __riscv_th_vlwu_v_u32m4_m (vbool8_t mask, const uint32_t *a, size_t vl);
vuint32m8_t __riscv_th_vlwu_v_u32m8_m (vbool4_t mask, const uint32_t *a, size_t vl);
vuint64m1_t __riscv_th_vlwu_v_u64m1_m (vbool64_t mask, const uint64_t *a, size_t vl);
vuint64m2_t __riscv_th_vlwu_v_u64m2_m (vbool32_t mask, const uint64_t *a, size_t vl);
vuint64m4_t __riscv_th_vlwu_v_u64m4_m (vbool16_t mask, const uint64_t *a, size_t vl);
vuint64m8_t __riscv_th_vlwu_v_u64m8_m (vbool8_t mask, const uint64_t *a, size_t vl);

----

[[xtheadvector-unit-stride-store]]
===== XTheadVector Unit-Stride Store Intrinsics

[,c]
----
void __riscv_th_vsb_v_i8m1 (int8_t *a, vint8m1_t b, size_t vl);
void __riscv_th_vsb_v_i8m2 (int8_t *a, vint8m2_t b, size_t vl);
void __riscv_th_vsb_v_i8m4 (int8_t *a, vint8m4_t b, size_t vl);
void __riscv_th_vsb_v_i8m8 (int8_t *a, vint8m8_t b, size_t vl);
void __riscv_th_vsb_v_i16m1 (int16_t *a, vint16m1_t b, size_t vl);
void __riscv_th_vsb_v_i16m2 (int16_t *a, vint16m2_t b, size_t vl);
void __riscv_th_vsb_v_i16m4 (int16_t *a, vint16m4_t b, size_t vl);
void __riscv_th_vsb_v_i16m8 (int16_t *a, vint16m8_t b, size_t vl);
void __riscv_th_vsb_v_i32m1 (int32_t *a, vint32m1_t b, size_t vl);
void __riscv_th_vsb_v_i32m2 (int32_t *a, vint32m2_t b, size_t vl);
void __riscv_th_vsb_v_i32m4 (int32_t *a, vint32m4_t b, size_t vl);
void __riscv_th_vsb_v_i32m8 (int32_t *a, vint32m8_t b, size_t vl);
void __riscv_th_vsb_v_u8m1 (uint8_t *a, vuint8m1_t b, size_t vl);
void __riscv_th_vsb_v_u8m2 (uint8_t *a, vuint8m2_t b, size_t vl);
void __riscv_th_vsb_v_u8m4 (uint8_t *a, vuint8m4_t b, size_t vl);
void __riscv_th_vsb_v_u8m8 (uint8_t *a, vuint8m8_t b, size_t vl);
void __riscv_th_vsb_v_u16m1 (uint16_t *a, vuint16m1_t b, size_t vl);
void __riscv_th_vsb_v_u16m2 (uint16_t *a, vuint16m2_t b, size_t vl);
void __riscv_th_vsb_v_u16m4 (uint16_t *a, vuint16m4_t b, size_t vl);
void __riscv_th_vsb_v_u16m8 (uint16_t *a, vuint16m8_t b, size_t vl);
void __riscv_th_vsb_v_u32m1 (uint32_t *a, vuint32m1_t b, size_t vl);
void __riscv_th_vsb_v_u32m2 (uint32_t *a, vuint32m2_t b, size_t vl);
void __riscv_th_vsb_v_u32m4 (uint32_t *a, vuint32m4_t b, size_t vl);
void __riscv_th_vsb_v_u32m8 (uint32_t *a, vuint32m8_t b, size_t vl);
void __riscv_th_vsh_v_i16m1 (int16_t *a, vint16m1_t b, size_t vl);
void __riscv_th_vsh_v_i16m2 (int16_t *a, vint16m2_t b, size_t vl);
void __riscv_th_vsh_v_i16m4 (int16_t *a, vint16m4_t b, size_t vl);
void __riscv_th_vsh_v_i16m8 (int16_t *a, vint16m8_t b, size_t vl);
void __riscv_th_vsh_v_i32m1 (int32_t *a, vint32m1_t b, size_t vl);
void __riscv_th_vsh_v_i32m2 (int32_t *a, vint32m2_t b, size_t vl);
void __riscv_th_vsh_v_i32m4 (int32_t *a, vint32m4_t b, size_t vl);
void __riscv_th_vsh_v_i32m8 (int32_t *a, vint32m8_t b, size_t vl);
void __riscv_th_vsh_v_u16m1 (uint16_t *a, vuint16m1_t b, size_t vl);
void __riscv_th_vsh_v_u16m2 (uint16_t *a, vuint16m2_t b, size_t vl);
void __riscv_th_vsh_v_u16m4 (uint16_t *a, vuint16m4_t b, size_t vl);
void __riscv_th_vsh_v_u16m8 (uint16_t *a, vuint16m8_t b, size_t vl);
void __riscv_th_vsh_v_u32m1 (uint32_t *a, vuint32m1_t b, size_t vl);
void __riscv_th_vsh_v_u32m2 (uint32_t *a, vuint32m2_t b, size_t vl);
void __riscv_th_vsh_v_u32m4 (uint32_t *a, vuint32m4_t b, size_t vl);
void __riscv_th_vsh_v_u32m8 (uint32_t *a, vuint32m8_t b, size_t vl);
void __riscv_th_vsw_v_i32m1 (int32_t *a, vint32m1_t b, size_t vl);
void __riscv_th_vsw_v_i32m2 (int32_t *a, vint32m2_t b, size_t vl);
void __riscv_th_vsw_v_i32m4 (int32_t *a, vint32m4_t b, size_t vl);
void __riscv_th_vsw_v_i32m8 (int32_t *a, vint32m8_t b, size_t vl);
void __riscv_th_vsw_v_u32m1 (uint32_t *a, vuint32m1_t b, size_t vl);
void __riscv_th_vsw_v_u32m2 (uint32_t *a, vuint32m2_t b, size_t vl);
void __riscv_th_vsw_v_u32m4 (uint32_t *a, vuint32m4_t b, size_t vl);
void __riscv_th_vsw_v_u32m8 (uint32_t *a, vuint32m8_t b, size_t vl);
// masked functions
void __riscv_th_vsb_v_i8m1_m (vbool8_t mask, int8_t *a, vint8m1_t b, size_t vl);
void __riscv_th_vsb_v_i8m2_m (vbool4_t mask, int8_t *a, vint8m2_t b, size_t vl);
void __riscv_th_vsb_v_i8m4_m (vbool2_t mask, int8_t *a, vint8m4_t b, size_t vl);
void __riscv_th_vsb_v_i8m8_m (vbool1_t mask, int8_t *a, vint8m8_t b, size_t vl);
void __riscv_th_vsb_v_i16m1_m (vbool16_t mask, int16_t *a, vint16m1_t b, size_t vl);
void __riscv_th_vsb_v_i16m2_m (vbool8_t mask, int16_t *a, vint16m2_t b, size_t vl);
void __riscv_th_vsb_v_i16m4_m (vbool4_t mask, int16_t *a, vint16m4_t b, size_t vl);
void __riscv_th_vsb_v_i16m8_m (vbool2_t mask, int16_t *a, vint16m8_t b, size_t vl);
void __riscv_th_vsb_v_i32m1_m (vbool32_t mask, int32_t *a, vint32m1_t b, size_t vl);
void __riscv_th_vsb_v_i32m2_m (vbool16_t mask, int32_t *a, vint32m2_t b, size_t vl);
void __riscv_th_vsb_v_i32m4_m (vbool8_t mask, int32_t *a, vint32m4_t b, size_t vl);
void __riscv_th_vsb_v_i32m8_m (vbool4_t mask, int32_t *a, vint32m8_t b, size_t vl);
void __riscv_th_vsb_v_u8m1_m (vbool8_t mask, uint8_t *a, vuint8m1_t b, size_t vl);
void __riscv_th_vsb_v_u8m2_m (vbool4_t mask, uint8_t *a, vuint8m2_t b, size_t vl);
void __riscv_th_vsb_v_u8m4_m (vbool2_t mask, uint8_t *a, vuint8m4_t b, size_t vl);
void __riscv_th_vsb_v_u8m8_m (vbool1_t mask, uint8_t *a, vuint8m8_t b, size_t vl);
void __riscv_th_vsb_v_u16m1_m (vbool16_t mask, uint16_t *a, vuint16m1_t b, size_t vl);
void __riscv_th_vsb_v_u16m2_m (vbool8_t mask, uint16_t *a, vuint16m2_t b, size_t vl);
void __riscv_th_vsb_v_u16m4_m (vbool4_t mask, uint16_t *a, vuint16m4_t b, size_t vl);
void __riscv_th_vsb_v_u16m8_m (vbool2_t mask, uint16_t *a, vuint16m8_t b, size_t vl);
void __riscv_th_vsb_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t b, size_t vl);
void __riscv_th_vsb_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t b, size_t vl);
void __riscv_th_vsb_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t b, size_t vl);
void __riscv_th_vsb_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t b, size_t vl);
void __riscv_th_vsh_v_i16m1_m (vbool16_t mask, int16_t *a, vint16m1_t b, size_t vl);
void __riscv_th_vsh_v_i16m2_m (vbool8_t mask, int16_t *a, vint16m2_t b, size_t vl);
void __riscv_th_vsh_v_i16m4_m (vbool4_t mask, int16_t *a, vint16m4_t b, size_t vl);
void __riscv_th_vsh_v_i16m8_m (vbool2_t mask, int16_t *a, vint16m8_t b, size_t vl);
void __riscv_th_vsh_v_i32m1_m (vbool32_t mask, int32_t *a, vint32m1_t b, size_t vl);
void __riscv_th_vsh_v_i32m2_m (vbool16_t mask, int32_t *a, vint32m2_t b, size_t vl);
void __riscv_th_vsh_v_i32m4_m (vbool8_t mask, int32_t *a, vint32m4_t b, size_t vl);
void __riscv_th_vsh_v_i32m8_m (vbool4_t mask, int32_t *a, vint32m8_t b, size_t vl);
void __riscv_th_vsh_v_u16m1_m (vbool16_t mask, uint16_t *a, vuint16m1_t b, size_t vl);
void __riscv_th_vsh_v_u16m2_m (vbool8_t mask, uint16_t *a, vuint16m2_t b, size_t vl);
void __riscv_th_vsh_v_u16m4_m (vbool4_t mask, uint16_t *a, vuint16m4_t b, size_t vl);
void __riscv_th_vsh_v_u16m8_m (vbool2_t mask, uint16_t *a, vuint16m8_t b, size_t vl);
void __riscv_th_vsh_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t b, size_t vl);
void __riscv_th_vsh_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t b, size_t vl);
void __riscv_th_vsh_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t b, size_t vl);
void __riscv_th_vsh_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t b, size_t vl);
void __riscv_th_vsw_v_i32m1_m (vbool32_t mask, int32_t *a, vint32m1_t b, size_t vl);
void __riscv_th_vsw_v_i32m2_m (vbool16_t mask, int32_t *a, vint32m2_t b, size_t vl);
void __riscv_th_vsw_v_i32m4_m (vbool8_t mask, int32_t *a, vint32m4_t b, size_t vl);
void __riscv_th_vsw_v_i32m8_m (vbool4_t mask, int32_t *a, vint32m8_t b, size_t vl);
void __riscv_th_vsw_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t b, size_t vl);
void __riscv_th_vsw_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t b, size_t vl);
void __riscv_th_vsw_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t b, size_t vl);
void __riscv_th_vsw_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t b, size_t vl);
----

[[xtheadvectorvector-strided-load]]
===== XTheadVector Strided Load Intrinsics

[,c]
----
vint8m1_t __riscv_th_vlsb_v_i8m1 (const int8_t *a, size_t stride, size_t vl);
vint8m2_t __riscv_th_vlsb_v_i8m2 (const int8_t *a, size_t stride, size_t vl);
vint8m4_t __riscv_th_vlsb_v_i8m4 (const int8_t *a, size_t stride, size_t vl);
vint8m8_t __riscv_th_vlsb_v_i8m8 (const int8_t *a, size_t stride, size_t vl);
vint16m1_t __riscv_th_vlsb_v_i16m1 (const int16_t *a, size_t stride, size_t vl);
vint16m2_t __riscv_th_vlsb_v_i16m2 (const int16_t *a, size_t stride, size_t vl);
vint16m4_t __riscv_th_vlsb_v_i16m4 (const int16_t *a, size_t stride, size_t vl);
vint16m8_t __riscv_th_vlsb_v_i16m8 (const int16_t *a, size_t stride, size_t vl);
vint32m1_t __riscv_th_vlsb_v_i32m1 (const int32_t *a, size_t stride, size_t vl);
vint32m2_t __riscv_th_vlsb_v_i32m2 (const int32_t *a, size_t stride, size_t vl);
vint32m4_t __riscv_th_vlsb_v_i32m4 (const int32_t *a, size_t stride, size_t vl);
vint32m8_t __riscv_th_vlsb_v_i32m8 (const int32_t *a, size_t stride, size_t vl);
vint64m1_t __riscv_th_vlsb_v_i64m1 (const int64_t *a, size_t stride, size_t vl);
vint64m2_t __riscv_th_vlsb_v_i64m2 (const int64_t *a, size_t stride, size_t vl);
vint64m4_t __riscv_th_vlsb_v_i64m4 (const int64_t *a, size_t stride, size_t vl);
vint64m8_t __riscv_th_vlsb_v_i64m8 (const int64_t *a, size_t stride, size_t vl);
vint16m1_t __riscv_th_vlsh_v_i16m1 (const int16_t *a, size_t stride, size_t vl);
vint16m2_t __riscv_th_vlsh_v_i16m2 (const int16_t *a, size_t stride, size_t vl);
vint16m4_t __riscv_th_vlsh_v_i16m4 (const int16_t *a, size_t stride, size_t vl);
vint16m8_t __riscv_th_vlsh_v_i16m8 (const int16_t *a, size_t stride, size_t vl);
vint32m1_t __riscv_th_vlsh_v_i32m1 (const int32_t *a, size_t stride, size_t vl);
vint32m2_t __riscv_th_vlsh_v_i32m2 (const int32_t *a, size_t stride, size_t vl);
vint32m4_t __riscv_th_vlsh_v_i32m4 (const int32_t *a, size_t stride, size_t vl);
vint32m8_t __riscv_th_vlsh_v_i32m8 (const int32_t *a, size_t stride, size_t vl);
vint64m1_t __riscv_th_vlsh_v_i64m1 (const int64_t *a, size_t stride, size_t vl);
vint64m2_t __riscv_th_vlsh_v_i64m2 (const int64_t *a, size_t stride, size_t vl);
vint64m4_t __riscv_th_vlsh_v_i64m4 (const int64_t *a, size_t stride, size_t vl);
vint64m8_t __riscv_th_vlsh_v_i64m8 (const int64_t *a, size_t stride, size_t vl);
vint32m1_t __riscv_th_vlsw_v_i32m1 (const int32_t *a, size_t stride, size_t vl);
vint32m2_t __riscv_th_vlsw_v_i32m2 (const int32_t *a, size_t stride, size_t vl);
vint32m4_t __riscv_th_vlsw_v_i32m4 (const int32_t *a, size_t stride, size_t vl);
vint32m8_t __riscv_th_vlsw_v_i32m8 (const int32_t *a, size_t stride, size_t vl);
vint64m1_t __riscv_th_vlsw_v_i64m1 (const int64_t *a, size_t stride, size_t vl);
vint64m2_t __riscv_th_vlsw_v_i64m2 (const int64_t *a, size_t stride, size_t vl);
vint64m4_t __riscv_th_vlsw_v_i64m4 (const int64_t *a, size_t stride, size_t vl);
vint64m8_t __riscv_th_vlsw_v_i64m8 (const int64_t *a, size_t stride, size_t vl);
vuint8m1_t __riscv_th_vlsbu_v_u8m1 (const uint8_t *a, size_t stride, size_t vl);
vuint8m2_t __riscv_th_vlsbu_v_u8m2 (const uint8_t *a, size_t stride, size_t vl);
vuint8m4_t __riscv_th_vlsbu_v_u8m4 (const uint8_t *a, size_t stride, size_t vl);
vuint8m8_t __riscv_th_vlsbu_v_u8m8 (const uint8_t *a, size_t stride, size_t vl);
vuint16m1_t __riscv_th_vlsbu_v_u16m1 (const uint16_t *a, size_t stride, size_t vl);
vuint16m2_t __riscv_th_vlsbu_v_u16m2 (const uint16_t *a, size_t stride, size_t vl);
vuint16m4_t __riscv_th_vlsbu_v_u16m4 (const uint16_t *a, size_t stride, size_t vl);
vuint16m8_t __riscv_th_vlsbu_v_u16m8 (const uint16_t *a, size_t stride, size_t vl);
vuint32m1_t __riscv_th_vlsbu_v_u32m1 (const uint32_t *a, size_t stride, size_t vl);
vuint32m2_t __riscv_th_vlsbu_v_u32m2 (const uint32_t *a, size_t stride, size_t vl);
vuint32m4_t __riscv_th_vlsbu_v_u32m4 (const uint32_t *a, size_t stride, size_t vl);
vuint32m8_t __riscv_th_vlsbu_v_u32m8 (const uint32_t *a, size_t stride, size_t vl);
vuint64m1_t __riscv_th_vlsbu_v_u64m1 (const uint64_t *a, size_t stride, size_t vl);
vuint64m2_t __riscv_th_vlsbu_v_u64m2 (const uint64_t *a, size_t stride, size_t vl);
vuint64m4_t __riscv_th_vlsbu_v_u64m4 (const uint64_t *a, size_t stride, size_t vl);
vuint64m8_t __riscv_th_vlsbu_v_u64m8 (const uint64_t *a, size_t stride, size_t vl);
vuint16m1_t __riscv_th_vlshu_v_u16m1 (const uint16_t *a, size_t stride, size_t vl);
vuint16m2_t __riscv_th_vlshu_v_u16m2 (const uint16_t *a, size_t stride, size_t vl);
vuint16m4_t __riscv_th_vlshu_v_u16m4 (const uint16_t *a, size_t stride, size_t vl);
vuint16m8_t __riscv_th_vlshu_v_u16m8 (const uint16_t *a, size_t stride, size_t vl);
vuint32m1_t __riscv_th_vlshu_v_u32m1 (const uint32_t *a, size_t stride, size_t vl);
vuint32m2_t __riscv_th_vlshu_v_u32m2 (const uint32_t *a, size_t stride, size_t vl);
vuint32m4_t __riscv_th_vlshu_v_u32m4 (const uint32_t *a, size_t stride, size_t vl);
vuint32m8_t __riscv_th_vlshu_v_u32m8 (const uint32_t *a, size_t stride, size_t vl);
vuint64m1_t __riscv_th_vlshu_v_u64m1 (const uint64_t *a, size_t stride, size_t vl);
vuint64m2_t __riscv_th_vlshu_v_u64m2 (const uint64_t *a, size_t stride, size_t vl);
vuint64m4_t __riscv_th_vlshu_v_u64m4 (const uint64_t *a, size_t stride, size_t vl);
vuint64m8_t __riscv_th_vlshu_v_u64m8 (const uint64_t *a, size_t stride, size_t vl);
vuint32m1_t __riscv_th_vlswu_v_u32m1 (const uint32_t *a, size_t stride, size_t vl);
vuint32m2_t __riscv_th_vlswu_v_u32m2 (const uint32_t *a, size_t stride, size_t vl);
vuint32m4_t __riscv_th_vlswu_v_u32m4 (const uint32_t *a, size_t stride, size_t vl);
vuint32m8_t __riscv_th_vlswu_v_u32m8 (const uint32_t *a, size_t stride, size_t vl);
vuint64m1_t __riscv_th_vlswu_v_u64m1 (const uint64_t *a, size_t stride, size_t vl);
vuint64m2_t __riscv_th_vlswu_v_u64m2 (const uint64_t *a, size_t stride, size_t vl);
vuint64m4_t __riscv_th_vlswu_v_u64m4 (const uint64_t *a, size_t stride, size_t vl);
vuint64m8_t __riscv_th_vlswu_v_u64m8 (const uint64_t *a, size_t stride, size_t vl);
// masked functions
vint8m1_t __riscv_th_vlsb_v_i8m1_m (vbool8_t mask, const int8_t *a, size_t stride, size_t vl);
vint8m2_t __riscv_th_vlsb_v_i8m2_m (vbool4_t mask, const int8_t *a, size_t stride, size_t vl);
vint8m4_t __riscv_th_vlsb_v_i8m4_m (vbool2_t mask, const int8_t *a, size_t stride, size_t vl);
vint8m8_t __riscv_th_vlsb_v_i8m8_m (vbool1_t mask, const int8_t *a, size_t stride, size_t vl);
vint16m1_t __riscv_th_vlsb_v_i16m1_m (vbool16_t mask, const int16_t *a, size_t stride, size_t vl);
vint16m2_t __riscv_th_vlsb_v_i16m2_m (vbool8_t mask, const int16_t *a, size_t stride, size_t vl);
vint16m4_t __riscv_th_vlsb_v_i16m4_m (vbool4_t mask, const int16_t *a, size_t stride, size_t vl);
vint16m8_t __riscv_th_vlsb_v_i16m8_m (vbool2_t mask, const int16_t *a, size_t stride, size_t vl);
vint32m1_t __riscv_th_vlsb_v_i32m1_m (vbool32_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m2_t __riscv_th_vlsb_v_i32m2_m (vbool16_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m4_t __riscv_th_vlsb_v_i32m4_m (vbool8_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m8_t __riscv_th_vlsb_v_i32m8_m (vbool4_t mask, const int32_t *a, size_t stride, size_t vl);
vint64m1_t __riscv_th_vlsb_v_i64m1_m (vbool64_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m2_t __riscv_th_vlsb_v_i64m2_m (vbool32_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m4_t __riscv_th_vlsb_v_i64m4_m (vbool16_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m8_t __riscv_th_vlsb_v_i64m8_m (vbool8_t mask, const int64_t *a, size_t stride, size_t vl);
vint16m1_t __riscv_th_vlsh_v_i16m1_m (vbool16_t mask, const int16_t *a, size_t stride, size_t vl);
vint16m2_t __riscv_th_vlsh_v_i16m2_m (vbool8_t mask, const int16_t *a, size_t stride, size_t vl);
vint16m4_t __riscv_th_vlsh_v_i16m4_m (vbool4_t mask, const int16_t *a, size_t stride, size_t vl);
vint16m8_t __riscv_th_vlsh_v_i16m8_m (vbool2_t mask, const int16_t *a, size_t stride, size_t vl);
vint32m1_t __riscv_th_vlsh_v_i32m1_m (vbool32_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m2_t __riscv_th_vlsh_v_i32m2_m (vbool16_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m4_t __riscv_th_vlsh_v_i32m4_m (vbool8_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m8_t __riscv_th_vlsh_v_i32m8_m (vbool4_t mask, const int32_t *a, size_t stride, size_t vl);
vint64m1_t __riscv_th_vlsh_v_i64m1_m (vbool64_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m2_t __riscv_th_vlsh_v_i64m2_m (vbool32_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m4_t __riscv_th_vlsh_v_i64m4_m (vbool16_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m8_t __riscv_th_vlsh_v_i64m8_m (vbool8_t mask, const int64_t *a, size_t stride, size_t vl);
vint32m1_t __riscv_th_vlsw_v_i32m1_m (vbool32_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m2_t __riscv_th_vlsw_v_i32m2_m (vbool16_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m4_t __riscv_th_vlsw_v_i32m4_m (vbool8_t mask, const int32_t *a, size_t stride, size_t vl);
vint32m8_t __riscv_th_vlsw_v_i32m8_m (vbool4_t mask, const int32_t *a, size_t stride, size_t vl);
vint64m1_t __riscv_th_vlsw_v_i64m1_m (vbool64_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m2_t __riscv_th_vlsw_v_i64m2_m (vbool32_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m4_t __riscv_th_vlsw_v_i64m4_m (vbool16_t mask, const int64_t *a, size_t stride, size_t vl);
vint64m8_t __riscv_th_vlsw_v_i64m8_m (vbool8_t mask, const int64_t *a, size_t stride, size_t vl);
vuint8m1_t __riscv_th_vlsbu_v_u8m1_m (vbool8_t mask, const uint8_t *a, size_t stride, size_t vl);
vuint8m2_t __riscv_th_vlsbu_v_u8m2_m (vbool4_t mask, const uint8_t *a, size_t stride, size_t vl);
vuint8m4_t __riscv_th_vlsbu_v_u8m4_m (vbool2_t mask, const uint8_t *a, size_t stride, size_t vl);
vuint8m8_t __riscv_th_vlsbu_v_u8m8_m (vbool1_t mask, const uint8_t *a, size_t stride, size_t vl);
vuint16m1_t __riscv_th_vlsbu_v_u16m1_m (vbool16_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint16m2_t __riscv_th_vlsbu_v_u16m2_m (vbool8_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint16m4_t __riscv_th_vlsbu_v_u16m4_m (vbool4_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint16m8_t __riscv_th_vlsbu_v_u16m8_m (vbool2_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint32m1_t __riscv_th_vlsbu_v_u32m1_m (vbool32_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m2_t __riscv_th_vlsbu_v_u32m2_m (vbool16_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m4_t __riscv_th_vlsbu_v_u32m4_m (vbool8_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m8_t __riscv_th_vlsbu_v_u32m8_m (vbool4_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint64m1_t __riscv_th_vlsbu_v_u64m1_m (vbool64_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m2_t __riscv_th_vlsbu_v_u64m2_m (vbool32_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m4_t __riscv_th_vlsbu_v_u64m4_m (vbool16_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m8_t __riscv_th_vlsbu_v_u64m8_m (vbool8_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint16m1_t __riscv_th_vlshu_v_u16m1_m (vbool16_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint16m2_t __riscv_th_vlshu_v_u16m2_m (vbool8_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint16m4_t __riscv_th_vlshu_v_u16m4_m (vbool4_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint16m8_t __riscv_th_vlshu_v_u16m8_m (vbool2_t mask, const uint16_t *a, size_t stride, size_t vl);
vuint32m1_t __riscv_th_vlshu_v_u32m1_m (vbool32_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m2_t __riscv_th_vlshu_v_u32m2_m (vbool16_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m4_t __riscv_th_vlshu_v_u32m4_m (vbool8_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m8_t __riscv_th_vlshu_v_u32m8_m (vbool4_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint64m1_t __riscv_th_vlshu_v_u64m1_m (vbool64_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m2_t __riscv_th_vlshu_v_u64m2_m (vbool32_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m4_t __riscv_th_vlshu_v_u64m4_m (vbool16_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m8_t __riscv_th_vlshu_v_u64m8_m (vbool8_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint32m1_t __riscv_th_vlswu_v_u32m1_m (vbool32_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m2_t __riscv_th_vlswu_v_u32m2_m (vbool16_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m4_t __riscv_th_vlswu_v_u32m4_m (vbool8_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint32m8_t __riscv_th_vlswu_v_u32m8_m (vbool4_t mask, const uint32_t *a, size_t stride, size_t vl);
vuint64m1_t __riscv_th_vlswu_v_u64m1_m (vbool64_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m2_t __riscv_th_vlswu_v_u64m2_m (vbool32_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m4_t __riscv_th_vlswu_v_u64m4_m (vbool16_t mask, const uint64_t *a, size_t stride, size_t vl);
vuint64m8_t __riscv_th_vlswu_v_u64m8_m (vbool8_t mask, const uint64_t *a, size_t stride, size_t vl);
----

[[xtheadvector-strided-store]]
===== XTheadVector Strided Store Intrinsics

[,c]
----
void __riscv_th_vssb_v_i8m1 (int8_t *a, size_t stride, vint8m1_t b, size_t vl);
void __riscv_th_vssb_v_i8m2 (int8_t *a, size_t stride, vint8m2_t b, size_t vl);
void __riscv_th_vssb_v_i8m4 (int8_t *a, size_t stride, vint8m4_t b, size_t vl);
void __riscv_th_vssb_v_i8m8 (int8_t *a, size_t stride, vint8m8_t b, size_t vl);
void __riscv_th_vssb_v_i16m1 (int16_t *a, size_t stride, vint16m1_t b, size_t vl);
void __riscv_th_vssb_v_i16m2 (int16_t *a, size_t stride, vint16m2_t b, size_t vl);
void __riscv_th_vssb_v_i16m4 (int16_t *a, size_t stride, vint16m4_t b, size_t vl);
void __riscv_th_vssb_v_i16m8 (int16_t *a, size_t stride, vint16m8_t b, size_t vl);
void __riscv_th_vssb_v_i32m1 (int32_t *a, size_t stride, vint32m1_t b, size_t vl);
void __riscv_th_vssb_v_i32m2 (int32_t *a, size_t stride, vint32m2_t b, size_t vl);
void __riscv_th_vssb_v_i32m4 (int32_t *a, size_t stride, vint32m4_t b, size_t vl);
void __riscv_th_vssb_v_i32m8 (int32_t *a, size_t stride, vint32m8_t b, size_t vl);
void __riscv_th_vssb_v_u8m1 (uint8_t *a, size_t stride, vuint8m1_t b, size_t vl);
void __riscv_th_vssb_v_u8m2 (uint8_t *a, size_t stride, vuint8m2_t b, size_t vl);
void __riscv_th_vssb_v_u8m4 (uint8_t *a, size_t stride, vuint8m4_t b, size_t vl);
void __riscv_th_vssb_v_u8m8 (uint8_t *a, size_t stride, vuint8m8_t b, size_t vl);
void __riscv_th_vssb_v_u16m1 (uint16_t *a, size_t stride, vuint16m1_t b, size_t vl);
void __riscv_th_vssb_v_u16m2 (uint16_t *a, size_t stride, vuint16m2_t b, size_t vl);
void __riscv_th_vssb_v_u16m4 (uint16_t *a, size_t stride, vuint16m4_t b, size_t vl);
void __riscv_th_vssb_v_u16m8 (uint16_t *a, size_t stride, vuint16m8_t b, size_t vl);
void __riscv_th_vssb_v_u32m1 (uint32_t *a, size_t stride, vuint32m1_t b, size_t vl);
void __riscv_th_vssb_v_u32m2 (uint32_t *a, size_t stride, vuint32m2_t b, size_t vl);
void __riscv_th_vssb_v_u32m4 (uint32_t *a, size_t stride, vuint32m4_t b, size_t vl);
void __riscv_th_vssb_v_u32m8 (uint32_t *a, size_t stride, vuint32m8_t b, size_t vl);
void __riscv_th_vssh_v_i16m1 (int16_t *a, size_t stride, vint16m1_t b, size_t vl);
void __riscv_th_vssh_v_i16m2 (int16_t *a, size_t stride, vint16m2_t b, size_t vl);
void __riscv_th_vssh_v_i16m4 (int16_t *a, size_t stride, vint16m4_t b, size_t vl);
void __riscv_th_vssh_v_i16m8 (int16_t *a, size_t stride, vint16m8_t b, size_t vl);
void __riscv_th_vssh_v_i32m1 (int32_t *a, size_t stride, vint32m1_t b, size_t vl);
void __riscv_th_vssh_v_i32m2 (int32_t *a, size_t stride, vint32m2_t b, size_t vl);
void __riscv_th_vssh_v_i32m4 (int32_t *a, size_t stride, vint32m4_t b, size_t vl);
void __riscv_th_vssh_v_i32m8 (int32_t *a, size_t stride, vint32m8_t b, size_t vl);
void __riscv_th_vssh_v_u16m1 (uint16_t *a, size_t stride, vuint16m1_t b, size_t vl);
void __riscv_th_vssh_v_u16m2 (uint16_t *a, size_t stride, vuint16m2_t b, size_t vl);
void __riscv_th_vssh_v_u16m4 (uint16_t *a, size_t stride, vuint16m4_t b, size_t vl);
void __riscv_th_vssh_v_u16m8 (uint16_t *a, size_t stride, vuint16m8_t b, size_t vl);
void __riscv_th_vssh_v_u32m1 (uint32_t *a, size_t stride, vuint32m1_t b, size_t vl);
void __riscv_th_vssh_v_u32m2 (uint32_t *a, size_t stride, vuint32m2_t b, size_t vl);
void __riscv_th_vssh_v_u32m4 (uint32_t *a, size_t stride, vuint32m4_t b, size_t vl);
void __riscv_th_vssh_v_u32m8 (uint32_t *a, size_t stride, vuint32m8_t b, size_t vl);
void __riscv_th_vssw_v_i32m1 (int32_t *a, size_t stride, vint32m1_t b, size_t vl);
void __riscv_th_vssw_v_i32m2 (int32_t *a, size_t stride, vint32m2_t b, size_t vl);
void __riscv_th_vssw_v_i32m4 (int32_t *a, size_t stride, vint32m4_t b, size_t vl);
void __riscv_th_vssw_v_i32m8 (int32_t *a, size_t stride, vint32m8_t b, size_t vl);
void __riscv_th_vssw_v_u32m1 (uint32_t *a, size_t stride, vuint32m1_t b, size_t vl);
void __riscv_th_vssw_v_u32m2 (uint32_t *a, size_t stride, vuint32m2_t b, size_t vl);
void __riscv_th_vssw_v_u32m4 (uint32_t *a, size_t stride, vuint32m4_t b, size_t vl);
void __riscv_th_vssw_v_u32m8 (uint32_t *a, size_t stride, vuint32m8_t b, size_t vl);
// masked functions
void __riscv_th_vssb_v_i8m1_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1_t b, size_t vl);
void __riscv_th_vssb_v_i8m2_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2_t b, size_t vl);
void __riscv_th_vssb_v_i8m4_m (vbool2_t mask, int8_t *a, size_t stride, vint8m4_t b, size_t vl);
void __riscv_th_vssb_v_i8m8_m (vbool1_t mask, int8_t *a, size_t stride, vint8m8_t b, size_t vl);
void __riscv_th_vssb_v_i16m1_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1_t b, size_t vl);
void __riscv_th_vssb_v_i16m2_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2_t b, size_t vl);
void __riscv_th_vssb_v_i16m4_m (vbool4_t mask, int16_t *a, size_t stride, vint16m4_t b, size_t vl);
void __riscv_th_vssb_v_i16m8_m (vbool2_t mask, int16_t *a, size_t stride, vint16m8_t b, size_t vl);
void __riscv_th_vssb_v_i32m1_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1_t b, size_t vl);
void __riscv_th_vssb_v_i32m2_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2_t b, size_t vl);
void __riscv_th_vssb_v_i32m4_m (vbool8_t mask, int32_t *a, size_t stride, vint32m4_t b, size_t vl);
void __riscv_th_vssb_v_i32m8_m (vbool4_t mask, int32_t *a, size_t stride, vint32m8_t b, size_t vl);
void __riscv_th_vssb_v_u8m1_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1_t b, size_t vl);
void __riscv_th_vssb_v_u8m2_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2_t b, size_t vl);
void __riscv_th_vssb_v_u8m4_m (vbool2_t mask, uint8_t *a, size_t stride, vuint8m4_t b, size_t vl);
void __riscv_th_vssb_v_u8m8_m (vbool1_t mask, uint8_t *a, size_t stride, vuint8m8_t b, size_t vl);
void __riscv_th_vssb_v_u16m1_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1_t b, size_t vl);
void __riscv_th_vssb_v_u16m2_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2_t b, size_t vl);
void __riscv_th_vssb_v_u16m4_m (vbool4_t mask, uint16_t *a, size_t stride, vuint16m4_t b, size_t vl);
void __riscv_th_vssb_v_u16m8_m (vbool2_t mask, uint16_t *a, size_t stride, vuint16m8_t b, size_t vl);
void __riscv_th_vssb_v_u32m1_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1_t b, size_t vl);
void __riscv_th_vssb_v_u32m2_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2_t b, size_t vl);
void __riscv_th_vssb_v_u32m4_m (vbool8_t mask, uint32_t *a, size_t stride, vuint32m4_t b, size_t vl);
void __riscv_th_vssb_v_u32m8_m (vbool4_t mask, uint32_t *a, size_t stride, vuint32m8_t b, size_t vl);
void __riscv_th_vssh_v_i16m1_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1_t b, size_t vl);
void __riscv_th_vssh_v_i16m2_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2_t b, size_t vl);
void __riscv_th_vssh_v_i16m4_m (vbool4_t mask, int16_t *a, size_t stride, vint16m4_t b, size_t vl);
void __riscv_th_vssh_v_i16m8_m (vbool2_t mask, int16_t *a, size_t stride, vint16m8_t b, size_t vl);
void __riscv_th_vssh_v_i32m1_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1_t b, size_t vl);
void __riscv_th_vssh_v_i32m2_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2_t b, size_t vl);
void __riscv_th_vssh_v_i32m4_m (vbool8_t mask, int32_t *a, size_t stride, vint32m4_t b, size_t vl);
void __riscv_th_vssh_v_i32m8_m (vbool4_t mask, int32_t *a, size_t stride, vint32m8_t b, size_t vl);
void __riscv_th_vssh_v_u16m1_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1_t b, size_t vl);
void __riscv_th_vssh_v_u16m2_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2_t b, size_t vl);
void __riscv_th_vssh_v_u16m4_m (vbool4_t mask, uint16_t *a, size_t stride, vuint16m4_t b, size_t vl);
void __riscv_th_vssh_v_u16m8_m (vbool2_t mask, uint16_t *a, size_t stride, vuint16m8_t b, size_t vl);
void __riscv_th_vssh_v_u32m1_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1_t b, size_t vl);
void __riscv_th_vssh_v_u32m2_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2_t b, size_t vl);
void __riscv_th_vssh_v_u32m4_m (vbool8_t mask, uint32_t *a, size_t stride, vuint32m4_t b, size_t vl);
void __riscv_th_vssh_v_u32m8_m (vbool4_t mask, uint32_t *a, size_t stride, vuint32m8_t b, size_t vl);
void __riscv_th_vssw_v_i32m1_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1_t b, size_t vl);
void __riscv_th_vssw_v_i32m2_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2_t b, size_t vl);
void __riscv_th_vssw_v_i32m4_m (vbool8_t mask, int32_t *a, size_t stride, vint32m4_t b, size_t vl);
void __riscv_th_vssw_v_i32m8_m (vbool4_t mask, int32_t *a, size_t stride, vint32m8_t b, size_t vl);
void __riscv_th_vssw_v_u32m1_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1_t b, size_t vl);
void __riscv_th_vssw_v_u32m2_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2_t b, size_t vl);
void __riscv_th_vssw_v_u32m4_m (vbool8_t mask, uint32_t *a, size_t stride, vuint32m4_t b, size_t vl);
void __riscv_th_vssw_v_u32m8_m (vbool4_t mask, uint32_t *a, size_t stride, vuint32m8_t b, size_t vl);
----

[[xtheadvector-indexed-load]]
===== XTheadVector Indexed Load Intrinsics

[,c]
----
vint8m1_t __riscv_th_vlxb_v_i8m1 (const int8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2_t __riscv_th_vlxb_v_i8m2 (const int8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4_t __riscv_th_vlxb_v_i8m4 (const int8_t *a, vuint8m4_t indexed, size_t vl);
vint8m8_t __riscv_th_vlxb_v_i8m8 (const int8_t *a, vuint8m8_t indexed, size_t vl);
vint16m1_t __riscv_th_vlxb_v_i16m1 (const int16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2_t __riscv_th_vlxb_v_i16m2 (const int16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4_t __riscv_th_vlxb_v_i16m4 (const int16_t *a, vuint16m4_t indexed, size_t vl);
vint16m8_t __riscv_th_vlxb_v_i16m8 (const int16_t *a, vuint16m8_t indexed, size_t vl);
vint32m1_t __riscv_th_vlxb_v_i32m1 (const int32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2_t __riscv_th_vlxb_v_i32m2 (const int32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4_t __riscv_th_vlxb_v_i32m4 (const int32_t *a, vuint32m4_t indexed, size_t vl);
vint32m8_t __riscv_th_vlxb_v_i32m8 (const int32_t *a, vuint32m8_t indexed, size_t vl);
vint64m1_t __riscv_th_vlxb_v_i64m1 (const int64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2_t __riscv_th_vlxb_v_i64m2 (const int64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4_t __riscv_th_vlxb_v_i64m4 (const int64_t *a, vuint64m4_t indexed, size_t vl);
vint64m8_t __riscv_th_vlxb_v_i64m8 (const int64_t *a, vuint64m8_t indexed, size_t vl);
vint16m1_t __riscv_th_vlxh_v_i16m1 (const int16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2_t __riscv_th_vlxh_v_i16m2 (const int16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4_t __riscv_th_vlxh_v_i16m4 (const int16_t *a, vuint16m4_t indexed, size_t vl);
vint16m8_t __riscv_th_vlxh_v_i16m8 (const int16_t *a, vuint16m8_t indexed, size_t vl);
vint32m1_t __riscv_th_vlxh_v_i32m1 (const int32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2_t __riscv_th_vlxh_v_i32m2 (const int32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4_t __riscv_th_vlxh_v_i32m4 (const int32_t *a, vuint32m4_t indexed, size_t vl);
vint32m8_t __riscv_th_vlxh_v_i32m8 (const int32_t *a, vuint32m8_t indexed, size_t vl);
vint64m1_t __riscv_th_vlxh_v_i64m1 (const int64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2_t __riscv_th_vlxh_v_i64m2 (const int64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4_t __riscv_th_vlxh_v_i64m4 (const int64_t *a, vuint64m4_t indexed, size_t vl);
vint64m8_t __riscv_th_vlxh_v_i64m8 (const int64_t *a, vuint64m8_t indexed, size_t vl);
vint32m1_t __riscv_th_vlxw_v_i32m1 (const int32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2_t __riscv_th_vlxw_v_i32m2 (const int32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4_t __riscv_th_vlxw_v_i32m4 (const int32_t *a, vuint32m4_t indexed, size_t vl);
vint32m8_t __riscv_th_vlxw_v_i32m8 (const int32_t *a, vuint32m8_t indexed, size_t vl);
vint64m1_t __riscv_th_vlxw_v_i64m1 (const int64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2_t __riscv_th_vlxw_v_i64m2 (const int64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4_t __riscv_th_vlxw_v_i64m4 (const int64_t *a, vuint64m4_t indexed, size_t vl);
vint64m8_t __riscv_th_vlxw_v_i64m8 (const int64_t *a, vuint64m8_t indexed, size_t vl);
vuint8m1_t __riscv_th_vlxbu_v_u8m1 (const uint8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m2_t __riscv_th_vlxbu_v_u8m2 (const uint8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m4_t __riscv_th_vlxbu_v_u8m4 (const uint8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m8_t __riscv_th_vlxbu_v_u8m8 (const uint8_t *a, vuint8m8_t indexed, size_t vl);
vuint16m1_t __riscv_th_vlxbu_v_u16m1 (const uint16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m2_t __riscv_th_vlxbu_v_u16m2 (const uint16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m4_t __riscv_th_vlxbu_v_u16m4 (const uint16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m8_t __riscv_th_vlxbu_v_u16m8 (const uint16_t *a, vuint16m8_t indexed, size_t vl);
vuint32m1_t __riscv_th_vlxbu_v_u32m1 (const uint32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m2_t __riscv_th_vlxbu_v_u32m2 (const uint32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m4_t __riscv_th_vlxbu_v_u32m4 (const uint32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m8_t __riscv_th_vlxbu_v_u32m8 (const uint32_t *a, vuint32m8_t indexed, size_t vl);
vuint64m1_t __riscv_th_vlxbu_v_u64m1 (const uint64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m2_t __riscv_th_vlxbu_v_u64m2 (const uint64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m4_t __riscv_th_vlxbu_v_u64m4 (const uint64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m8_t __riscv_th_vlxbu_v_u64m8 (const uint64_t *a, vuint64m8_t indexed, size_t vl);
vuint16m1_t __riscv_th_vlxhu_v_u16m1 (const uint16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m2_t __riscv_th_vlxhu_v_u16m2 (const uint16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m4_t __riscv_th_vlxhu_v_u16m4 (const uint16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m8_t __riscv_th_vlxhu_v_u16m8 (const uint16_t *a, vuint16m8_t indexed, size_t vl);
vuint32m1_t __riscv_th_vlxhu_v_u32m1 (const uint32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m2_t __riscv_th_vlxhu_v_u32m2 (const uint32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m4_t __riscv_th_vlxhu_v_u32m4 (const uint32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m8_t __riscv_th_vlxhu_v_u32m8 (const uint32_t *a, vuint32m8_t indexed, size_t vl);
vuint64m1_t __riscv_th_vlxhu_v_u64m1 (const uint64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m2_t __riscv_th_vlxhu_v_u64m2 (const uint64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m4_t __riscv_th_vlxhu_v_u64m4 (const uint64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m8_t __riscv_th_vlxhu_v_u64m8 (const uint64_t *a, vuint64m8_t indexed, size_t vl);
vuint32m1_t __riscv_th_vlxwu_v_u32m1 (const uint32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m2_t __riscv_th_vlxwu_v_u32m2 (const uint32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m4_t __riscv_th_vlxwu_v_u32m4 (const uint32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m8_t __riscv_th_vlxwu_v_u32m8 (const uint32_t *a, vuint32m8_t indexed, size_t vl);
vuint64m1_t __riscv_th_vlxwu_v_u64m1 (const uint64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m2_t __riscv_th_vlxwu_v_u64m2 (const uint64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m4_t __riscv_th_vlxwu_v_u64m4 (const uint64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m8_t __riscv_th_vlxwu_v_u64m8 (const uint64_t *a, vuint64m8_t indexed, size_t vl);
// masked functions
vint8m1_t __riscv_th_vlxb_v_i8m1_m (vbool8_t mask, const int8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2_t __riscv_th_vlxb_v_i8m2_m (vbool4_t mask, const int8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4_t __riscv_th_vlxb_v_i8m4_m (vbool2_t mask, const int8_t *a, vuint8m4_t indexed, size_t vl);
vint8m8_t __riscv_th_vlxb_v_i8m8_m (vbool1_t mask, const int8_t *a, vuint8m8_t indexed, size_t vl);
vint16m1_t __riscv_th_vlxb_v_i16m1_m (vbool16_t mask, const int16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2_t __riscv_th_vlxb_v_i16m2_m (vbool8_t mask, const int16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4_t __riscv_th_vlxb_v_i16m4_m (vbool4_t mask, const int16_t *a, vuint16m4_t indexed, size_t vl);
vint16m8_t __riscv_th_vlxb_v_i16m8_m (vbool2_t mask, const int16_t *a, vuint16m8_t indexed, size_t vl);
vint32m1_t __riscv_th_vlxb_v_i32m1_m (vbool32_t mask, const int32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2_t __riscv_th_vlxb_v_i32m2_m (vbool16_t mask, const int32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4_t __riscv_th_vlxb_v_i32m4_m (vbool8_t mask, const int32_t *a, vuint32m4_t indexed, size_t vl);
vint32m8_t __riscv_th_vlxb_v_i32m8_m (vbool4_t mask, const int32_t *a, vuint32m8_t indexed, size_t vl);
vint64m1_t __riscv_th_vlxb_v_i64m1_m (vbool64_t mask, const int64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2_t __riscv_th_vlxb_v_i64m2_m (vbool32_t mask, const int64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4_t __riscv_th_vlxb_v_i64m4_m (vbool16_t mask, const int64_t *a, vuint64m4_t indexed, size_t vl);
vint64m8_t __riscv_th_vlxb_v_i64m8_m (vbool8_t mask, const int64_t *a, vuint64m8_t indexed, size_t vl);
vint16m1_t __riscv_th_vlxh_v_i16m1_m (vbool16_t mask, const int16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2_t __riscv_th_vlxh_v_i16m2_m (vbool8_t mask, const int16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4_t __riscv_th_vlxh_v_i16m4_m (vbool4_t mask, const int16_t *a, vuint16m4_t indexed, size_t vl);
vint16m8_t __riscv_th_vlxh_v_i16m8_m (vbool2_t mask, const int16_t *a, vuint16m8_t indexed, size_t vl);
vint32m1_t __riscv_th_vlxh_v_i32m1_m (vbool32_t mask, const int32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2_t __riscv_th_vlxh_v_i32m2_m (vbool16_t mask, const int32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4_t __riscv_th_vlxh_v_i32m4_m (vbool8_t mask, const int32_t *a, vuint32m4_t indexed, size_t vl);
vint32m8_t __riscv_th_vlxh_v_i32m8_m (vbool4_t mask, const int32_t *a, vuint32m8_t indexed, size_t vl);
vint64m1_t __riscv_th_vlxh_v_i64m1_m (vbool64_t mask, const int64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2_t __riscv_th_vlxh_v_i64m2_m (vbool32_t mask, const int64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4_t __riscv_th_vlxh_v_i64m4_m (vbool16_t mask, const int64_t *a, vuint64m4_t indexed, size_t vl);
vint64m8_t __riscv_th_vlxh_v_i64m8_m (vbool8_t mask, const int64_t *a, vuint64m8_t indexed, size_t vl);
vint32m1_t __riscv_th_vlxw_v_i32m1_m (vbool32_t mask, const int32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2_t __riscv_th_vlxw_v_i32m2_m (vbool16_t mask, const int32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4_t __riscv_th_vlxw_v_i32m4_m (vbool8_t mask, const int32_t *a, vuint32m4_t indexed, size_t vl);
vint32m8_t __riscv_th_vlxw_v_i32m8_m (vbool4_t mask, const int32_t *a, vuint32m8_t indexed, size_t vl);
vint64m1_t __riscv_th_vlxw_v_i64m1_m (vbool64_t mask, const int64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2_t __riscv_th_vlxw_v_i64m2_m (vbool32_t mask, const int64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4_t __riscv_th_vlxw_v_i64m4_m (vbool16_t mask, const int64_t *a, vuint64m4_t indexed, size_t vl);
vint64m8_t __riscv_th_vlxw_v_i64m8_m (vbool8_t mask, const int64_t *a, vuint64m8_t indexed, size_t vl);
vuint8m1_t __riscv_th_vlxbu_v_u8m1_m (vbool8_t mask, const uint8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m2_t __riscv_th_vlxbu_v_u8m2_m (vbool4_t mask, const uint8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m4_t __riscv_th_vlxbu_v_u8m4_m (vbool2_t mask, const uint8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m8_t __riscv_th_vlxbu_v_u8m8_m (vbool1_t mask, const uint8_t *a, vuint8m8_t indexed, size_t vl);
vuint16m1_t __riscv_th_vlxbu_v_u16m1_m (vbool16_t mask, const uint16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m2_t __riscv_th_vlxbu_v_u16m2_m (vbool8_t mask, const uint16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m4_t __riscv_th_vlxbu_v_u16m4_m (vbool4_t mask, const uint16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m8_t __riscv_th_vlxbu_v_u16m8_m (vbool2_t mask, const uint16_t *a, vuint16m8_t indexed, size_t vl);
vuint32m1_t __riscv_th_vlxbu_v_u32m1_m (vbool32_t mask, const uint32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m2_t __riscv_th_vlxbu_v_u32m2_m (vbool16_t mask, const uint32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m4_t __riscv_th_vlxbu_v_u32m4_m (vbool8_t mask, const uint32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m8_t __riscv_th_vlxbu_v_u32m8_m (vbool4_t mask, const uint32_t *a, vuint32m8_t indexed, size_t vl);
vuint64m1_t __riscv_th_vlxbu_v_u64m1_m (vbool64_t mask, const uint64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m2_t __riscv_th_vlxbu_v_u64m2_m (vbool32_t mask, const uint64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m4_t __riscv_th_vlxbu_v_u64m4_m (vbool16_t mask, const uint64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m8_t __riscv_th_vlxbu_v_u64m8_m (vbool8_t mask, const uint64_t *a, vuint64m8_t indexed, size_t vl);
vuint16m1_t __riscv_th_vlxhu_v_u16m1_m (vbool16_t mask, const uint16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m2_t __riscv_th_vlxhu_v_u16m2_m (vbool8_t mask, const uint16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m4_t __riscv_th_vlxhu_v_u16m4_m (vbool4_t mask, const uint16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m8_t __riscv_th_vlxhu_v_u16m8_m (vbool2_t mask, const uint16_t *a, vuint16m8_t indexed, size_t vl);
vuint32m1_t __riscv_th_vlxhu_v_u32m1_m (vbool32_t mask, const uint32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m2_t __riscv_th_vlxhu_v_u32m2_m (vbool16_t mask, const uint32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m4_t __riscv_th_vlxhu_v_u32m4_m (vbool8_t mask, const uint32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m8_t __riscv_th_vlxhu_v_u32m8_m (vbool4_t mask, const uint32_t *a, vuint32m8_t indexed, size_t vl);
vuint64m1_t __riscv_th_vlxhu_v_u64m1_m (vbool64_t mask, const uint64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m2_t __riscv_th_vlxhu_v_u64m2_m (vbool32_t mask, const uint64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m4_t __riscv_th_vlxhu_v_u64m4_m (vbool16_t mask, const uint64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m8_t __riscv_th_vlxhu_v_u64m8_m (vbool8_t mask, const uint64_t *a, vuint64m8_t indexed, size_t vl);
vuint32m1_t __riscv_th_vlxwu_v_u32m1_m (vbool32_t mask, const uint32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m2_t __riscv_th_vlxwu_v_u32m2_m (vbool16_t mask, const uint32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m4_t __riscv_th_vlxwu_v_u32m4_m (vbool8_t mask, const uint32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m8_t __riscv_th_vlxwu_v_u32m8_m (vbool4_t mask, const uint32_t *a, vuint32m8_t indexed, size_t vl);
vuint64m1_t __riscv_th_vlxwu_v_u64m1_m (vbool64_t mask, const uint64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m2_t __riscv_th_vlxwu_v_u64m2_m (vbool32_t mask, const uint64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m4_t __riscv_th_vlxwu_v_u64m4_m (vbool16_t mask, const uint64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m8_t __riscv_th_vlxwu_v_u64m8_m (vbool8_t mask, const uint64_t *a, vuint64m8_t indexed, size_t vl);
----

[[xtheadvector-indexed-store]]
===== XTheadVector Indexed Store Intrinsics

[,c]
----
void __riscv_th_vsxb_v_i8m1 (int8_t *a, vuint8m1_t indexed, vint8m1_t value, size_t vl);
void __riscv_th_vsxb_v_i8m2 (int8_t *a, vuint8m2_t indexed, vint8m2_t value, size_t vl);
void __riscv_th_vsxb_v_i8m4 (int8_t *a, vuint8m4_t indexed, vint8m4_t value, size_t vl);
void __riscv_th_vsxb_v_i8m8 (int8_t *a, vuint8m8_t indexed, vint8m8_t value, size_t vl);
void __riscv_th_vsxb_v_i16m1 (int16_t *a, vuint16m1_t indexed, vint16m1_t value, size_t vl);
void __riscv_th_vsxb_v_i16m2 (int16_t *a, vuint16m2_t indexed, vint16m2_t value, size_t vl);
void __riscv_th_vsxb_v_i16m4 (int16_t *a, vuint16m4_t indexed, vint16m4_t value, size_t vl);
void __riscv_th_vsxb_v_i16m8 (int16_t *a, vuint16m8_t indexed, vint16m8_t value, size_t vl);
void __riscv_th_vsxb_v_i32m1 (int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsxb_v_i32m2 (int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsxb_v_i32m4 (int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsxb_v_i32m8 (int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m1 (int8_t *a, vuint8m1_t indexed, vint8m1_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m2 (int8_t *a, vuint8m2_t indexed, vint8m2_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m4 (int8_t *a, vuint8m4_t indexed, vint8m4_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m8 (int8_t *a, vuint8m8_t indexed, vint8m8_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m1 (int16_t *a, vuint16m1_t indexed, vint16m1_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m2 (int16_t *a, vuint16m2_t indexed, vint16m2_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m4 (int16_t *a, vuint16m4_t indexed, vint16m4_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m8 (int16_t *a, vuint16m8_t indexed, vint16m8_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m1 (int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m2 (int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m4 (int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m8 (int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsxb_v_u8m1 (uint8_t *a, vuint8m1_t indexed, vuint8m1_t value, size_t vl);
void __riscv_th_vsxb_v_u8m2 (uint8_t *a, vuint8m2_t indexed, vuint8m2_t value, size_t vl);
void __riscv_th_vsxb_v_u8m4 (uint8_t *a, vuint8m4_t indexed, vuint8m4_t value, size_t vl);
void __riscv_th_vsxb_v_u8m8 (uint8_t *a, vuint8m8_t indexed, vuint8m8_t value, size_t vl);
void __riscv_th_vsxb_v_u16m1 (uint16_t *a, vuint16m1_t indexed, vuint16m1_t value, size_t vl);
void __riscv_th_vsxb_v_u16m2 (uint16_t *a, vuint16m2_t indexed, vuint16m2_t value, size_t vl);
void __riscv_th_vsxb_v_u16m4 (uint16_t *a, vuint16m4_t indexed, vuint16m4_t value, size_t vl);
void __riscv_th_vsxb_v_u16m8 (uint16_t *a, vuint16m8_t indexed, vuint16m8_t value, size_t vl);
void __riscv_th_vsxb_v_u32m1 (uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsxb_v_u32m2 (uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsxb_v_u32m4 (uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsxb_v_u32m8 (uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m1 (uint8_t *a, vuint8m1_t indexed, vuint8m1_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m2 (uint8_t *a, vuint8m2_t indexed, vuint8m2_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m4 (uint8_t *a, vuint8m4_t indexed, vuint8m4_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m8 (uint8_t *a, vuint8m8_t indexed, vuint8m8_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m1 (uint16_t *a, vuint16m1_t indexed, vuint16m1_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m2 (uint16_t *a, vuint16m2_t indexed, vuint16m2_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m4 (uint16_t *a, vuint16m4_t indexed, vuint16m4_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m8 (uint16_t *a, vuint16m8_t indexed, vuint16m8_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m1 (uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m2 (uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m4 (uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m8 (uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
// masked functions
void __riscv_th_vsxb_v_i8m1_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1_t value, size_t vl);
void __riscv_th_vsxb_v_i8m2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2_t value, size_t vl);
void __riscv_th_vsxb_v_i8m4_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, vint8m4_t value, size_t vl);
void __riscv_th_vsxb_v_i8m8_m (vbool1_t mask, int8_t *a, vuint8m8_t indexed, vint8m8_t value, size_t vl);
void __riscv_th_vsxb_v_i16m1_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1_t value, size_t vl);
void __riscv_th_vsxb_v_i16m2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2_t value, size_t vl);
void __riscv_th_vsxb_v_i16m4_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4_t value, size_t vl);
void __riscv_th_vsxb_v_i16m8_m (vbool2_t mask, int16_t *a, vuint16m8_t indexed, vint16m8_t value, size_t vl);
void __riscv_th_vsxb_v_i32m1_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsxb_v_i32m2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsxb_v_i32m4_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsxb_v_i32m8_m (vbool4_t mask, int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m1_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m4_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, vint8m4_t value, size_t vl);
void __riscv_th_vsuxb_v_i8m8_m (vbool1_t mask, int8_t *a, vuint8m8_t indexed, vint8m8_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m1_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m4_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4_t value, size_t vl);
void __riscv_th_vsuxb_v_i16m8_m (vbool2_t mask, int16_t *a, vuint16m8_t indexed, vint16m8_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m1_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m4_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsuxb_v_i32m8_m (vbool4_t mask, int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsxb_v_u8m1_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1_t value, size_t vl);
void __riscv_th_vsxb_v_u8m2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2_t value, size_t vl);
void __riscv_th_vsxb_v_u8m4_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, vuint8m4_t value, size_t vl);
void __riscv_th_vsxb_v_u8m8_m (vbool1_t mask, uint8_t *a, vuint8m8_t indexed, vuint8m8_t value, size_t vl);
void __riscv_th_vsxb_v_u16m1_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1_t value, size_t vl);
void __riscv_th_vsxb_v_u16m2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2_t value, size_t vl);
void __riscv_th_vsxb_v_u16m4_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4_t value, size_t vl);
void __riscv_th_vsxb_v_u16m8_m (vbool2_t mask, uint16_t *a, vuint16m8_t indexed, vuint16m8_t value, size_t vl);
void __riscv_th_vsxb_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsxb_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsxb_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsxb_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m1_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m4_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, vuint8m4_t value, size_t vl);
void __riscv_th_vsuxb_v_u8m8_m (vbool1_t mask, uint8_t *a, vuint8m8_t indexed, vuint8m8_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m1_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m4_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4_t value, size_t vl);
void __riscv_th_vsuxb_v_u16m8_m (vbool2_t mask, uint16_t *a, vuint16m8_t indexed, vuint16m8_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsuxb_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
void __riscv_th_vsxh_v_i16m1_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1_t value, size_t vl);
void __riscv_th_vsxh_v_i16m2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2_t value, size_t vl);
void __riscv_th_vsxh_v_i16m4_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4_t value, size_t vl);
void __riscv_th_vsxh_v_i16m8_m (vbool2_t mask, int16_t *a, vuint16m8_t indexed, vint16m8_t value, size_t vl);
void __riscv_th_vsxh_v_i32m1_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsxh_v_i32m2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsxh_v_i32m4_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsxh_v_i32m8_m (vbool4_t mask, int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsuxh_v_i16m1_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1_t value, size_t vl);
void __riscv_th_vsuxh_v_i16m2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2_t value, size_t vl);
void __riscv_th_vsuxh_v_i16m4_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4_t value, size_t vl);
void __riscv_th_vsuxh_v_i16m8_m (vbool2_t mask, int16_t *a, vuint16m8_t indexed, vint16m8_t value, size_t vl);
void __riscv_th_vsuxh_v_i32m1_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsuxh_v_i32m2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsuxh_v_i32m4_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsuxh_v_i32m8_m (vbool4_t mask, int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsxh_v_u16m1_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1_t value, size_t vl);
void __riscv_th_vsxh_v_u16m2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2_t value, size_t vl);
void __riscv_th_vsxh_v_u16m4_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4_t value, size_t vl);
void __riscv_th_vsxh_v_u16m8_m (vbool2_t mask, uint16_t *a, vuint16m8_t indexed, vuint16m8_t value, size_t vl);
void __riscv_th_vsxh_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsxh_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsxh_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsxh_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
void __riscv_th_vsuxh_v_u16m1_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1_t value, size_t vl);
void __riscv_th_vsuxh_v_u16m2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2_t value, size_t vl);
void __riscv_th_vsuxh_v_u16m4_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4_t value, size_t vl);
void __riscv_th_vsuxh_v_u16m8_m (vbool2_t mask, uint16_t *a, vuint16m8_t indexed, vuint16m8_t value, size_t vl);
void __riscv_th_vsuxh_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsuxh_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsuxh_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsuxh_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
void __riscv_th_vsxw_v_i32m1_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsxw_v_i32m2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsxw_v_i32m4_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsxw_v_i32m8_m (vbool4_t mask, int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsuxw_v_i32m1_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1_t value, size_t vl);
void __riscv_th_vsuxw_v_i32m2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2_t value, size_t vl);
void __riscv_th_vsuxw_v_i32m4_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4_t value, size_t vl);
void __riscv_th_vsuxw_v_i32m8_m (vbool4_t mask, int32_t *a, vuint32m8_t indexed, vint32m8_t value, size_t vl);
void __riscv_th_vsxw_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsxw_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsxw_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsxw_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
void __riscv_th_vsuxw_v_u32m1_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1_t value, size_t vl);
void __riscv_th_vsuxw_v_u32m2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2_t value, size_t vl);
void __riscv_th_vsuxw_v_u32m4_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4_t value, size_t vl);
void __riscv_th_vsuxw_v_u32m8_m (vbool4_t mask, uint32_t *a, vuint32m8_t indexed, vuint32m8_t value, size_t vl);
----


[[xtheadvector-integer-extract]]
===== XTheadVector Integer Extract Intrinsics

[,c]
----
int8_t __riscv_th_vext_x_v_i8m1_i8 (vint8m1_t a, unsigned idx);
int8_t __riscv_th_vext_x_v_i8m2_i8 (vint8m2_t a, unsigned idx);
int8_t __riscv_th_vext_x_v_i8m4_i8 (vint8m4_t a, unsigned idx);
int8_t __riscv_th_vext_x_v_i8m8_i8 (vint8m8_t a, unsigned idx);
int16_t __riscv_th_vext_x_v_i16m1_i16 (vint16m1_t a, unsigned idx);
int16_t __riscv_th_vext_x_v_i16m2_i16 (vint16m2_t a, unsigned idx);
int16_t __riscv_th_vext_x_v_i16m4_i16 (vint16m4_t a, unsigned idx);
int16_t __riscv_th_vext_x_v_i16m8_i16 (vint16m8_t a, unsigned idx);
int32_t __riscv_th_vext_x_v_i32m1_i32 (vint32m1_t a, unsigned idx);
int32_t __riscv_th_vext_x_v_i32m2_i32 (vint32m2_t a, unsigned idx);
int32_t __riscv_th_vext_x_v_i32m4_i32 (vint32m4_t a, unsigned idx);
int32_t __riscv_th_vext_x_v_i32m8_i32 (vint32m8_t a, unsigned idx);
int64_t __riscv_th_vext_x_v_i64m1_i64 (vint64m1_t a, unsigned idx);
int64_t __riscv_th_vext_x_v_i64m2_i64 (vint64m2_t a, unsigned idx);
int64_t __riscv_th_vext_x_v_i64m4_i64 (vint64m4_t a, unsigned idx);
int64_t __riscv_th_vext_x_v_i64m8_i64 (vint64m8_t a, unsigned idx);
uint8_t __riscv_th_vext_x_v_u8m1_u8 (vuint8m1_t a, unsigned idx);
uint8_t __riscv_th_vext_x_v_u8m2_u8 (vuint8m2_t a, unsigned idx);
uint8_t __riscv_th_vext_x_v_u8m4_u8 (vuint8m4_t a, unsigned idx);
uint8_t __riscv_th_vext_x_v_u8m8_u8 (vuint8m8_t a, unsigned idx);
uint16_t __riscv_th_vext_x_v_u16m1_u16 (vuint16m1_t a, unsigned idx);
uint16_t __riscv_th_vext_x_v_u16m2_u16 (vuint16m2_t a, unsigned idx);
uint16_t __riscv_th_vext_x_v_u16m4_u16 (vuint16m4_t a, unsigned idx);
uint16_t __riscv_th_vext_x_v_u16m8_u16 (vuint16m8_t a, unsigned idx);
uint32_t __riscv_th_vext_x_v_u32m1_u32 (vuint32m1_t a, unsigned idx);
uint32_t __riscv_th_vext_x_v_u32m2_u32 (vuint32m2_t a, unsigned idx);
uint32_t __riscv_th_vext_x_v_u32m4_u32 (vuint32m4_t a, unsigned idx);
uint32_t __riscv_th_vext_x_v_u32m8_u32 (vuint32m8_t a, unsigned idx);
uint64_t __riscv_th_vext_x_v_u64m1_u64 (vuint64m1_t a, unsigned idx);
uint64_t __riscv_th_vext_x_v_u64m2_u64 (vuint64m2_t a, unsigned idx);
uint64_t __riscv_th_vext_x_v_u64m4_u64 (vuint64m4_t a, unsigned idx);
uint64_t __riscv_th_vext_x_v_u64m8_u64 (vuint64m8_t a, unsigned idx);
----

[[xtheadvector-unit-stride-segment-load]]
===== XTheadVector Unit-Stride Segment Load Intrinsics

[,c]
----
vint8m1x2_t __riscv_th_vlseg2b_v_i8m1x2 (int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2b_v_u8m1x2 (uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3b_v_i8m1x3 (int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3b_v_u8m1x3 (uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4b_v_i8m1x4 (int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4b_v_u8m1x4 (uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5b_v_i8m1x5 (int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5b_v_u8m1x5 (uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6b_v_i8m1x6 (int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6b_v_u8m1x6 (uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7b_v_i8m1x7 (int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7b_v_u8m1x7 (uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8b_v_i8m1x8 (int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8b_v_u8m1x8 (uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2b_v_i8m2x2 (int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2b_v_u8m2x2 (uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3b_v_i8m2x3 (int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3b_v_u8m2x3 (uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4b_v_i8m2x4 (int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4b_v_u8m2x4 (uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2b_v_i8m4x2 (int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2b_v_u8m4x2 (uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2b_v_i16m1x2 (int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2b_v_u16m1x2 (uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3b_v_i16m1x3 (int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3b_v_u16m1x3 (uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4b_v_i16m1x4 (int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4b_v_u16m1x4 (uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5b_v_i16m1x5 (int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5b_v_u16m1x5 (uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6b_v_i16m1x6 (int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6b_v_u16m1x6 (uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7b_v_i16m1x7 (int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7b_v_u16m1x7 (uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8b_v_i16m1x8 (int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8b_v_u16m1x8 (uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2b_v_i16m2x2 (int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2b_v_u16m2x2 (uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3b_v_i16m2x3 (int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3b_v_u16m2x3 (uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4b_v_i16m2x4 (int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4b_v_u16m2x4 (uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2b_v_i16m4x2 (int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2b_v_u16m4x2 (uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2b_v_i32m1x2 (int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2b_v_u32m1x2 (uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3b_v_i32m1x3 (int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3b_v_u32m1x3 (uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4b_v_i32m1x4 (int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4b_v_u32m1x4 (uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5b_v_i32m1x5 (int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5b_v_u32m1x5 (uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6b_v_i32m1x6 (int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6b_v_u32m1x6 (uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7b_v_i32m1x7 (int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7b_v_u32m1x7 (uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8b_v_i32m1x8 (int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8b_v_u32m1x8 (uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2b_v_i32m2x2 (int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2b_v_u32m2x2 (uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3b_v_i32m2x3 (int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3b_v_u32m2x3 (uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4b_v_i32m2x4 (int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4b_v_u32m2x4 (uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2b_v_i32m4x2 (int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2b_v_u32m4x2 (uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2b_v_i64m1x2 (int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2b_v_u64m1x2 (uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3b_v_i64m1x3 (int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3b_v_u64m1x3 (uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4b_v_i64m1x4 (int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4b_v_u64m1x4 (uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5b_v_i64m1x5 (int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5b_v_u64m1x5 (uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6b_v_i64m1x6 (int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6b_v_u64m1x6 (uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7b_v_i64m1x7 (int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7b_v_u64m1x7 (uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8b_v_i64m1x8 (int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8b_v_u64m1x8 (uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2b_v_i64m2x2 (int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2b_v_u64m2x2 (uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3b_v_i64m2x3 (int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3b_v_u64m2x3 (uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4b_v_i64m2x4 (int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4b_v_u64m2x4 (uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2b_v_i64m4x2 (int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2b_v_u64m4x2 (uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2b_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2b_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3b_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3b_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4b_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4b_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5b_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5b_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6b_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6b_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7b_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7b_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8b_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8b_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2b_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2b_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3b_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3b_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4b_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4b_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2b_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2b_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2b_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2b_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3b_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3b_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4b_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4b_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5b_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5b_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6b_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6b_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7b_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7b_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8b_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8b_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2b_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2b_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3b_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3b_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4b_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4b_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2b_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2b_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2b_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2b_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3b_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3b_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4b_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4b_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5b_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5b_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6b_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6b_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7b_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7b_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8b_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8b_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2b_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2b_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3b_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3b_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4b_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4b_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2b_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2b_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2b_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2b_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3b_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3b_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4b_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4b_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5b_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5b_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6b_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6b_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7b_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7b_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8b_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8b_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2b_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2b_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3b_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3b_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4b_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4b_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2b_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2b_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2bu_v_i8m1x2 (int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2bu_v_u8m1x2 (uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3bu_v_i8m1x3 (int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3bu_v_u8m1x3 (uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4bu_v_i8m1x4 (int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4bu_v_u8m1x4 (uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5bu_v_i8m1x5 (int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5bu_v_u8m1x5 (uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6bu_v_i8m1x6 (int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6bu_v_u8m1x6 (uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7bu_v_i8m1x7 (int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7bu_v_u8m1x7 (uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8bu_v_i8m1x8 (int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8bu_v_u8m1x8 (uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2bu_v_i8m2x2 (int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2bu_v_u8m2x2 (uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3bu_v_i8m2x3 (int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3bu_v_u8m2x3 (uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4bu_v_i8m2x4 (int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4bu_v_u8m2x4 (uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2bu_v_i8m4x2 (int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2bu_v_u8m4x2 (uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2bu_v_i16m1x2 (int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2bu_v_u16m1x2 (uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3bu_v_i16m1x3 (int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3bu_v_u16m1x3 (uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4bu_v_i16m1x4 (int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4bu_v_u16m1x4 (uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5bu_v_i16m1x5 (int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5bu_v_u16m1x5 (uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6bu_v_i16m1x6 (int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6bu_v_u16m1x6 (uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7bu_v_i16m1x7 (int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7bu_v_u16m1x7 (uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8bu_v_i16m1x8 (int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8bu_v_u16m1x8 (uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2bu_v_i16m2x2 (int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2bu_v_u16m2x2 (uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3bu_v_i16m2x3 (int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3bu_v_u16m2x3 (uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4bu_v_i16m2x4 (int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4bu_v_u16m2x4 (uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2bu_v_i16m4x2 (int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2bu_v_u16m4x2 (uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2bu_v_i32m1x2 (int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2bu_v_u32m1x2 (uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3bu_v_i32m1x3 (int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3bu_v_u32m1x3 (uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4bu_v_i32m1x4 (int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4bu_v_u32m1x4 (uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5bu_v_i32m1x5 (int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5bu_v_u32m1x5 (uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6bu_v_i32m1x6 (int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6bu_v_u32m1x6 (uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7bu_v_i32m1x7 (int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7bu_v_u32m1x7 (uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8bu_v_i32m1x8 (int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8bu_v_u32m1x8 (uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2bu_v_i32m2x2 (int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2bu_v_u32m2x2 (uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3bu_v_i32m2x3 (int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3bu_v_u32m2x3 (uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4bu_v_i32m2x4 (int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4bu_v_u32m2x4 (uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2bu_v_i32m4x2 (int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2bu_v_u32m4x2 (uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2bu_v_i64m1x2 (int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2bu_v_u64m1x2 (uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3bu_v_i64m1x3 (int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3bu_v_u64m1x3 (uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4bu_v_i64m1x4 (int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4bu_v_u64m1x4 (uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5bu_v_i64m1x5 (int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5bu_v_u64m1x5 (uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6bu_v_i64m1x6 (int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6bu_v_u64m1x6 (uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7bu_v_i64m1x7 (int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7bu_v_u64m1x7 (uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8bu_v_i64m1x8 (int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8bu_v_u64m1x8 (uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2bu_v_i64m2x2 (int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2bu_v_u64m2x2 (uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3bu_v_i64m2x3 (int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3bu_v_u64m2x3 (uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4bu_v_i64m2x4 (int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4bu_v_u64m2x4 (uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2bu_v_i64m4x2 (int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2bu_v_u64m4x2 (uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2bu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2bu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3bu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3bu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4bu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4bu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5bu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5bu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6bu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6bu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7bu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7bu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8bu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8bu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2bu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2bu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3bu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3bu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4bu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4bu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2bu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2bu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2bu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2bu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3bu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3bu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4bu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4bu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5bu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5bu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6bu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6bu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7bu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7bu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8bu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8bu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2bu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2bu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3bu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3bu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4bu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4bu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2bu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2bu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2bu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2bu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3bu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3bu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4bu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4bu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5bu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5bu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6bu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6bu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7bu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7bu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8bu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8bu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2bu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2bu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3bu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3bu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4bu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4bu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2bu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2bu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2bu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2bu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3bu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3bu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4bu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4bu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5bu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5bu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6bu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6bu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7bu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7bu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8bu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8bu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2bu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2bu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3bu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3bu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4bu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4bu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2bu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2bu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2h_v_i8m1x2 (int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2h_v_u8m1x2 (uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3h_v_i8m1x3 (int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3h_v_u8m1x3 (uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4h_v_i8m1x4 (int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4h_v_u8m1x4 (uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5h_v_i8m1x5 (int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5h_v_u8m1x5 (uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6h_v_i8m1x6 (int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6h_v_u8m1x6 (uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7h_v_i8m1x7 (int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7h_v_u8m1x7 (uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8h_v_i8m1x8 (int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8h_v_u8m1x8 (uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2h_v_i8m2x2 (int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2h_v_u8m2x2 (uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3h_v_i8m2x3 (int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3h_v_u8m2x3 (uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4h_v_i8m2x4 (int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4h_v_u8m2x4 (uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2h_v_i8m4x2 (int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2h_v_u8m4x2 (uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2h_v_i16m1x2 (int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2h_v_u16m1x2 (uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3h_v_i16m1x3 (int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3h_v_u16m1x3 (uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4h_v_i16m1x4 (int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4h_v_u16m1x4 (uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5h_v_i16m1x5 (int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5h_v_u16m1x5 (uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6h_v_i16m1x6 (int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6h_v_u16m1x6 (uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7h_v_i16m1x7 (int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7h_v_u16m1x7 (uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8h_v_i16m1x8 (int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8h_v_u16m1x8 (uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2h_v_i16m2x2 (int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2h_v_u16m2x2 (uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3h_v_i16m2x3 (int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3h_v_u16m2x3 (uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4h_v_i16m2x4 (int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4h_v_u16m2x4 (uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2h_v_i16m4x2 (int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2h_v_u16m4x2 (uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2h_v_i32m1x2 (int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2h_v_u32m1x2 (uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3h_v_i32m1x3 (int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3h_v_u32m1x3 (uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4h_v_i32m1x4 (int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4h_v_u32m1x4 (uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5h_v_i32m1x5 (int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5h_v_u32m1x5 (uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6h_v_i32m1x6 (int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6h_v_u32m1x6 (uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7h_v_i32m1x7 (int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7h_v_u32m1x7 (uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8h_v_i32m1x8 (int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8h_v_u32m1x8 (uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2h_v_i32m2x2 (int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2h_v_u32m2x2 (uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3h_v_i32m2x3 (int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3h_v_u32m2x3 (uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4h_v_i32m2x4 (int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4h_v_u32m2x4 (uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2h_v_i32m4x2 (int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2h_v_u32m4x2 (uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2h_v_i64m1x2 (int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2h_v_u64m1x2 (uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3h_v_i64m1x3 (int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3h_v_u64m1x3 (uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4h_v_i64m1x4 (int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4h_v_u64m1x4 (uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5h_v_i64m1x5 (int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5h_v_u64m1x5 (uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6h_v_i64m1x6 (int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6h_v_u64m1x6 (uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7h_v_i64m1x7 (int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7h_v_u64m1x7 (uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8h_v_i64m1x8 (int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8h_v_u64m1x8 (uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2h_v_i64m2x2 (int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2h_v_u64m2x2 (uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3h_v_i64m2x3 (int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3h_v_u64m2x3 (uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4h_v_i64m2x4 (int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4h_v_u64m2x4 (uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2h_v_i64m4x2 (int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2h_v_u64m4x2 (uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2h_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2h_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3h_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3h_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4h_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4h_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5h_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5h_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6h_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6h_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7h_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7h_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8h_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8h_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2h_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2h_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3h_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3h_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4h_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4h_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2h_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2h_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2h_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2h_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3h_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3h_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4h_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4h_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5h_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5h_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6h_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6h_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7h_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7h_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8h_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8h_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2h_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2h_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3h_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3h_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4h_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4h_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2h_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2h_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2h_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2h_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3h_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3h_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4h_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4h_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5h_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5h_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6h_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6h_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7h_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7h_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8h_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8h_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2h_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2h_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3h_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3h_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4h_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4h_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2h_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2h_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2h_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2h_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3h_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3h_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4h_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4h_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5h_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5h_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6h_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6h_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7h_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7h_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8h_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8h_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2h_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2h_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3h_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3h_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4h_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4h_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2h_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2h_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2hu_v_i8m1x2 (int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2hu_v_u8m1x2 (uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3hu_v_i8m1x3 (int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3hu_v_u8m1x3 (uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4hu_v_i8m1x4 (int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4hu_v_u8m1x4 (uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5hu_v_i8m1x5 (int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5hu_v_u8m1x5 (uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6hu_v_i8m1x6 (int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6hu_v_u8m1x6 (uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7hu_v_i8m1x7 (int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7hu_v_u8m1x7 (uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8hu_v_i8m1x8 (int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8hu_v_u8m1x8 (uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2hu_v_i8m2x2 (int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2hu_v_u8m2x2 (uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3hu_v_i8m2x3 (int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3hu_v_u8m2x3 (uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4hu_v_i8m2x4 (int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4hu_v_u8m2x4 (uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2hu_v_i8m4x2 (int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2hu_v_u8m4x2 (uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2hu_v_i16m1x2 (int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2hu_v_u16m1x2 (uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3hu_v_i16m1x3 (int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3hu_v_u16m1x3 (uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4hu_v_i16m1x4 (int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4hu_v_u16m1x4 (uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5hu_v_i16m1x5 (int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5hu_v_u16m1x5 (uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6hu_v_i16m1x6 (int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6hu_v_u16m1x6 (uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7hu_v_i16m1x7 (int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7hu_v_u16m1x7 (uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8hu_v_i16m1x8 (int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8hu_v_u16m1x8 (uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2hu_v_i16m2x2 (int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2hu_v_u16m2x2 (uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3hu_v_i16m2x3 (int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3hu_v_u16m2x3 (uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4hu_v_i16m2x4 (int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4hu_v_u16m2x4 (uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2hu_v_i16m4x2 (int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2hu_v_u16m4x2 (uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2hu_v_i32m1x2 (int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2hu_v_u32m1x2 (uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3hu_v_i32m1x3 (int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3hu_v_u32m1x3 (uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4hu_v_i32m1x4 (int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4hu_v_u32m1x4 (uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5hu_v_i32m1x5 (int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5hu_v_u32m1x5 (uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6hu_v_i32m1x6 (int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6hu_v_u32m1x6 (uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7hu_v_i32m1x7 (int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7hu_v_u32m1x7 (uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8hu_v_i32m1x8 (int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8hu_v_u32m1x8 (uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2hu_v_i32m2x2 (int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2hu_v_u32m2x2 (uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3hu_v_i32m2x3 (int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3hu_v_u32m2x3 (uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4hu_v_i32m2x4 (int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4hu_v_u32m2x4 (uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2hu_v_i32m4x2 (int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2hu_v_u32m4x2 (uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2hu_v_i64m1x2 (int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2hu_v_u64m1x2 (uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3hu_v_i64m1x3 (int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3hu_v_u64m1x3 (uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4hu_v_i64m1x4 (int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4hu_v_u64m1x4 (uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5hu_v_i64m1x5 (int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5hu_v_u64m1x5 (uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6hu_v_i64m1x6 (int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6hu_v_u64m1x6 (uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7hu_v_i64m1x7 (int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7hu_v_u64m1x7 (uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8hu_v_i64m1x8 (int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8hu_v_u64m1x8 (uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2hu_v_i64m2x2 (int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2hu_v_u64m2x2 (uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3hu_v_i64m2x3 (int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3hu_v_u64m2x3 (uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4hu_v_i64m2x4 (int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4hu_v_u64m2x4 (uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2hu_v_i64m4x2 (int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2hu_v_u64m4x2 (uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2hu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2hu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3hu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3hu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4hu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4hu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5hu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5hu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6hu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6hu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7hu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7hu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8hu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8hu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2hu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2hu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3hu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3hu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4hu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4hu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2hu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2hu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2hu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2hu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3hu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3hu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4hu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4hu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5hu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5hu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6hu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6hu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7hu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7hu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8hu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8hu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2hu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2hu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3hu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3hu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4hu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4hu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2hu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2hu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2hu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2hu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3hu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3hu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4hu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4hu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5hu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5hu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6hu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6hu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7hu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7hu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8hu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8hu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2hu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2hu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3hu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3hu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4hu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4hu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2hu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2hu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2hu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2hu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3hu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3hu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4hu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4hu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5hu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5hu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6hu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6hu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7hu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7hu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8hu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8hu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2hu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2hu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3hu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3hu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4hu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4hu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2hu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2hu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2w_v_i8m1x2 (int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2w_v_u8m1x2 (uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3w_v_i8m1x3 (int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3w_v_u8m1x3 (uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4w_v_i8m1x4 (int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4w_v_u8m1x4 (uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5w_v_i8m1x5 (int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5w_v_u8m1x5 (uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6w_v_i8m1x6 (int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6w_v_u8m1x6 (uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7w_v_i8m1x7 (int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7w_v_u8m1x7 (uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8w_v_i8m1x8 (int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8w_v_u8m1x8 (uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2w_v_i8m2x2 (int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2w_v_u8m2x2 (uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3w_v_i8m2x3 (int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3w_v_u8m2x3 (uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4w_v_i8m2x4 (int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4w_v_u8m2x4 (uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2w_v_i8m4x2 (int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2w_v_u8m4x2 (uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2w_v_i16m1x2 (int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2w_v_u16m1x2 (uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3w_v_i16m1x3 (int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3w_v_u16m1x3 (uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4w_v_i16m1x4 (int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4w_v_u16m1x4 (uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5w_v_i16m1x5 (int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5w_v_u16m1x5 (uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6w_v_i16m1x6 (int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6w_v_u16m1x6 (uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7w_v_i16m1x7 (int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7w_v_u16m1x7 (uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8w_v_i16m1x8 (int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8w_v_u16m1x8 (uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2w_v_i16m2x2 (int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2w_v_u16m2x2 (uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3w_v_i16m2x3 (int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3w_v_u16m2x3 (uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4w_v_i16m2x4 (int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4w_v_u16m2x4 (uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2w_v_i16m4x2 (int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2w_v_u16m4x2 (uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2w_v_i32m1x2 (int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2w_v_u32m1x2 (uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3w_v_i32m1x3 (int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3w_v_u32m1x3 (uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4w_v_i32m1x4 (int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4w_v_u32m1x4 (uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5w_v_i32m1x5 (int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5w_v_u32m1x5 (uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6w_v_i32m1x6 (int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6w_v_u32m1x6 (uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7w_v_i32m1x7 (int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7w_v_u32m1x7 (uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8w_v_i32m1x8 (int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8w_v_u32m1x8 (uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2w_v_i32m2x2 (int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2w_v_u32m2x2 (uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3w_v_i32m2x3 (int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3w_v_u32m2x3 (uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4w_v_i32m2x4 (int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4w_v_u32m2x4 (uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2w_v_i32m4x2 (int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2w_v_u32m4x2 (uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2w_v_i64m1x2 (int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2w_v_u64m1x2 (uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3w_v_i64m1x3 (int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3w_v_u64m1x3 (uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4w_v_i64m1x4 (int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4w_v_u64m1x4 (uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5w_v_i64m1x5 (int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5w_v_u64m1x5 (uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6w_v_i64m1x6 (int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6w_v_u64m1x6 (uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7w_v_i64m1x7 (int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7w_v_u64m1x7 (uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8w_v_i64m1x8 (int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8w_v_u64m1x8 (uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2w_v_i64m2x2 (int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2w_v_u64m2x2 (uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3w_v_i64m2x3 (int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3w_v_u64m2x3 (uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4w_v_i64m2x4 (int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4w_v_u64m2x4 (uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2w_v_i64m4x2 (int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2w_v_u64m4x2 (uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2w_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2w_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3w_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3w_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4w_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4w_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5w_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5w_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6w_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6w_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7w_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7w_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8w_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8w_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2w_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2w_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3w_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3w_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4w_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4w_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2w_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2w_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2w_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2w_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3w_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3w_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4w_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4w_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5w_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5w_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6w_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6w_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7w_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7w_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8w_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8w_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2w_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2w_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3w_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3w_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4w_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4w_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2w_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2w_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2w_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2w_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3w_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3w_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4w_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4w_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5w_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5w_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6w_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6w_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7w_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7w_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8w_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8w_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2w_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2w_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3w_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3w_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4w_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4w_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2w_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2w_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2w_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2w_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3w_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3w_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4w_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4w_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5w_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5w_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6w_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6w_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7w_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7w_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8w_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8w_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2w_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2w_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3w_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3w_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4w_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4w_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2w_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2w_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2wu_v_i8m1x2 (int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2wu_v_u8m1x2 (uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3wu_v_i8m1x3 (int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3wu_v_u8m1x3 (uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4wu_v_i8m1x4 (int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4wu_v_u8m1x4 (uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5wu_v_i8m1x5 (int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5wu_v_u8m1x5 (uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6wu_v_i8m1x6 (int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6wu_v_u8m1x6 (uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7wu_v_i8m1x7 (int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7wu_v_u8m1x7 (uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8wu_v_i8m1x8 (int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8wu_v_u8m1x8 (uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2wu_v_i8m2x2 (int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2wu_v_u8m2x2 (uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3wu_v_i8m2x3 (int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3wu_v_u8m2x3 (uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4wu_v_i8m2x4 (int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4wu_v_u8m2x4 (uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2wu_v_i8m4x2 (int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2wu_v_u8m4x2 (uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2wu_v_i16m1x2 (int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2wu_v_u16m1x2 (uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3wu_v_i16m1x3 (int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3wu_v_u16m1x3 (uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4wu_v_i16m1x4 (int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4wu_v_u16m1x4 (uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5wu_v_i16m1x5 (int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5wu_v_u16m1x5 (uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6wu_v_i16m1x6 (int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6wu_v_u16m1x6 (uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7wu_v_i16m1x7 (int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7wu_v_u16m1x7 (uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8wu_v_i16m1x8 (int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8wu_v_u16m1x8 (uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2wu_v_i16m2x2 (int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2wu_v_u16m2x2 (uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3wu_v_i16m2x3 (int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3wu_v_u16m2x3 (uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4wu_v_i16m2x4 (int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4wu_v_u16m2x4 (uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2wu_v_i16m4x2 (int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2wu_v_u16m4x2 (uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2wu_v_i32m1x2 (int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2wu_v_u32m1x2 (uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3wu_v_i32m1x3 (int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3wu_v_u32m1x3 (uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4wu_v_i32m1x4 (int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4wu_v_u32m1x4 (uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5wu_v_i32m1x5 (int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5wu_v_u32m1x5 (uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6wu_v_i32m1x6 (int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6wu_v_u32m1x6 (uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7wu_v_i32m1x7 (int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7wu_v_u32m1x7 (uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8wu_v_i32m1x8 (int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8wu_v_u32m1x8 (uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2wu_v_i32m2x2 (int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2wu_v_u32m2x2 (uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3wu_v_i32m2x3 (int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3wu_v_u32m2x3 (uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4wu_v_i32m2x4 (int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4wu_v_u32m2x4 (uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2wu_v_i32m4x2 (int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2wu_v_u32m4x2 (uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2wu_v_i64m1x2 (int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2wu_v_u64m1x2 (uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3wu_v_i64m1x3 (int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3wu_v_u64m1x3 (uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4wu_v_i64m1x4 (int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4wu_v_u64m1x4 (uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5wu_v_i64m1x5 (int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5wu_v_u64m1x5 (uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6wu_v_i64m1x6 (int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6wu_v_u64m1x6 (uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7wu_v_i64m1x7 (int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7wu_v_u64m1x7 (uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8wu_v_i64m1x8 (int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8wu_v_u64m1x8 (uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2wu_v_i64m2x2 (int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2wu_v_u64m2x2 (uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3wu_v_i64m2x3 (int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3wu_v_u64m2x3 (uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4wu_v_i64m2x4 (int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4wu_v_u64m2x4 (uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2wu_v_i64m4x2 (int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2wu_v_u64m4x2 (uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2wu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2wu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3wu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3wu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4wu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4wu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5wu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5wu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6wu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6wu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7wu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7wu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8wu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8wu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2wu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2wu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3wu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3wu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4wu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4wu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2wu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2wu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2wu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2wu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3wu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3wu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4wu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4wu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5wu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5wu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6wu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6wu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7wu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7wu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8wu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8wu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2wu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2wu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3wu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3wu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4wu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4wu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2wu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2wu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2wu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2wu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3wu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3wu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4wu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4wu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5wu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5wu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6wu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6wu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7wu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7wu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8wu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8wu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2wu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2wu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3wu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3wu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4wu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4wu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2wu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2wu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2wu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2wu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3wu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3wu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4wu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4wu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5wu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5wu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6wu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6wu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7wu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7wu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8wu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8wu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2wu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2wu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3wu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3wu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4wu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4wu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2wu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2wu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t vl);
// masked functions
vint8m1x2_t __riscv_th_vlseg2b_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2b_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3b_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3b_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4b_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4b_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5b_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5b_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6b_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6b_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7b_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7b_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8b_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8b_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2b_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2b_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3b_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3b_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4b_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4b_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2b_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2b_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2b_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2b_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3b_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3b_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4b_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4b_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5b_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5b_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6b_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6b_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7b_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7b_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8b_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8b_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2b_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2b_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3b_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3b_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4b_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4b_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2b_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2b_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2b_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2b_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3b_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3b_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4b_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4b_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5b_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5b_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6b_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6b_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7b_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7b_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8b_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8b_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2b_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2b_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3b_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3b_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4b_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4b_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2b_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2b_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2b_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2b_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3b_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3b_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4b_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4b_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5b_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5b_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6b_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6b_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7b_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7b_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8b_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8b_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2b_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2b_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3b_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3b_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4b_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4b_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2b_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2b_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2b_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2b_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3b_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3b_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4b_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4b_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5b_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5b_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6b_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6b_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7b_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7b_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8b_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8b_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2b_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2b_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3b_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3b_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4b_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4b_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2b_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2b_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2b_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2b_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3b_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3b_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4b_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4b_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5b_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5b_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6b_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6b_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7b_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7b_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8b_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8b_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2b_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2b_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3b_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3b_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4b_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4b_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2b_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2b_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2b_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2b_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3b_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3b_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4b_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4b_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5b_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5b_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6b_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6b_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7b_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7b_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8b_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8b_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2b_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2b_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3b_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3b_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4b_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4b_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2b_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2b_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2b_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2b_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3b_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3b_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4b_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4b_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5b_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5b_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6b_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6b_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7b_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7b_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8b_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8b_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2b_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2b_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3b_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3b_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4b_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4b_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2b_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2b_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2b_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2b_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3b_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3b_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4b_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4b_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5b_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5b_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6b_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6b_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7b_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7b_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8b_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8b_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2b_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2b_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3b_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3b_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4b_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4b_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2b_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2b_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2b_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2b_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3b_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3b_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4b_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4b_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5b_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5b_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6b_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6b_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7b_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7b_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8b_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8b_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2b_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2b_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3b_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3b_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4b_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4b_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2b_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2b_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2b_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2b_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3b_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3b_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4b_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4b_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5b_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5b_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6b_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6b_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7b_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7b_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8b_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8b_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2b_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2b_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3b_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3b_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4b_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4b_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2b_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2b_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2b_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2b_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3b_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3b_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4b_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4b_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5b_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5b_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6b_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6b_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7b_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7b_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8b_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8b_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2b_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2b_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3b_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3b_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4b_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4b_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2b_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2b_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2b_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2b_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3b_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3b_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4b_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4b_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5b_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5b_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6b_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6b_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7b_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7b_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8b_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8b_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2b_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2b_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3b_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3b_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4b_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4b_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2b_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2b_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2b_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2b_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3b_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3b_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4b_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4b_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5b_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5b_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6b_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6b_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7b_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7b_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8b_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8b_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2b_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2b_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3b_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3b_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4b_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4b_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2b_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2b_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2b_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2b_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3b_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3b_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4b_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4b_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5b_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5b_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6b_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6b_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7b_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7b_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8b_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8b_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2b_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2b_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3b_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3b_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4b_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4b_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2b_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2b_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2b_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2b_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3b_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3b_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4b_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4b_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5b_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5b_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6b_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6b_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7b_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7b_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8b_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8b_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2b_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2b_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3b_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3b_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4b_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4b_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2b_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2b_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2bu_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2bu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3bu_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3bu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4bu_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4bu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5bu_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5bu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6bu_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6bu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7bu_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7bu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8bu_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8bu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2bu_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2bu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3bu_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3bu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4bu_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4bu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2bu_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2bu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2bu_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2bu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3bu_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3bu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4bu_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4bu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5bu_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5bu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6bu_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6bu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7bu_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7bu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8bu_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8bu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2bu_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2bu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3bu_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3bu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4bu_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4bu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2bu_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2bu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2bu_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2bu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3bu_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3bu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4bu_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4bu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5bu_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5bu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6bu_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6bu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7bu_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7bu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8bu_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8bu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2bu_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2bu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3bu_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3bu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4bu_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4bu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2bu_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2bu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2bu_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2bu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3bu_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3bu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4bu_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4bu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5bu_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5bu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6bu_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6bu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7bu_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7bu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8bu_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8bu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2bu_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2bu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3bu_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3bu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4bu_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4bu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2bu_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2bu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2bu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2bu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3bu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3bu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4bu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4bu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5bu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5bu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6bu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6bu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7bu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7bu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8bu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8bu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2bu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2bu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3bu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3bu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4bu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4bu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2bu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2bu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2bu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2bu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3bu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3bu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4bu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4bu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5bu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5bu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6bu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6bu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7bu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7bu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8bu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8bu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2bu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2bu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3bu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3bu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4bu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4bu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2bu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2bu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2bu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2bu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3bu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3bu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4bu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4bu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5bu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5bu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6bu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6bu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7bu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7bu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8bu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8bu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2bu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2bu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3bu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3bu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4bu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4bu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2bu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2bu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2bu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2bu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3bu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3bu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4bu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4bu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5bu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5bu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6bu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6bu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7bu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7bu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8bu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8bu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2bu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2bu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3bu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3bu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4bu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4bu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2bu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2bu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2bu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2bu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3bu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3bu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4bu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4bu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5bu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5bu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6bu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6bu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7bu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7bu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8bu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8bu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2bu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2bu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3bu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3bu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4bu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4bu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2bu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2bu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2bu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2bu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3bu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3bu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4bu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4bu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5bu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5bu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6bu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6bu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7bu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7bu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8bu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8bu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2bu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2bu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3bu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3bu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4bu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4bu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2bu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2bu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2bu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2bu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3bu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3bu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4bu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4bu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5bu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5bu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6bu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6bu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7bu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7bu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8bu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8bu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2bu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2bu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3bu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3bu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4bu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4bu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2bu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2bu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2bu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2bu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3bu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3bu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4bu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4bu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5bu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5bu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6bu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6bu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7bu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7bu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8bu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8bu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2bu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2bu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3bu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3bu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4bu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4bu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2bu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2bu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2bu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2bu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3bu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3bu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4bu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4bu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5bu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5bu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6bu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6bu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7bu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7bu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8bu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8bu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2bu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2bu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3bu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3bu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4bu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4bu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2bu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2bu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2bu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2bu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3bu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3bu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4bu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4bu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5bu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5bu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6bu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6bu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7bu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7bu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8bu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8bu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2bu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2bu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3bu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3bu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4bu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4bu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2bu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2bu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2bu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2bu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3bu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3bu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4bu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4bu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5bu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5bu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6bu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6bu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7bu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7bu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8bu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8bu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2bu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2bu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3bu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3bu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4bu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4bu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2bu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2bu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2bu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2bu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3bu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3bu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4bu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4bu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5bu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5bu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6bu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6bu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7bu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7bu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8bu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8bu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2bu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2bu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3bu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3bu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4bu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4bu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2bu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2bu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2h_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2h_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3h_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3h_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4h_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4h_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5h_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5h_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6h_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6h_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7h_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7h_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8h_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8h_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2h_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2h_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3h_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3h_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4h_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4h_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2h_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2h_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2h_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2h_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3h_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3h_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4h_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4h_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5h_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5h_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6h_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6h_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7h_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7h_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8h_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8h_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2h_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2h_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3h_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3h_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4h_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4h_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2h_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2h_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2h_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2h_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3h_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3h_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4h_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4h_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5h_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5h_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6h_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6h_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7h_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7h_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8h_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8h_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2h_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2h_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3h_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3h_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4h_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4h_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2h_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2h_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2h_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2h_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3h_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3h_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4h_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4h_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5h_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5h_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6h_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6h_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7h_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7h_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8h_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8h_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2h_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2h_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3h_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3h_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4h_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4h_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2h_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2h_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2h_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2h_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3h_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3h_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4h_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4h_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5h_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5h_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6h_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6h_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7h_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7h_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8h_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8h_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2h_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2h_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3h_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3h_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4h_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4h_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2h_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2h_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2h_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2h_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3h_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3h_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4h_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4h_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5h_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5h_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6h_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6h_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7h_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7h_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8h_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8h_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2h_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2h_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3h_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3h_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4h_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4h_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2h_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2h_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2h_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2h_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3h_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3h_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4h_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4h_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5h_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5h_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6h_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6h_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7h_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7h_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8h_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8h_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2h_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2h_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3h_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3h_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4h_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4h_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2h_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2h_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2h_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2h_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3h_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3h_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4h_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4h_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5h_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5h_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6h_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6h_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7h_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7h_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8h_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8h_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2h_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2h_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3h_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3h_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4h_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4h_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2h_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2h_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2h_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2h_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3h_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3h_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4h_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4h_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5h_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5h_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6h_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6h_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7h_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7h_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8h_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8h_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2h_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2h_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3h_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3h_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4h_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4h_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2h_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2h_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2h_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2h_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3h_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3h_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4h_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4h_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5h_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5h_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6h_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6h_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7h_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7h_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8h_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8h_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2h_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2h_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3h_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3h_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4h_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4h_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2h_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2h_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2h_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2h_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3h_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3h_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4h_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4h_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5h_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5h_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6h_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6h_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7h_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7h_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8h_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8h_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2h_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2h_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3h_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3h_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4h_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4h_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2h_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2h_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2h_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2h_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3h_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3h_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4h_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4h_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5h_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5h_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6h_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6h_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7h_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7h_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8h_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8h_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2h_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2h_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3h_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3h_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4h_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4h_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2h_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2h_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2h_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2h_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3h_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3h_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4h_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4h_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5h_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5h_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6h_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6h_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7h_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7h_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8h_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8h_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2h_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2h_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3h_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3h_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4h_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4h_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2h_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2h_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2h_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2h_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3h_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3h_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4h_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4h_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5h_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5h_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6h_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6h_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7h_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7h_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8h_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8h_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2h_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2h_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3h_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3h_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4h_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4h_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2h_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2h_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2h_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2h_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3h_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3h_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4h_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4h_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5h_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5h_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6h_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6h_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7h_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7h_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8h_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8h_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2h_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2h_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3h_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3h_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4h_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4h_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2h_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2h_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2h_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2h_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3h_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3h_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4h_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4h_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5h_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5h_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6h_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6h_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7h_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7h_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8h_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8h_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2h_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2h_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3h_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3h_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4h_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4h_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2h_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2h_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2hu_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2hu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3hu_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3hu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4hu_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4hu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5hu_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5hu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6hu_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6hu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7hu_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7hu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8hu_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8hu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2hu_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2hu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3hu_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3hu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4hu_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4hu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2hu_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2hu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2hu_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2hu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3hu_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3hu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4hu_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4hu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5hu_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5hu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6hu_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6hu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7hu_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7hu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8hu_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8hu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2hu_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2hu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3hu_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3hu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4hu_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4hu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2hu_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2hu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2hu_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2hu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3hu_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3hu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4hu_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4hu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5hu_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5hu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6hu_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6hu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7hu_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7hu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8hu_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8hu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2hu_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2hu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3hu_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3hu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4hu_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4hu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2hu_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2hu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2hu_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2hu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3hu_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3hu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4hu_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4hu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5hu_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5hu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6hu_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6hu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7hu_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7hu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8hu_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8hu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2hu_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2hu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3hu_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3hu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4hu_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4hu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2hu_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2hu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2hu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2hu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3hu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3hu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4hu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4hu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5hu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5hu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6hu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6hu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7hu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7hu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8hu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8hu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2hu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2hu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3hu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3hu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4hu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4hu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2hu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2hu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2hu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2hu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3hu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3hu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4hu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4hu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5hu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5hu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6hu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6hu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7hu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7hu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8hu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8hu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2hu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2hu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3hu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3hu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4hu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4hu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2hu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2hu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2hu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2hu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3hu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3hu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4hu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4hu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5hu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5hu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6hu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6hu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7hu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7hu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8hu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8hu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2hu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2hu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3hu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3hu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4hu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4hu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2hu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2hu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2hu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2hu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3hu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3hu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4hu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4hu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5hu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5hu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6hu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6hu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7hu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7hu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8hu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8hu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2hu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2hu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3hu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3hu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4hu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4hu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2hu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2hu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2hu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2hu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3hu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3hu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4hu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4hu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5hu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5hu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6hu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6hu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7hu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7hu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8hu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8hu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2hu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2hu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3hu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3hu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4hu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4hu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2hu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2hu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2hu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2hu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3hu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3hu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4hu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4hu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5hu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5hu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6hu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6hu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7hu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7hu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8hu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8hu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2hu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2hu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3hu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3hu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4hu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4hu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2hu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2hu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2hu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2hu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3hu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3hu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4hu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4hu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5hu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5hu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6hu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6hu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7hu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7hu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8hu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8hu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2hu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2hu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3hu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3hu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4hu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4hu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2hu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2hu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2hu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2hu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3hu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3hu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4hu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4hu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5hu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5hu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6hu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6hu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7hu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7hu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8hu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8hu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2hu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2hu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3hu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3hu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4hu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4hu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2hu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2hu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2hu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2hu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3hu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3hu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4hu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4hu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5hu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5hu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6hu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6hu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7hu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7hu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8hu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8hu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2hu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2hu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3hu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3hu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4hu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4hu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2hu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2hu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2hu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2hu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3hu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3hu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4hu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4hu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5hu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5hu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6hu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6hu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7hu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7hu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8hu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8hu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2hu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2hu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3hu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3hu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4hu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4hu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2hu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2hu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2hu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2hu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3hu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3hu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4hu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4hu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5hu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5hu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6hu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6hu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7hu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7hu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8hu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8hu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2hu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2hu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3hu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3hu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4hu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4hu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2hu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2hu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2hu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2hu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3hu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3hu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4hu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4hu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5hu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5hu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6hu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6hu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7hu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7hu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8hu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8hu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2hu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2hu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3hu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3hu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4hu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4hu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2hu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2hu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2w_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2w_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3w_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3w_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4w_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4w_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5w_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5w_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6w_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6w_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7w_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7w_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8w_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8w_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2w_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2w_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3w_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3w_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4w_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4w_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2w_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2w_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2w_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2w_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3w_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3w_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4w_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4w_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5w_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5w_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6w_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6w_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7w_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7w_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8w_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8w_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2w_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2w_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3w_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3w_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4w_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4w_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2w_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2w_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2w_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2w_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3w_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3w_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4w_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4w_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5w_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5w_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6w_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6w_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7w_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7w_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8w_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8w_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2w_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2w_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3w_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3w_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4w_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4w_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2w_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2w_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2w_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2w_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3w_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3w_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4w_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4w_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5w_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5w_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6w_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6w_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7w_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7w_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8w_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8w_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2w_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2w_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3w_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3w_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4w_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4w_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2w_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2w_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2w_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2w_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3w_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3w_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4w_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4w_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5w_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5w_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6w_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6w_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7w_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7w_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8w_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8w_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2w_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2w_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3w_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3w_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4w_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4w_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2w_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2w_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2w_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2w_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3w_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3w_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4w_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4w_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5w_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5w_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6w_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6w_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7w_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7w_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8w_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8w_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2w_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2w_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3w_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3w_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4w_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4w_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2w_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2w_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2w_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2w_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3w_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3w_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4w_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4w_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5w_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5w_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6w_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6w_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7w_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7w_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8w_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8w_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2w_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2w_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3w_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3w_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4w_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4w_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2w_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2w_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2w_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2w_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3w_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3w_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4w_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4w_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5w_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5w_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6w_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6w_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7w_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7w_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8w_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8w_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2w_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2w_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3w_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3w_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4w_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4w_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2w_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2w_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2w_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2w_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3w_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3w_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4w_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4w_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5w_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5w_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6w_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6w_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7w_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7w_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8w_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8w_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2w_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2w_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3w_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3w_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4w_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4w_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2w_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2w_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2w_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2w_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3w_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3w_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4w_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4w_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5w_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5w_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6w_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6w_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7w_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7w_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8w_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8w_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2w_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2w_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3w_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3w_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4w_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4w_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2w_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2w_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2w_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2w_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3w_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3w_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4w_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4w_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5w_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5w_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6w_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6w_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7w_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7w_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8w_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8w_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2w_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2w_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3w_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3w_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4w_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4w_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2w_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2w_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2w_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2w_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3w_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3w_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4w_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4w_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5w_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5w_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6w_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6w_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7w_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7w_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8w_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8w_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2w_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2w_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3w_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3w_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4w_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4w_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2w_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2w_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2w_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2w_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3w_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3w_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4w_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4w_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5w_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5w_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6w_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6w_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7w_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7w_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8w_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8w_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2w_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2w_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3w_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3w_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4w_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4w_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2w_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2w_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2w_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2w_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3w_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3w_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4w_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4w_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5w_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5w_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6w_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6w_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7w_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7w_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8w_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8w_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2w_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2w_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3w_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3w_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4w_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4w_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2w_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2w_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2w_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2w_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3w_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3w_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4w_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4w_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5w_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5w_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6w_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6w_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7w_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7w_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8w_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8w_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2w_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2w_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3w_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3w_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4w_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4w_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2w_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2w_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2w_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2w_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3w_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3w_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4w_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4w_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5w_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5w_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6w_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6w_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7w_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7w_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8w_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8w_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2w_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2w_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3w_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3w_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4w_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4w_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2w_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2w_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2wu_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2wu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x3_t __riscv_th_vlseg3wu_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3wu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x4_t __riscv_th_vlseg4wu_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4wu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x5_t __riscv_th_vlseg5wu_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5wu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x6_t __riscv_th_vlseg6wu_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6wu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x7_t __riscv_th_vlseg7wu_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7wu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m1x8_t __riscv_th_vlseg8wu_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8wu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t vl);
vint8m2x2_t __riscv_th_vlseg2wu_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2wu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x3_t __riscv_th_vlseg3wu_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3wu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m2x4_t __riscv_th_vlseg4wu_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4wu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t vl);
vint8m4x2_t __riscv_th_vlseg2wu_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2wu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t vl);
vint16m1x2_t __riscv_th_vlseg2wu_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2wu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x3_t __riscv_th_vlseg3wu_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3wu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x4_t __riscv_th_vlseg4wu_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4wu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x5_t __riscv_th_vlseg5wu_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5wu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x6_t __riscv_th_vlseg6wu_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6wu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x7_t __riscv_th_vlseg7wu_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7wu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m1x8_t __riscv_th_vlseg8wu_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8wu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t vl);
vint16m2x2_t __riscv_th_vlseg2wu_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2wu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x3_t __riscv_th_vlseg3wu_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3wu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m2x4_t __riscv_th_vlseg4wu_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4wu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t vl);
vint16m4x2_t __riscv_th_vlseg2wu_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2wu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t vl);
vint32m1x2_t __riscv_th_vlseg2wu_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2wu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x3_t __riscv_th_vlseg3wu_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3wu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x4_t __riscv_th_vlseg4wu_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4wu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x5_t __riscv_th_vlseg5wu_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5wu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x6_t __riscv_th_vlseg6wu_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6wu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x7_t __riscv_th_vlseg7wu_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7wu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m1x8_t __riscv_th_vlseg8wu_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8wu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t vl);
vint32m2x2_t __riscv_th_vlseg2wu_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2wu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x3_t __riscv_th_vlseg3wu_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3wu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m2x4_t __riscv_th_vlseg4wu_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4wu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t vl);
vint32m4x2_t __riscv_th_vlseg2wu_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2wu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t vl);
vint64m1x2_t __riscv_th_vlseg2wu_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2wu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x3_t __riscv_th_vlseg3wu_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3wu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x4_t __riscv_th_vlseg4wu_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4wu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x5_t __riscv_th_vlseg5wu_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5wu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x6_t __riscv_th_vlseg6wu_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6wu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x7_t __riscv_th_vlseg7wu_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7wu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m1x8_t __riscv_th_vlseg8wu_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8wu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t vl);
vint64m2x2_t __riscv_th_vlseg2wu_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2wu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x3_t __riscv_th_vlseg3wu_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3wu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m2x4_t __riscv_th_vlseg4wu_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4wu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t vl);
vint64m4x2_t __riscv_th_vlseg2wu_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2wu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t vl);
vint8m1x2_t __riscv_th_vlseg2wu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2wu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3wu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3wu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4wu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4wu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5wu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5wu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6wu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6wu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7wu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7wu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8wu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8wu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2wu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2wu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3wu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3wu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4wu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4wu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2wu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2wu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2wu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2wu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3wu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3wu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4wu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4wu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5wu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5wu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6wu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6wu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7wu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7wu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8wu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8wu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2wu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2wu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3wu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3wu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4wu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4wu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2wu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2wu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2wu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2wu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3wu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3wu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4wu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4wu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5wu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5wu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6wu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6wu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7wu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7wu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8wu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8wu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2wu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2wu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3wu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3wu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4wu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4wu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2wu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2wu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2wu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2wu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3wu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3wu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4wu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4wu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5wu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5wu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6wu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6wu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7wu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7wu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8wu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8wu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2wu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2wu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3wu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3wu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4wu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4wu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2wu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2wu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2wu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2wu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3wu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3wu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4wu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4wu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5wu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5wu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6wu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6wu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7wu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7wu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8wu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8wu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2wu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2wu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3wu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3wu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4wu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4wu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2wu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2wu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2wu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2wu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3wu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3wu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4wu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4wu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5wu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5wu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6wu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6wu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7wu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7wu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8wu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8wu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2wu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2wu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3wu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3wu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4wu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4wu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2wu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2wu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2wu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2wu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3wu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3wu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4wu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4wu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5wu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5wu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6wu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6wu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7wu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7wu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8wu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8wu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2wu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2wu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3wu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3wu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4wu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4wu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2wu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2wu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2wu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2wu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3wu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3wu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4wu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4wu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5wu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5wu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6wu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6wu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7wu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7wu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8wu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8wu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2wu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2wu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3wu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3wu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4wu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4wu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2wu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2wu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
vint8m1x2_t __riscv_th_vlseg2wu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t vl);
vuint8m1x2_t __riscv_th_vlseg2wu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t vl);
vint8m1x3_t __riscv_th_vlseg3wu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t vl);
vuint8m1x3_t __riscv_th_vlseg3wu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t vl);
vint8m1x4_t __riscv_th_vlseg4wu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t vl);
vuint8m1x4_t __riscv_th_vlseg4wu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t vl);
vint8m1x5_t __riscv_th_vlseg5wu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t vl);
vuint8m1x5_t __riscv_th_vlseg5wu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t vl);
vint8m1x6_t __riscv_th_vlseg6wu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t vl);
vuint8m1x6_t __riscv_th_vlseg6wu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t vl);
vint8m1x7_t __riscv_th_vlseg7wu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t vl);
vuint8m1x7_t __riscv_th_vlseg7wu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t vl);
vint8m1x8_t __riscv_th_vlseg8wu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t vl);
vuint8m1x8_t __riscv_th_vlseg8wu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t vl);
vint8m2x2_t __riscv_th_vlseg2wu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t vl);
vuint8m2x2_t __riscv_th_vlseg2wu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t vl);
vint8m2x3_t __riscv_th_vlseg3wu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t vl);
vuint8m2x3_t __riscv_th_vlseg3wu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t vl);
vint8m2x4_t __riscv_th_vlseg4wu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t vl);
vuint8m2x4_t __riscv_th_vlseg4wu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t vl);
vint8m4x2_t __riscv_th_vlseg2wu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t vl);
vuint8m4x2_t __riscv_th_vlseg2wu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t vl);
vint16m1x2_t __riscv_th_vlseg2wu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t vl);
vuint16m1x2_t __riscv_th_vlseg2wu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t vl);
vint16m1x3_t __riscv_th_vlseg3wu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t vl);
vuint16m1x3_t __riscv_th_vlseg3wu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t vl);
vint16m1x4_t __riscv_th_vlseg4wu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t vl);
vuint16m1x4_t __riscv_th_vlseg4wu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t vl);
vint16m1x5_t __riscv_th_vlseg5wu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t vl);
vuint16m1x5_t __riscv_th_vlseg5wu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t vl);
vint16m1x6_t __riscv_th_vlseg6wu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t vl);
vuint16m1x6_t __riscv_th_vlseg6wu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t vl);
vint16m1x7_t __riscv_th_vlseg7wu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t vl);
vuint16m1x7_t __riscv_th_vlseg7wu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t vl);
vint16m1x8_t __riscv_th_vlseg8wu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t vl);
vuint16m1x8_t __riscv_th_vlseg8wu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t vl);
vint16m2x2_t __riscv_th_vlseg2wu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t vl);
vuint16m2x2_t __riscv_th_vlseg2wu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t vl);
vint16m2x3_t __riscv_th_vlseg3wu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t vl);
vuint16m2x3_t __riscv_th_vlseg3wu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t vl);
vint16m2x4_t __riscv_th_vlseg4wu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t vl);
vuint16m2x4_t __riscv_th_vlseg4wu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t vl);
vint16m4x2_t __riscv_th_vlseg2wu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t vl);
vuint16m4x2_t __riscv_th_vlseg2wu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t vl);
vint32m1x2_t __riscv_th_vlseg2wu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t vl);
vuint32m1x2_t __riscv_th_vlseg2wu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t vl);
vint32m1x3_t __riscv_th_vlseg3wu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t vl);
vuint32m1x3_t __riscv_th_vlseg3wu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t vl);
vint32m1x4_t __riscv_th_vlseg4wu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t vl);
vuint32m1x4_t __riscv_th_vlseg4wu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t vl);
vint32m1x5_t __riscv_th_vlseg5wu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t vl);
vuint32m1x5_t __riscv_th_vlseg5wu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t vl);
vint32m1x6_t __riscv_th_vlseg6wu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t vl);
vuint32m1x6_t __riscv_th_vlseg6wu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t vl);
vint32m1x7_t __riscv_th_vlseg7wu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t vl);
vuint32m1x7_t __riscv_th_vlseg7wu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t vl);
vint32m1x8_t __riscv_th_vlseg8wu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t vl);
vuint32m1x8_t __riscv_th_vlseg8wu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t vl);
vint32m2x2_t __riscv_th_vlseg2wu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t vl);
vuint32m2x2_t __riscv_th_vlseg2wu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t vl);
vint32m2x3_t __riscv_th_vlseg3wu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t vl);
vuint32m2x3_t __riscv_th_vlseg3wu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t vl);
vint32m2x4_t __riscv_th_vlseg4wu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t vl);
vuint32m2x4_t __riscv_th_vlseg4wu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t vl);
vint32m4x2_t __riscv_th_vlseg2wu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t vl);
vuint32m4x2_t __riscv_th_vlseg2wu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t vl);
vint64m1x2_t __riscv_th_vlseg2wu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t vl);
vuint64m1x2_t __riscv_th_vlseg2wu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t vl);
vint64m1x3_t __riscv_th_vlseg3wu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t vl);
vuint64m1x3_t __riscv_th_vlseg3wu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t vl);
vint64m1x4_t __riscv_th_vlseg4wu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t vl);
vuint64m1x4_t __riscv_th_vlseg4wu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t vl);
vint64m1x5_t __riscv_th_vlseg5wu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t vl);
vuint64m1x5_t __riscv_th_vlseg5wu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t vl);
vint64m1x6_t __riscv_th_vlseg6wu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t vl);
vuint64m1x6_t __riscv_th_vlseg6wu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t vl);
vint64m1x7_t __riscv_th_vlseg7wu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t vl);
vuint64m1x7_t __riscv_th_vlseg7wu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t vl);
vint64m1x8_t __riscv_th_vlseg8wu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t vl);
vuint64m1x8_t __riscv_th_vlseg8wu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t vl);
vint64m2x2_t __riscv_th_vlseg2wu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t vl);
vuint64m2x2_t __riscv_th_vlseg2wu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t vl);
vint64m2x3_t __riscv_th_vlseg3wu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t vl);
vuint64m2x3_t __riscv_th_vlseg3wu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t vl);
vint64m2x4_t __riscv_th_vlseg4wu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t vl);
vuint64m2x4_t __riscv_th_vlseg4wu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t vl);
vint64m4x2_t __riscv_th_vlseg2wu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t vl);
vuint64m4x2_t __riscv_th_vlseg2wu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t vl);
----

[[xtheadvector-unit-stride-segment-store]]
===== XTheadVector Unit-Stride Segment Store Intrinsics

[,c]
----
void __riscv_th_vsseg2b_v_i8m1x2 (int8_t *a, vint8m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u8m1x2 (uint8_t *a, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i8m1x3 (int8_t *a, vint8m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u8m1x3 (uint8_t *a, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i8m1x4 (int8_t *a, vint8m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u8m1x4 (uint8_t *a, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i8m1x5 (int8_t *a, vint8m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u8m1x5 (uint8_t *a, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i8m1x6 (int8_t *a, vint8m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u8m1x6 (uint8_t *a, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i8m1x7 (int8_t *a, vint8m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u8m1x7 (uint8_t *a, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i8m1x8 (int8_t *a, vint8m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u8m1x8 (uint8_t *a, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i8m2x2 (int8_t *a, vint8m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u8m2x2 (uint8_t *a, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i8m2x3 (int8_t *a, vint8m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u8m2x3 (uint8_t *a, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i8m2x4 (int8_t *a, vint8m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u8m2x4 (uint8_t *a, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i8m4x2 (int8_t *a, vint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u8m4x2 (uint8_t *a, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_i16m1x2 (int16_t *a, vint16m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u16m1x2 (uint16_t *a, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i16m1x3 (int16_t *a, vint16m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u16m1x3 (uint16_t *a, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i16m1x4 (int16_t *a, vint16m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u16m1x4 (uint16_t *a, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i16m1x5 (int16_t *a, vint16m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u16m1x5 (uint16_t *a, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i16m1x6 (int16_t *a, vint16m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u16m1x6 (uint16_t *a, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i16m1x7 (int16_t *a, vint16m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u16m1x7 (uint16_t *a, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i16m1x8 (int16_t *a, vint16m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u16m1x8 (uint16_t *a, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i16m2x2 (int16_t *a, vint16m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u16m2x2 (uint16_t *a, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i16m2x3 (int16_t *a, vint16m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u16m2x3 (uint16_t *a, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i16m2x4 (int16_t *a, vint16m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u16m2x4 (uint16_t *a, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i16m4x2 (int16_t *a, vint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u16m4x2 (uint16_t *a, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_i32m1x2 (int32_t *a, vint32m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u32m1x2 (uint32_t *a, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i32m1x3 (int32_t *a, vint32m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u32m1x3 (uint32_t *a, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i32m1x4 (int32_t *a, vint32m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u32m1x4 (uint32_t *a, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i32m1x5 (int32_t *a, vint32m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u32m1x5 (uint32_t *a, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i32m1x6 (int32_t *a, vint32m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u32m1x6 (uint32_t *a, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i32m1x7 (int32_t *a, vint32m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u32m1x7 (uint32_t *a, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i32m1x8 (int32_t *a, vint32m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u32m1x8 (uint32_t *a, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i32m2x2 (int32_t *a, vint32m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u32m2x2 (uint32_t *a, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i32m2x3 (int32_t *a, vint32m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u32m2x3 (uint32_t *a, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i32m2x4 (int32_t *a, vint32m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u32m2x4 (uint32_t *a, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i32m4x2 (int32_t *a, vint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u32m4x2 (uint32_t *a, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_i64m1x2 (int64_t *a, vint64m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u64m1x2 (uint64_t *a, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i64m1x3 (int64_t *a, vint64m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u64m1x3 (uint64_t *a, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i64m1x4 (int64_t *a, vint64m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u64m1x4 (uint64_t *a, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i64m1x5 (int64_t *a, vint64m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u64m1x5 (uint64_t *a, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i64m1x6 (int64_t *a, vint64m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u64m1x6 (uint64_t *a, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i64m1x7 (int64_t *a, vint64m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u64m1x7 (uint64_t *a, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i64m1x8 (int64_t *a, vint64m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u64m1x8 (uint64_t *a, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i64m2x2 (int64_t *a, vint64m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u64m2x2 (uint64_t *a, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i64m2x3 (int64_t *a, vint64m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u64m2x3 (uint64_t *a, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i64m2x4 (int64_t *a, vint64m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u64m2x4 (uint64_t *a, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i64m4x2 (int64_t *a, vint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u64m4x2 (uint64_t *a, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i8m1x2 (int8_t *a, vint8m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u8m1x2 (uint8_t *a, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i8m1x3 (int8_t *a, vint8m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u8m1x3 (uint8_t *a, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i8m1x4 (int8_t *a, vint8m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u8m1x4 (uint8_t *a, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i8m1x5 (int8_t *a, vint8m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u8m1x5 (uint8_t *a, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i8m1x6 (int8_t *a, vint8m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u8m1x6 (uint8_t *a, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i8m1x7 (int8_t *a, vint8m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u8m1x7 (uint8_t *a, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i8m1x8 (int8_t *a, vint8m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u8m1x8 (uint8_t *a, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i8m2x2 (int8_t *a, vint8m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u8m2x2 (uint8_t *a, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i8m2x3 (int8_t *a, vint8m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u8m2x3 (uint8_t *a, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i8m2x4 (int8_t *a, vint8m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u8m2x4 (uint8_t *a, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i8m4x2 (int8_t *a, vint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u8m4x2 (uint8_t *a, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i16m1x2 (int16_t *a, vint16m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u16m1x2 (uint16_t *a, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i16m1x3 (int16_t *a, vint16m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u16m1x3 (uint16_t *a, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i16m1x4 (int16_t *a, vint16m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u16m1x4 (uint16_t *a, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i16m1x5 (int16_t *a, vint16m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u16m1x5 (uint16_t *a, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i16m1x6 (int16_t *a, vint16m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u16m1x6 (uint16_t *a, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i16m1x7 (int16_t *a, vint16m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u16m1x7 (uint16_t *a, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i16m1x8 (int16_t *a, vint16m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u16m1x8 (uint16_t *a, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i16m2x2 (int16_t *a, vint16m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u16m2x2 (uint16_t *a, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i16m2x3 (int16_t *a, vint16m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u16m2x3 (uint16_t *a, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i16m2x4 (int16_t *a, vint16m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u16m2x4 (uint16_t *a, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i16m4x2 (int16_t *a, vint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u16m4x2 (uint16_t *a, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i32m1x2 (int32_t *a, vint32m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u32m1x2 (uint32_t *a, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i32m1x3 (int32_t *a, vint32m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u32m1x3 (uint32_t *a, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i32m1x4 (int32_t *a, vint32m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u32m1x4 (uint32_t *a, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i32m1x5 (int32_t *a, vint32m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u32m1x5 (uint32_t *a, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i32m1x6 (int32_t *a, vint32m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u32m1x6 (uint32_t *a, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i32m1x7 (int32_t *a, vint32m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u32m1x7 (uint32_t *a, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i32m1x8 (int32_t *a, vint32m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u32m1x8 (uint32_t *a, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i32m2x2 (int32_t *a, vint32m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u32m2x2 (uint32_t *a, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i32m2x3 (int32_t *a, vint32m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u32m2x3 (uint32_t *a, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i32m2x4 (int32_t *a, vint32m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u32m2x4 (uint32_t *a, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i32m4x2 (int32_t *a, vint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u32m4x2 (uint32_t *a, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i64m1x2 (int64_t *a, vint64m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u64m1x2 (uint64_t *a, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i64m1x3 (int64_t *a, vint64m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u64m1x3 (uint64_t *a, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i64m1x4 (int64_t *a, vint64m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u64m1x4 (uint64_t *a, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i64m1x5 (int64_t *a, vint64m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u64m1x5 (uint64_t *a, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i64m1x6 (int64_t *a, vint64m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u64m1x6 (uint64_t *a, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i64m1x7 (int64_t *a, vint64m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u64m1x7 (uint64_t *a, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i64m1x8 (int64_t *a, vint64m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u64m1x8 (uint64_t *a, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i64m2x2 (int64_t *a, vint64m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u64m2x2 (uint64_t *a, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i64m2x3 (int64_t *a, vint64m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u64m2x3 (uint64_t *a, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i64m2x4 (int64_t *a, vint64m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u64m2x4 (uint64_t *a, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i64m4x2 (int64_t *a, vint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u64m4x2 (uint64_t *a, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i8m1x2 (int8_t *a, vint8m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u8m1x2 (uint8_t *a, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i8m1x3 (int8_t *a, vint8m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u8m1x3 (uint8_t *a, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i8m1x4 (int8_t *a, vint8m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u8m1x4 (uint8_t *a, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i8m1x5 (int8_t *a, vint8m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u8m1x5 (uint8_t *a, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i8m1x6 (int8_t *a, vint8m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u8m1x6 (uint8_t *a, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i8m1x7 (int8_t *a, vint8m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u8m1x7 (uint8_t *a, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i8m1x8 (int8_t *a, vint8m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u8m1x8 (uint8_t *a, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i8m2x2 (int8_t *a, vint8m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u8m2x2 (uint8_t *a, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i8m2x3 (int8_t *a, vint8m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u8m2x3 (uint8_t *a, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i8m2x4 (int8_t *a, vint8m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u8m2x4 (uint8_t *a, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i8m4x2 (int8_t *a, vint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u8m4x2 (uint8_t *a, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i16m1x2 (int16_t *a, vint16m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u16m1x2 (uint16_t *a, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i16m1x3 (int16_t *a, vint16m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u16m1x3 (uint16_t *a, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i16m1x4 (int16_t *a, vint16m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u16m1x4 (uint16_t *a, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i16m1x5 (int16_t *a, vint16m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u16m1x5 (uint16_t *a, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i16m1x6 (int16_t *a, vint16m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u16m1x6 (uint16_t *a, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i16m1x7 (int16_t *a, vint16m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u16m1x7 (uint16_t *a, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i16m1x8 (int16_t *a, vint16m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u16m1x8 (uint16_t *a, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i16m2x2 (int16_t *a, vint16m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u16m2x2 (uint16_t *a, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i16m2x3 (int16_t *a, vint16m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u16m2x3 (uint16_t *a, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i16m2x4 (int16_t *a, vint16m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u16m2x4 (uint16_t *a, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i16m4x2 (int16_t *a, vint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u16m4x2 (uint16_t *a, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i32m1x2 (int32_t *a, vint32m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u32m1x2 (uint32_t *a, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i32m1x3 (int32_t *a, vint32m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u32m1x3 (uint32_t *a, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i32m1x4 (int32_t *a, vint32m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u32m1x4 (uint32_t *a, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i32m1x5 (int32_t *a, vint32m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u32m1x5 (uint32_t *a, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i32m1x6 (int32_t *a, vint32m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u32m1x6 (uint32_t *a, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i32m1x7 (int32_t *a, vint32m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u32m1x7 (uint32_t *a, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i32m1x8 (int32_t *a, vint32m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u32m1x8 (uint32_t *a, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i32m2x2 (int32_t *a, vint32m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u32m2x2 (uint32_t *a, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i32m2x3 (int32_t *a, vint32m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u32m2x3 (uint32_t *a, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i32m2x4 (int32_t *a, vint32m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u32m2x4 (uint32_t *a, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i32m4x2 (int32_t *a, vint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u32m4x2 (uint32_t *a, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i64m1x2 (int64_t *a, vint64m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u64m1x2 (uint64_t *a, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i64m1x3 (int64_t *a, vint64m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u64m1x3 (uint64_t *a, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i64m1x4 (int64_t *a, vint64m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u64m1x4 (uint64_t *a, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i64m1x5 (int64_t *a, vint64m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u64m1x5 (uint64_t *a, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i64m1x6 (int64_t *a, vint64m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u64m1x6 (uint64_t *a, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i64m1x7 (int64_t *a, vint64m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u64m1x7 (uint64_t *a, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i64m1x8 (int64_t *a, vint64m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u64m1x8 (uint64_t *a, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i64m2x2 (int64_t *a, vint64m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u64m2x2 (uint64_t *a, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i64m2x3 (int64_t *a, vint64m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u64m2x3 (uint64_t *a, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i64m2x4 (int64_t *a, vint64m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u64m2x4 (uint64_t *a, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i64m4x2 (int64_t *a, vint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u64m4x2 (uint64_t *a, vuint64m4x2_t b, size_t vl);
// masked functions
void __riscv_th_vsseg2b_v_i8m1x2_m (vbool8_t mask, int8_t *a, vint8m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i8m1x3_m (vbool8_t mask, int8_t *a, vint8m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i8m1x4_m (vbool8_t mask, int8_t *a, vint8m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i8m1x5_m (vbool8_t mask, int8_t *a, vint8m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i8m1x6_m (vbool8_t mask, int8_t *a, vint8m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i8m1x7_m (vbool8_t mask, int8_t *a, vint8m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i8m1x8_m (vbool8_t mask, int8_t *a, vint8m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i8m2x2_m (vbool4_t mask, int8_t *a, vint8m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i8m2x3_m (vbool4_t mask, int8_t *a, vint8m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i8m2x4_m (vbool4_t mask, int8_t *a, vint8m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i8m4x2_m (vbool2_t mask, int8_t *a, vint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_i16m1x2_m (vbool16_t mask, int16_t *a, vint16m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i16m1x3_m (vbool16_t mask, int16_t *a, vint16m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i16m1x4_m (vbool16_t mask, int16_t *a, vint16m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i16m1x5_m (vbool16_t mask, int16_t *a, vint16m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i16m1x6_m (vbool16_t mask, int16_t *a, vint16m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i16m1x7_m (vbool16_t mask, int16_t *a, vint16m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i16m1x8_m (vbool16_t mask, int16_t *a, vint16m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i16m2x2_m (vbool8_t mask, int16_t *a, vint16m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i16m2x3_m (vbool8_t mask, int16_t *a, vint16m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i16m2x4_m (vbool8_t mask, int16_t *a, vint16m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i16m4x2_m (vbool4_t mask, int16_t *a, vint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_i32m1x2_m (vbool32_t mask, int32_t *a, vint32m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i32m1x3_m (vbool32_t mask, int32_t *a, vint32m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i32m1x4_m (vbool32_t mask, int32_t *a, vint32m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i32m1x5_m (vbool32_t mask, int32_t *a, vint32m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i32m1x6_m (vbool32_t mask, int32_t *a, vint32m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i32m1x7_m (vbool32_t mask, int32_t *a, vint32m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i32m1x8_m (vbool32_t mask, int32_t *a, vint32m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i32m2x2_m (vbool16_t mask, int32_t *a, vint32m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i32m2x3_m (vbool16_t mask, int32_t *a, vint32m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i32m2x4_m (vbool16_t mask, int32_t *a, vint32m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i32m4x2_m (vbool8_t mask, int32_t *a, vint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_i64m1x2_m (vbool64_t mask, int64_t *a, vint64m1x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i64m1x3_m (vbool64_t mask, int64_t *a, vint64m1x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i64m1x4_m (vbool64_t mask, int64_t *a, vint64m1x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsseg5b_v_i64m1x5_m (vbool64_t mask, int64_t *a, vint64m1x5_t b, size_t vl);
void __riscv_th_vsseg5b_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsseg6b_v_i64m1x6_m (vbool64_t mask, int64_t *a, vint64m1x6_t b, size_t vl);
void __riscv_th_vsseg6b_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsseg7b_v_i64m1x7_m (vbool64_t mask, int64_t *a, vint64m1x7_t b, size_t vl);
void __riscv_th_vsseg7b_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsseg8b_v_i64m1x8_m (vbool64_t mask, int64_t *a, vint64m1x8_t b, size_t vl);
void __riscv_th_vsseg8b_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsseg2b_v_i64m2x2_m (vbool32_t mask, int64_t *a, vint64m2x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsseg3b_v_i64m2x3_m (vbool32_t mask, int64_t *a, vint64m2x3_t b, size_t vl);
void __riscv_th_vsseg3b_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsseg4b_v_i64m2x4_m (vbool32_t mask, int64_t *a, vint64m2x4_t b, size_t vl);
void __riscv_th_vsseg4b_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsseg2b_v_i64m4x2_m (vbool16_t mask, int64_t *a, vint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2b_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i8m1x2_m (vbool8_t mask, int8_t *a, vint8m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i8m1x3_m (vbool8_t mask, int8_t *a, vint8m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i8m1x4_m (vbool8_t mask, int8_t *a, vint8m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i8m1x5_m (vbool8_t mask, int8_t *a, vint8m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i8m1x6_m (vbool8_t mask, int8_t *a, vint8m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i8m1x7_m (vbool8_t mask, int8_t *a, vint8m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i8m1x8_m (vbool8_t mask, int8_t *a, vint8m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i8m2x2_m (vbool4_t mask, int8_t *a, vint8m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i8m2x3_m (vbool4_t mask, int8_t *a, vint8m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i8m2x4_m (vbool4_t mask, int8_t *a, vint8m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i8m4x2_m (vbool2_t mask, int8_t *a, vint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i16m1x2_m (vbool16_t mask, int16_t *a, vint16m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i16m1x3_m (vbool16_t mask, int16_t *a, vint16m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i16m1x4_m (vbool16_t mask, int16_t *a, vint16m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i16m1x5_m (vbool16_t mask, int16_t *a, vint16m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i16m1x6_m (vbool16_t mask, int16_t *a, vint16m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i16m1x7_m (vbool16_t mask, int16_t *a, vint16m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i16m1x8_m (vbool16_t mask, int16_t *a, vint16m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i16m2x2_m (vbool8_t mask, int16_t *a, vint16m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i16m2x3_m (vbool8_t mask, int16_t *a, vint16m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i16m2x4_m (vbool8_t mask, int16_t *a, vint16m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i16m4x2_m (vbool4_t mask, int16_t *a, vint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i32m1x2_m (vbool32_t mask, int32_t *a, vint32m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i32m1x3_m (vbool32_t mask, int32_t *a, vint32m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i32m1x4_m (vbool32_t mask, int32_t *a, vint32m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i32m1x5_m (vbool32_t mask, int32_t *a, vint32m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i32m1x6_m (vbool32_t mask, int32_t *a, vint32m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i32m1x7_m (vbool32_t mask, int32_t *a, vint32m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i32m1x8_m (vbool32_t mask, int32_t *a, vint32m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i32m2x2_m (vbool16_t mask, int32_t *a, vint32m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i32m2x3_m (vbool16_t mask, int32_t *a, vint32m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i32m2x4_m (vbool16_t mask, int32_t *a, vint32m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i32m4x2_m (vbool8_t mask, int32_t *a, vint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_i64m1x2_m (vbool64_t mask, int64_t *a, vint64m1x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i64m1x3_m (vbool64_t mask, int64_t *a, vint64m1x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i64m1x4_m (vbool64_t mask, int64_t *a, vint64m1x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsseg5h_v_i64m1x5_m (vbool64_t mask, int64_t *a, vint64m1x5_t b, size_t vl);
void __riscv_th_vsseg5h_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsseg6h_v_i64m1x6_m (vbool64_t mask, int64_t *a, vint64m1x6_t b, size_t vl);
void __riscv_th_vsseg6h_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsseg7h_v_i64m1x7_m (vbool64_t mask, int64_t *a, vint64m1x7_t b, size_t vl);
void __riscv_th_vsseg7h_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsseg8h_v_i64m1x8_m (vbool64_t mask, int64_t *a, vint64m1x8_t b, size_t vl);
void __riscv_th_vsseg8h_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsseg2h_v_i64m2x2_m (vbool32_t mask, int64_t *a, vint64m2x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsseg3h_v_i64m2x3_m (vbool32_t mask, int64_t *a, vint64m2x3_t b, size_t vl);
void __riscv_th_vsseg3h_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsseg4h_v_i64m2x4_m (vbool32_t mask, int64_t *a, vint64m2x4_t b, size_t vl);
void __riscv_th_vsseg4h_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsseg2h_v_i64m4x2_m (vbool16_t mask, int64_t *a, vint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2h_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i8m1x2_m (vbool8_t mask, int8_t *a, vint8m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i8m1x3_m (vbool8_t mask, int8_t *a, vint8m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i8m1x4_m (vbool8_t mask, int8_t *a, vint8m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i8m1x5_m (vbool8_t mask, int8_t *a, vint8m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i8m1x6_m (vbool8_t mask, int8_t *a, vint8m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i8m1x7_m (vbool8_t mask, int8_t *a, vint8m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i8m1x8_m (vbool8_t mask, int8_t *a, vint8m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i8m2x2_m (vbool4_t mask, int8_t *a, vint8m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i8m2x3_m (vbool4_t mask, int8_t *a, vint8m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i8m2x4_m (vbool4_t mask, int8_t *a, vint8m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i8m4x2_m (vbool2_t mask, int8_t *a, vint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i16m1x2_m (vbool16_t mask, int16_t *a, vint16m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i16m1x3_m (vbool16_t mask, int16_t *a, vint16m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i16m1x4_m (vbool16_t mask, int16_t *a, vint16m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i16m1x5_m (vbool16_t mask, int16_t *a, vint16m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i16m1x6_m (vbool16_t mask, int16_t *a, vint16m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i16m1x7_m (vbool16_t mask, int16_t *a, vint16m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i16m1x8_m (vbool16_t mask, int16_t *a, vint16m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i16m2x2_m (vbool8_t mask, int16_t *a, vint16m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i16m2x3_m (vbool8_t mask, int16_t *a, vint16m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i16m2x4_m (vbool8_t mask, int16_t *a, vint16m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i16m4x2_m (vbool4_t mask, int16_t *a, vint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i32m1x2_m (vbool32_t mask, int32_t *a, vint32m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i32m1x3_m (vbool32_t mask, int32_t *a, vint32m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i32m1x4_m (vbool32_t mask, int32_t *a, vint32m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i32m1x5_m (vbool32_t mask, int32_t *a, vint32m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i32m1x6_m (vbool32_t mask, int32_t *a, vint32m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i32m1x7_m (vbool32_t mask, int32_t *a, vint32m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i32m1x8_m (vbool32_t mask, int32_t *a, vint32m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i32m2x2_m (vbool16_t mask, int32_t *a, vint32m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i32m2x3_m (vbool16_t mask, int32_t *a, vint32m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i32m2x4_m (vbool16_t mask, int32_t *a, vint32m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i32m4x2_m (vbool8_t mask, int32_t *a, vint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_i64m1x2_m (vbool64_t mask, int64_t *a, vint64m1x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i64m1x3_m (vbool64_t mask, int64_t *a, vint64m1x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i64m1x4_m (vbool64_t mask, int64_t *a, vint64m1x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsseg5w_v_i64m1x5_m (vbool64_t mask, int64_t *a, vint64m1x5_t b, size_t vl);
void __riscv_th_vsseg5w_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsseg6w_v_i64m1x6_m (vbool64_t mask, int64_t *a, vint64m1x6_t b, size_t vl);
void __riscv_th_vsseg6w_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsseg7w_v_i64m1x7_m (vbool64_t mask, int64_t *a, vint64m1x7_t b, size_t vl);
void __riscv_th_vsseg7w_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsseg8w_v_i64m1x8_m (vbool64_t mask, int64_t *a, vint64m1x8_t b, size_t vl);
void __riscv_th_vsseg8w_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsseg2w_v_i64m2x2_m (vbool32_t mask, int64_t *a, vint64m2x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsseg3w_v_i64m2x3_m (vbool32_t mask, int64_t *a, vint64m2x3_t b, size_t vl);
void __riscv_th_vsseg3w_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsseg4w_v_i64m2x4_m (vbool32_t mask, int64_t *a, vint64m2x4_t b, size_t vl);
void __riscv_th_vsseg4w_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsseg2w_v_i64m4x2_m (vbool16_t mask, int64_t *a, vint64m4x2_t b, size_t vl);
void __riscv_th_vsseg2w_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4x2_t b, size_t vl);
----

[[xtheadvector-strided-segment-load]]
===== XTheadVector Strided Segment Load Intrinsics

[,c]
----
vint8m1x2_t __riscv_th_vlsseg2b_v_i8m1x2 (int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2b_v_u8m1x2 (uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3b_v_i8m1x3 (int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3b_v_u8m1x3 (uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4b_v_i8m1x4 (int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4b_v_u8m1x4 (uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5b_v_i8m1x5 (int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5b_v_u8m1x5 (uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6b_v_i8m1x6 (int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6b_v_u8m1x6 (uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7b_v_i8m1x7 (int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7b_v_u8m1x7 (uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8b_v_i8m1x8 (int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8b_v_u8m1x8 (uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2b_v_i8m2x2 (int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2b_v_u8m2x2 (uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3b_v_i8m2x3 (int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3b_v_u8m2x3 (uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4b_v_i8m2x4 (int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4b_v_u8m2x4 (uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2b_v_i8m4x2 (int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2b_v_u8m4x2 (uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2b_v_i16m1x2 (int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2b_v_u16m1x2 (uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3b_v_i16m1x3 (int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3b_v_u16m1x3 (uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4b_v_i16m1x4 (int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4b_v_u16m1x4 (uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5b_v_i16m1x5 (int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5b_v_u16m1x5 (uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6b_v_i16m1x6 (int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6b_v_u16m1x6 (uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7b_v_i16m1x7 (int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7b_v_u16m1x7 (uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8b_v_i16m1x8 (int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8b_v_u16m1x8 (uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2b_v_i16m2x2 (int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2b_v_u16m2x2 (uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3b_v_i16m2x3 (int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3b_v_u16m2x3 (uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4b_v_i16m2x4 (int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4b_v_u16m2x4 (uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2b_v_i16m4x2 (int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2b_v_u16m4x2 (uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2b_v_i32m1x2 (int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2b_v_u32m1x2 (uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3b_v_i32m1x3 (int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3b_v_u32m1x3 (uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4b_v_i32m1x4 (int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4b_v_u32m1x4 (uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5b_v_i32m1x5 (int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5b_v_u32m1x5 (uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6b_v_i32m1x6 (int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6b_v_u32m1x6 (uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7b_v_i32m1x7 (int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7b_v_u32m1x7 (uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8b_v_i32m1x8 (int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8b_v_u32m1x8 (uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2b_v_i32m2x2 (int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2b_v_u32m2x2 (uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3b_v_i32m2x3 (int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3b_v_u32m2x3 (uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4b_v_i32m2x4 (int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4b_v_u32m2x4 (uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2b_v_i32m4x2 (int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2b_v_u32m4x2 (uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2b_v_i64m1x2 (int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2b_v_u64m1x2 (uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3b_v_i64m1x3 (int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3b_v_u64m1x3 (uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4b_v_i64m1x4 (int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4b_v_u64m1x4 (uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5b_v_i64m1x5 (int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5b_v_u64m1x5 (uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6b_v_i64m1x6 (int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6b_v_u64m1x6 (uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7b_v_i64m1x7 (int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7b_v_u64m1x7 (uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8b_v_i64m1x8 (int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8b_v_u64m1x8 (uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2b_v_i64m2x2 (int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2b_v_u64m2x2 (uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3b_v_i64m2x3 (int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3b_v_u64m2x3 (uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4b_v_i64m2x4 (int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4b_v_u64m2x4 (uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2b_v_i64m4x2 (int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2b_v_u64m4x2 (uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2b_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2b_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3b_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3b_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4b_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4b_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5b_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5b_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6b_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6b_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7b_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7b_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8b_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8b_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2b_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2b_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3b_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3b_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4b_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4b_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2b_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2b_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2b_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2b_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3b_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3b_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4b_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4b_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5b_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5b_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6b_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6b_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7b_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7b_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8b_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8b_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2b_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2b_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3b_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3b_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4b_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4b_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2b_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2b_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2b_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2b_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3b_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3b_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4b_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4b_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5b_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5b_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6b_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6b_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7b_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7b_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8b_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8b_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2b_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2b_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3b_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3b_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4b_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4b_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2b_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2b_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2b_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2b_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3b_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3b_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4b_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4b_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5b_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5b_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6b_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6b_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7b_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7b_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8b_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8b_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2b_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2b_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3b_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3b_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4b_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4b_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2b_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2b_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2bu_v_i8m1x2 (int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2bu_v_u8m1x2 (uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3bu_v_i8m1x3 (int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3bu_v_u8m1x3 (uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4bu_v_i8m1x4 (int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4bu_v_u8m1x4 (uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5bu_v_i8m1x5 (int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5bu_v_u8m1x5 (uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6bu_v_i8m1x6 (int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6bu_v_u8m1x6 (uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7bu_v_i8m1x7 (int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7bu_v_u8m1x7 (uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8bu_v_i8m1x8 (int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8bu_v_u8m1x8 (uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2bu_v_i8m2x2 (int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2bu_v_u8m2x2 (uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3bu_v_i8m2x3 (int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3bu_v_u8m2x3 (uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4bu_v_i8m2x4 (int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4bu_v_u8m2x4 (uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2bu_v_i8m4x2 (int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2bu_v_u8m4x2 (uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2bu_v_i16m1x2 (int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2bu_v_u16m1x2 (uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3bu_v_i16m1x3 (int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3bu_v_u16m1x3 (uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4bu_v_i16m1x4 (int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4bu_v_u16m1x4 (uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5bu_v_i16m1x5 (int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5bu_v_u16m1x5 (uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6bu_v_i16m1x6 (int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6bu_v_u16m1x6 (uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7bu_v_i16m1x7 (int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7bu_v_u16m1x7 (uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8bu_v_i16m1x8 (int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8bu_v_u16m1x8 (uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2bu_v_i16m2x2 (int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2bu_v_u16m2x2 (uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3bu_v_i16m2x3 (int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3bu_v_u16m2x3 (uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4bu_v_i16m2x4 (int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4bu_v_u16m2x4 (uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2bu_v_i16m4x2 (int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2bu_v_u16m4x2 (uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2bu_v_i32m1x2 (int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2bu_v_u32m1x2 (uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3bu_v_i32m1x3 (int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3bu_v_u32m1x3 (uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4bu_v_i32m1x4 (int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4bu_v_u32m1x4 (uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5bu_v_i32m1x5 (int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5bu_v_u32m1x5 (uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6bu_v_i32m1x6 (int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6bu_v_u32m1x6 (uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7bu_v_i32m1x7 (int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7bu_v_u32m1x7 (uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8bu_v_i32m1x8 (int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8bu_v_u32m1x8 (uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2bu_v_i32m2x2 (int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2bu_v_u32m2x2 (uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3bu_v_i32m2x3 (int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3bu_v_u32m2x3 (uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4bu_v_i32m2x4 (int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4bu_v_u32m2x4 (uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2bu_v_i32m4x2 (int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2bu_v_u32m4x2 (uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2bu_v_i64m1x2 (int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2bu_v_u64m1x2 (uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3bu_v_i64m1x3 (int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3bu_v_u64m1x3 (uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4bu_v_i64m1x4 (int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4bu_v_u64m1x4 (uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5bu_v_i64m1x5 (int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5bu_v_u64m1x5 (uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6bu_v_i64m1x6 (int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6bu_v_u64m1x6 (uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7bu_v_i64m1x7 (int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7bu_v_u64m1x7 (uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8bu_v_i64m1x8 (int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8bu_v_u64m1x8 (uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2bu_v_i64m2x2 (int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2bu_v_u64m2x2 (uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3bu_v_i64m2x3 (int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3bu_v_u64m2x3 (uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4bu_v_i64m2x4 (int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4bu_v_u64m2x4 (uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2bu_v_i64m4x2 (int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2bu_v_u64m4x2 (uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2bu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2bu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3bu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3bu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4bu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4bu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5bu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5bu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6bu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6bu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7bu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7bu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8bu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8bu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2bu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2bu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3bu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3bu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4bu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4bu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2bu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2bu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2bu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2bu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3bu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3bu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4bu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4bu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5bu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5bu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6bu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6bu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7bu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7bu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8bu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8bu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2bu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2bu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3bu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3bu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4bu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4bu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2bu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2bu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2bu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2bu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3bu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3bu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4bu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4bu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5bu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5bu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6bu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6bu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7bu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7bu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8bu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8bu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2bu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2bu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3bu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3bu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4bu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4bu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2bu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2bu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2bu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2bu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3bu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3bu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4bu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4bu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5bu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5bu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6bu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6bu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7bu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7bu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8bu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8bu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2bu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2bu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3bu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3bu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4bu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4bu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2bu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2bu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2h_v_i8m1x2 (int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2h_v_u8m1x2 (uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3h_v_i8m1x3 (int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3h_v_u8m1x3 (uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4h_v_i8m1x4 (int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4h_v_u8m1x4 (uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5h_v_i8m1x5 (int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5h_v_u8m1x5 (uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6h_v_i8m1x6 (int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6h_v_u8m1x6 (uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7h_v_i8m1x7 (int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7h_v_u8m1x7 (uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8h_v_i8m1x8 (int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8h_v_u8m1x8 (uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2h_v_i8m2x2 (int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2h_v_u8m2x2 (uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3h_v_i8m2x3 (int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3h_v_u8m2x3 (uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4h_v_i8m2x4 (int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4h_v_u8m2x4 (uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2h_v_i8m4x2 (int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2h_v_u8m4x2 (uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2h_v_i16m1x2 (int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2h_v_u16m1x2 (uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3h_v_i16m1x3 (int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3h_v_u16m1x3 (uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4h_v_i16m1x4 (int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4h_v_u16m1x4 (uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5h_v_i16m1x5 (int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5h_v_u16m1x5 (uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6h_v_i16m1x6 (int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6h_v_u16m1x6 (uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7h_v_i16m1x7 (int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7h_v_u16m1x7 (uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8h_v_i16m1x8 (int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8h_v_u16m1x8 (uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2h_v_i16m2x2 (int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2h_v_u16m2x2 (uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3h_v_i16m2x3 (int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3h_v_u16m2x3 (uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4h_v_i16m2x4 (int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4h_v_u16m2x4 (uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2h_v_i16m4x2 (int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2h_v_u16m4x2 (uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2h_v_i32m1x2 (int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2h_v_u32m1x2 (uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3h_v_i32m1x3 (int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3h_v_u32m1x3 (uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4h_v_i32m1x4 (int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4h_v_u32m1x4 (uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5h_v_i32m1x5 (int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5h_v_u32m1x5 (uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6h_v_i32m1x6 (int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6h_v_u32m1x6 (uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7h_v_i32m1x7 (int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7h_v_u32m1x7 (uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8h_v_i32m1x8 (int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8h_v_u32m1x8 (uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2h_v_i32m2x2 (int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2h_v_u32m2x2 (uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3h_v_i32m2x3 (int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3h_v_u32m2x3 (uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4h_v_i32m2x4 (int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4h_v_u32m2x4 (uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2h_v_i32m4x2 (int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2h_v_u32m4x2 (uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2h_v_i64m1x2 (int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2h_v_u64m1x2 (uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3h_v_i64m1x3 (int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3h_v_u64m1x3 (uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4h_v_i64m1x4 (int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4h_v_u64m1x4 (uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5h_v_i64m1x5 (int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5h_v_u64m1x5 (uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6h_v_i64m1x6 (int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6h_v_u64m1x6 (uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7h_v_i64m1x7 (int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7h_v_u64m1x7 (uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8h_v_i64m1x8 (int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8h_v_u64m1x8 (uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2h_v_i64m2x2 (int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2h_v_u64m2x2 (uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3h_v_i64m2x3 (int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3h_v_u64m2x3 (uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4h_v_i64m2x4 (int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4h_v_u64m2x4 (uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2h_v_i64m4x2 (int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2h_v_u64m4x2 (uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2h_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2h_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3h_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3h_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4h_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4h_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5h_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5h_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6h_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6h_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7h_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7h_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8h_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8h_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2h_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2h_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3h_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3h_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4h_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4h_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2h_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2h_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2h_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2h_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3h_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3h_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4h_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4h_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5h_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5h_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6h_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6h_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7h_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7h_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8h_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8h_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2h_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2h_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3h_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3h_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4h_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4h_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2h_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2h_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2h_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2h_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3h_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3h_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4h_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4h_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5h_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5h_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6h_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6h_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7h_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7h_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8h_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8h_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2h_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2h_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3h_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3h_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4h_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4h_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2h_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2h_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2h_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2h_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3h_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3h_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4h_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4h_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5h_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5h_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6h_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6h_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7h_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7h_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8h_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8h_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2h_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2h_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3h_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3h_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4h_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4h_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2h_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2h_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2hu_v_i8m1x2 (int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2hu_v_u8m1x2 (uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3hu_v_i8m1x3 (int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3hu_v_u8m1x3 (uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4hu_v_i8m1x4 (int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4hu_v_u8m1x4 (uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5hu_v_i8m1x5 (int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5hu_v_u8m1x5 (uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6hu_v_i8m1x6 (int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6hu_v_u8m1x6 (uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7hu_v_i8m1x7 (int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7hu_v_u8m1x7 (uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8hu_v_i8m1x8 (int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8hu_v_u8m1x8 (uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2hu_v_i8m2x2 (int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2hu_v_u8m2x2 (uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3hu_v_i8m2x3 (int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3hu_v_u8m2x3 (uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4hu_v_i8m2x4 (int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4hu_v_u8m2x4 (uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2hu_v_i8m4x2 (int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2hu_v_u8m4x2 (uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2hu_v_i16m1x2 (int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2hu_v_u16m1x2 (uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3hu_v_i16m1x3 (int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3hu_v_u16m1x3 (uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4hu_v_i16m1x4 (int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4hu_v_u16m1x4 (uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5hu_v_i16m1x5 (int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5hu_v_u16m1x5 (uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6hu_v_i16m1x6 (int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6hu_v_u16m1x6 (uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7hu_v_i16m1x7 (int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7hu_v_u16m1x7 (uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8hu_v_i16m1x8 (int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8hu_v_u16m1x8 (uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2hu_v_i16m2x2 (int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2hu_v_u16m2x2 (uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3hu_v_i16m2x3 (int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3hu_v_u16m2x3 (uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4hu_v_i16m2x4 (int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4hu_v_u16m2x4 (uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2hu_v_i16m4x2 (int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2hu_v_u16m4x2 (uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2hu_v_i32m1x2 (int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2hu_v_u32m1x2 (uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3hu_v_i32m1x3 (int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3hu_v_u32m1x3 (uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4hu_v_i32m1x4 (int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4hu_v_u32m1x4 (uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5hu_v_i32m1x5 (int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5hu_v_u32m1x5 (uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6hu_v_i32m1x6 (int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6hu_v_u32m1x6 (uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7hu_v_i32m1x7 (int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7hu_v_u32m1x7 (uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8hu_v_i32m1x8 (int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8hu_v_u32m1x8 (uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2hu_v_i32m2x2 (int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2hu_v_u32m2x2 (uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3hu_v_i32m2x3 (int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3hu_v_u32m2x3 (uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4hu_v_i32m2x4 (int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4hu_v_u32m2x4 (uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2hu_v_i32m4x2 (int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2hu_v_u32m4x2 (uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2hu_v_i64m1x2 (int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2hu_v_u64m1x2 (uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3hu_v_i64m1x3 (int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3hu_v_u64m1x3 (uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4hu_v_i64m1x4 (int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4hu_v_u64m1x4 (uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5hu_v_i64m1x5 (int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5hu_v_u64m1x5 (uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6hu_v_i64m1x6 (int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6hu_v_u64m1x6 (uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7hu_v_i64m1x7 (int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7hu_v_u64m1x7 (uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8hu_v_i64m1x8 (int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8hu_v_u64m1x8 (uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2hu_v_i64m2x2 (int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2hu_v_u64m2x2 (uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3hu_v_i64m2x3 (int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3hu_v_u64m2x3 (uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4hu_v_i64m2x4 (int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4hu_v_u64m2x4 (uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2hu_v_i64m4x2 (int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2hu_v_u64m4x2 (uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2hu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2hu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3hu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3hu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4hu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4hu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5hu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5hu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6hu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6hu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7hu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7hu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8hu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8hu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2hu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2hu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3hu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3hu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4hu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4hu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2hu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2hu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2hu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2hu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3hu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3hu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4hu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4hu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5hu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5hu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6hu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6hu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7hu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7hu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8hu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8hu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2hu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2hu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3hu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3hu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4hu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4hu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2hu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2hu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2hu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2hu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3hu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3hu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4hu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4hu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5hu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5hu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6hu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6hu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7hu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7hu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8hu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8hu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2hu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2hu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3hu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3hu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4hu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4hu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2hu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2hu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2hu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2hu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3hu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3hu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4hu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4hu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5hu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5hu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6hu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6hu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7hu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7hu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8hu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8hu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2hu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2hu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3hu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3hu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4hu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4hu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2hu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2hu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2w_v_i8m1x2 (int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2w_v_u8m1x2 (uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3w_v_i8m1x3 (int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3w_v_u8m1x3 (uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4w_v_i8m1x4 (int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4w_v_u8m1x4 (uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5w_v_i8m1x5 (int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5w_v_u8m1x5 (uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6w_v_i8m1x6 (int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6w_v_u8m1x6 (uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7w_v_i8m1x7 (int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7w_v_u8m1x7 (uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8w_v_i8m1x8 (int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8w_v_u8m1x8 (uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2w_v_i8m2x2 (int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2w_v_u8m2x2 (uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3w_v_i8m2x3 (int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3w_v_u8m2x3 (uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4w_v_i8m2x4 (int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4w_v_u8m2x4 (uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2w_v_i8m4x2 (int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2w_v_u8m4x2 (uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2w_v_i16m1x2 (int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2w_v_u16m1x2 (uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3w_v_i16m1x3 (int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3w_v_u16m1x3 (uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4w_v_i16m1x4 (int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4w_v_u16m1x4 (uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5w_v_i16m1x5 (int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5w_v_u16m1x5 (uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6w_v_i16m1x6 (int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6w_v_u16m1x6 (uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7w_v_i16m1x7 (int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7w_v_u16m1x7 (uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8w_v_i16m1x8 (int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8w_v_u16m1x8 (uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2w_v_i16m2x2 (int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2w_v_u16m2x2 (uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3w_v_i16m2x3 (int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3w_v_u16m2x3 (uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4w_v_i16m2x4 (int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4w_v_u16m2x4 (uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2w_v_i16m4x2 (int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2w_v_u16m4x2 (uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2w_v_i32m1x2 (int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2w_v_u32m1x2 (uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3w_v_i32m1x3 (int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3w_v_u32m1x3 (uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4w_v_i32m1x4 (int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4w_v_u32m1x4 (uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5w_v_i32m1x5 (int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5w_v_u32m1x5 (uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6w_v_i32m1x6 (int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6w_v_u32m1x6 (uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7w_v_i32m1x7 (int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7w_v_u32m1x7 (uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8w_v_i32m1x8 (int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8w_v_u32m1x8 (uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2w_v_i32m2x2 (int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2w_v_u32m2x2 (uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3w_v_i32m2x3 (int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3w_v_u32m2x3 (uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4w_v_i32m2x4 (int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4w_v_u32m2x4 (uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2w_v_i32m4x2 (int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2w_v_u32m4x2 (uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2w_v_i64m1x2 (int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2w_v_u64m1x2 (uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3w_v_i64m1x3 (int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3w_v_u64m1x3 (uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4w_v_i64m1x4 (int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4w_v_u64m1x4 (uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5w_v_i64m1x5 (int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5w_v_u64m1x5 (uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6w_v_i64m1x6 (int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6w_v_u64m1x6 (uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7w_v_i64m1x7 (int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7w_v_u64m1x7 (uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8w_v_i64m1x8 (int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8w_v_u64m1x8 (uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2w_v_i64m2x2 (int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2w_v_u64m2x2 (uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3w_v_i64m2x3 (int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3w_v_u64m2x3 (uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4w_v_i64m2x4 (int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4w_v_u64m2x4 (uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2w_v_i64m4x2 (int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2w_v_u64m4x2 (uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2w_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2w_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3w_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3w_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4w_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4w_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5w_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5w_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6w_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6w_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7w_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7w_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8w_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8w_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2w_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2w_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3w_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3w_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4w_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4w_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2w_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2w_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2w_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2w_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3w_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3w_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4w_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4w_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5w_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5w_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6w_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6w_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7w_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7w_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8w_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8w_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2w_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2w_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3w_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3w_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4w_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4w_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2w_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2w_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2w_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2w_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3w_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3w_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4w_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4w_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5w_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5w_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6w_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6w_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7w_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7w_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8w_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8w_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2w_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2w_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3w_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3w_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4w_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4w_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2w_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2w_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2w_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2w_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3w_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3w_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4w_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4w_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5w_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5w_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6w_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6w_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7w_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7w_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8w_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8w_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2w_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2w_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3w_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3w_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4w_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4w_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2w_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2w_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2wu_v_i8m1x2 (int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2wu_v_u8m1x2 (uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3wu_v_i8m1x3 (int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3wu_v_u8m1x3 (uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4wu_v_i8m1x4 (int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4wu_v_u8m1x4 (uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5wu_v_i8m1x5 (int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5wu_v_u8m1x5 (uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6wu_v_i8m1x6 (int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6wu_v_u8m1x6 (uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7wu_v_i8m1x7 (int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7wu_v_u8m1x7 (uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8wu_v_i8m1x8 (int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8wu_v_u8m1x8 (uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2wu_v_i8m2x2 (int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2wu_v_u8m2x2 (uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3wu_v_i8m2x3 (int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3wu_v_u8m2x3 (uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4wu_v_i8m2x4 (int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4wu_v_u8m2x4 (uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2wu_v_i8m4x2 (int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2wu_v_u8m4x2 (uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2wu_v_i16m1x2 (int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2wu_v_u16m1x2 (uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3wu_v_i16m1x3 (int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3wu_v_u16m1x3 (uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4wu_v_i16m1x4 (int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4wu_v_u16m1x4 (uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5wu_v_i16m1x5 (int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5wu_v_u16m1x5 (uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6wu_v_i16m1x6 (int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6wu_v_u16m1x6 (uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7wu_v_i16m1x7 (int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7wu_v_u16m1x7 (uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8wu_v_i16m1x8 (int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8wu_v_u16m1x8 (uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2wu_v_i16m2x2 (int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2wu_v_u16m2x2 (uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3wu_v_i16m2x3 (int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3wu_v_u16m2x3 (uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4wu_v_i16m2x4 (int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4wu_v_u16m2x4 (uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2wu_v_i16m4x2 (int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2wu_v_u16m4x2 (uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2wu_v_i32m1x2 (int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2wu_v_u32m1x2 (uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3wu_v_i32m1x3 (int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3wu_v_u32m1x3 (uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4wu_v_i32m1x4 (int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4wu_v_u32m1x4 (uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5wu_v_i32m1x5 (int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5wu_v_u32m1x5 (uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6wu_v_i32m1x6 (int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6wu_v_u32m1x6 (uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7wu_v_i32m1x7 (int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7wu_v_u32m1x7 (uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8wu_v_i32m1x8 (int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8wu_v_u32m1x8 (uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2wu_v_i32m2x2 (int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2wu_v_u32m2x2 (uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3wu_v_i32m2x3 (int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3wu_v_u32m2x3 (uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4wu_v_i32m2x4 (int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4wu_v_u32m2x4 (uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2wu_v_i32m4x2 (int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2wu_v_u32m4x2 (uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2wu_v_i64m1x2 (int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2wu_v_u64m1x2 (uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3wu_v_i64m1x3 (int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3wu_v_u64m1x3 (uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4wu_v_i64m1x4 (int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4wu_v_u64m1x4 (uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5wu_v_i64m1x5 (int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5wu_v_u64m1x5 (uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6wu_v_i64m1x6 (int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6wu_v_u64m1x6 (uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7wu_v_i64m1x7 (int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7wu_v_u64m1x7 (uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8wu_v_i64m1x8 (int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8wu_v_u64m1x8 (uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2wu_v_i64m2x2 (int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2wu_v_u64m2x2 (uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3wu_v_i64m2x3 (int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3wu_v_u64m2x3 (uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4wu_v_i64m2x4 (int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4wu_v_u64m2x4 (uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2wu_v_i64m4x2 (int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2wu_v_u64m4x2 (uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2wu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2wu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3wu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3wu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4wu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4wu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5wu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5wu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6wu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6wu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7wu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7wu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8wu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8wu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2wu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2wu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3wu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3wu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4wu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4wu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2wu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2wu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2wu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2wu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3wu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3wu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4wu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4wu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5wu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5wu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6wu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6wu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7wu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7wu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8wu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8wu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2wu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2wu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3wu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3wu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4wu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4wu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2wu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2wu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2wu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2wu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3wu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3wu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4wu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4wu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5wu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5wu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6wu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6wu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7wu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7wu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8wu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8wu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2wu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2wu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3wu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3wu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4wu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4wu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2wu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2wu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2wu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2wu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3wu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3wu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4wu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4wu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5wu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5wu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6wu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6wu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7wu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7wu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8wu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8wu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2wu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2wu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3wu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3wu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4wu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4wu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2wu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2wu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
// masked functions
vint8m1x2_t __riscv_th_vlsseg2b_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2b_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3b_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3b_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4b_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4b_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5b_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5b_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6b_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6b_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7b_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7b_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8b_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8b_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2b_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2b_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3b_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3b_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4b_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4b_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2b_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2b_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2b_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2b_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3b_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3b_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4b_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4b_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5b_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5b_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6b_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6b_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7b_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7b_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8b_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8b_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2b_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2b_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3b_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3b_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4b_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4b_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2b_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2b_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2b_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2b_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3b_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3b_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4b_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4b_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5b_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5b_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6b_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6b_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7b_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7b_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8b_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8b_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2b_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2b_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3b_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3b_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4b_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4b_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2b_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2b_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2b_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2b_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3b_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3b_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4b_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4b_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5b_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5b_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6b_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6b_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7b_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7b_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8b_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8b_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2b_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2b_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3b_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3b_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4b_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4b_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2b_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2b_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2b_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2b_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3b_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3b_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4b_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4b_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5b_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5b_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6b_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6b_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7b_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7b_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8b_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8b_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2b_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2b_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3b_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3b_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4b_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4b_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2b_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2b_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2b_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2b_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3b_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3b_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4b_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4b_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5b_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5b_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6b_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6b_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7b_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7b_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8b_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8b_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2b_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2b_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3b_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3b_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4b_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4b_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2b_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2b_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2b_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2b_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3b_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3b_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4b_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4b_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5b_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5b_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6b_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6b_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7b_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7b_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8b_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8b_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2b_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2b_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3b_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3b_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4b_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4b_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2b_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2b_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2b_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2b_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3b_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3b_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4b_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4b_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5b_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5b_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6b_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6b_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7b_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7b_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8b_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8b_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2b_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2b_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3b_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3b_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4b_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4b_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2b_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2b_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2b_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2b_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3b_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3b_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4b_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4b_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5b_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5b_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6b_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6b_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7b_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7b_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8b_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8b_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2b_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2b_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3b_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3b_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4b_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4b_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2b_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2b_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2b_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2b_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3b_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3b_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4b_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4b_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5b_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5b_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6b_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6b_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7b_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7b_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8b_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8b_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2b_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2b_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3b_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3b_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4b_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4b_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2b_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2b_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2b_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2b_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3b_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3b_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4b_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4b_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5b_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5b_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6b_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6b_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7b_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7b_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8b_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8b_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2b_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2b_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3b_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3b_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4b_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4b_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2b_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2b_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2b_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2b_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3b_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3b_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4b_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4b_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5b_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5b_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6b_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6b_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7b_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7b_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8b_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8b_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2b_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2b_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3b_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3b_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4b_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4b_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2b_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2b_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2b_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2b_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3b_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3b_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4b_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4b_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5b_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5b_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6b_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6b_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7b_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7b_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8b_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8b_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2b_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2b_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3b_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3b_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4b_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4b_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2b_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2b_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2b_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2b_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3b_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3b_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4b_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4b_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5b_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5b_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6b_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6b_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7b_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7b_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8b_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8b_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2b_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2b_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3b_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3b_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4b_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4b_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2b_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2b_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2b_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2b_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3b_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3b_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4b_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4b_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5b_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5b_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6b_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6b_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7b_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7b_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8b_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8b_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2b_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2b_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3b_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3b_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4b_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4b_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2b_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2b_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2b_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2b_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3b_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3b_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4b_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4b_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5b_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5b_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6b_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6b_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7b_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7b_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8b_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8b_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2b_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2b_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3b_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3b_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4b_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4b_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2b_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2b_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2bu_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2bu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3bu_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3bu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4bu_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4bu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5bu_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5bu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6bu_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6bu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7bu_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7bu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8bu_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8bu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2bu_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2bu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3bu_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3bu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4bu_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4bu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2bu_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2bu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2bu_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2bu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3bu_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3bu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4bu_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4bu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5bu_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5bu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6bu_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6bu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7bu_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7bu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8bu_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8bu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2bu_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2bu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3bu_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3bu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4bu_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4bu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2bu_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2bu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2bu_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2bu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3bu_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3bu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4bu_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4bu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5bu_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5bu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6bu_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6bu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7bu_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7bu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8bu_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8bu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2bu_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2bu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3bu_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3bu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4bu_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4bu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2bu_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2bu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2bu_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2bu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3bu_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3bu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4bu_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4bu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5bu_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5bu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6bu_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6bu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7bu_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7bu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8bu_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8bu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2bu_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2bu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3bu_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3bu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4bu_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4bu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2bu_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2bu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2bu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2bu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3bu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3bu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4bu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4bu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5bu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5bu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6bu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6bu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7bu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7bu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8bu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8bu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2bu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2bu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3bu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3bu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4bu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4bu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2bu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2bu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2bu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2bu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3bu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3bu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4bu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4bu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5bu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5bu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6bu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6bu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7bu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7bu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8bu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8bu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2bu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2bu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3bu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3bu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4bu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4bu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2bu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2bu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2bu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2bu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3bu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3bu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4bu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4bu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5bu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5bu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6bu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6bu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7bu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7bu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8bu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8bu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2bu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2bu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3bu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3bu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4bu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4bu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2bu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2bu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2bu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2bu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3bu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3bu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4bu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4bu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5bu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5bu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6bu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6bu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7bu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7bu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8bu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8bu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2bu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2bu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3bu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3bu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4bu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4bu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2bu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2bu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2bu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2bu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3bu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3bu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4bu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4bu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5bu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5bu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6bu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6bu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7bu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7bu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8bu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8bu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2bu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2bu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3bu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3bu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4bu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4bu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2bu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2bu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2bu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2bu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3bu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3bu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4bu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4bu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5bu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5bu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6bu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6bu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7bu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7bu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8bu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8bu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2bu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2bu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3bu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3bu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4bu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4bu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2bu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2bu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2bu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2bu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3bu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3bu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4bu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4bu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5bu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5bu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6bu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6bu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7bu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7bu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8bu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8bu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2bu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2bu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3bu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3bu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4bu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4bu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2bu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2bu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2bu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2bu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3bu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3bu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4bu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4bu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5bu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5bu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6bu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6bu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7bu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7bu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8bu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8bu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2bu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2bu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3bu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3bu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4bu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4bu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2bu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2bu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2bu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2bu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3bu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3bu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4bu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4bu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5bu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5bu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6bu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6bu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7bu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7bu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8bu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8bu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2bu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2bu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3bu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3bu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4bu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4bu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2bu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2bu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2bu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2bu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3bu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3bu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4bu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4bu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5bu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5bu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6bu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6bu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7bu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7bu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8bu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8bu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2bu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2bu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3bu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3bu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4bu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4bu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2bu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2bu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2bu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2bu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3bu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3bu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4bu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4bu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5bu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5bu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6bu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6bu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7bu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7bu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8bu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8bu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2bu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2bu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3bu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3bu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4bu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4bu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2bu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2bu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2bu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2bu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3bu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3bu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4bu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4bu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5bu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5bu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6bu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6bu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7bu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7bu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8bu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8bu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2bu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2bu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3bu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3bu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4bu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4bu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2bu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2bu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2h_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2h_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3h_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3h_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4h_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4h_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5h_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5h_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6h_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6h_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7h_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7h_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8h_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8h_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2h_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2h_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3h_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3h_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4h_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4h_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2h_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2h_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2h_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2h_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3h_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3h_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4h_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4h_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5h_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5h_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6h_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6h_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7h_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7h_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8h_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8h_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2h_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2h_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3h_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3h_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4h_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4h_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2h_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2h_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2h_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2h_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3h_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3h_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4h_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4h_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5h_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5h_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6h_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6h_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7h_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7h_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8h_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8h_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2h_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2h_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3h_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3h_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4h_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4h_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2h_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2h_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2h_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2h_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3h_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3h_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4h_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4h_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5h_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5h_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6h_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6h_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7h_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7h_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8h_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8h_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2h_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2h_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3h_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3h_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4h_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4h_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2h_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2h_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2h_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2h_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3h_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3h_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4h_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4h_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5h_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5h_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6h_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6h_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7h_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7h_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8h_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8h_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2h_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2h_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3h_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3h_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4h_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4h_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2h_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2h_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2h_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2h_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3h_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3h_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4h_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4h_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5h_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5h_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6h_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6h_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7h_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7h_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8h_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8h_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2h_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2h_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3h_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3h_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4h_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4h_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2h_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2h_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2h_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2h_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3h_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3h_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4h_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4h_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5h_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5h_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6h_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6h_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7h_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7h_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8h_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8h_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2h_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2h_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3h_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3h_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4h_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4h_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2h_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2h_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2h_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2h_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3h_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3h_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4h_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4h_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5h_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5h_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6h_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6h_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7h_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7h_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8h_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8h_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2h_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2h_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3h_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3h_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4h_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4h_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2h_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2h_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2h_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2h_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3h_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3h_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4h_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4h_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5h_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5h_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6h_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6h_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7h_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7h_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8h_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8h_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2h_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2h_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3h_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3h_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4h_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4h_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2h_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2h_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2h_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2h_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3h_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3h_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4h_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4h_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5h_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5h_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6h_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6h_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7h_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7h_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8h_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8h_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2h_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2h_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3h_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3h_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4h_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4h_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2h_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2h_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2h_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2h_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3h_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3h_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4h_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4h_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5h_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5h_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6h_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6h_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7h_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7h_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8h_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8h_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2h_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2h_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3h_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3h_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4h_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4h_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2h_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2h_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2h_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2h_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3h_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3h_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4h_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4h_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5h_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5h_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6h_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6h_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7h_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7h_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8h_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8h_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2h_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2h_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3h_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3h_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4h_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4h_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2h_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2h_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2h_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2h_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3h_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3h_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4h_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4h_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5h_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5h_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6h_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6h_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7h_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7h_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8h_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8h_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2h_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2h_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3h_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3h_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4h_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4h_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2h_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2h_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2h_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2h_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3h_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3h_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4h_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4h_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5h_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5h_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6h_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6h_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7h_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7h_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8h_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8h_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2h_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2h_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3h_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3h_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4h_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4h_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2h_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2h_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2h_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2h_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3h_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3h_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4h_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4h_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5h_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5h_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6h_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6h_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7h_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7h_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8h_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8h_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2h_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2h_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3h_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3h_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4h_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4h_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2h_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2h_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2h_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2h_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3h_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3h_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4h_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4h_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5h_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5h_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6h_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6h_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7h_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7h_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8h_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8h_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2h_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2h_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3h_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3h_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4h_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4h_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2h_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2h_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2hu_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2hu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3hu_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3hu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4hu_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4hu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5hu_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5hu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6hu_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6hu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7hu_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7hu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8hu_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8hu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2hu_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2hu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3hu_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3hu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4hu_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4hu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2hu_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2hu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2hu_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2hu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3hu_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3hu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4hu_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4hu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5hu_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5hu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6hu_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6hu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7hu_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7hu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8hu_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8hu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2hu_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2hu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3hu_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3hu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4hu_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4hu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2hu_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2hu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2hu_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2hu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3hu_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3hu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4hu_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4hu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5hu_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5hu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6hu_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6hu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7hu_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7hu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8hu_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8hu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2hu_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2hu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3hu_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3hu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4hu_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4hu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2hu_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2hu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2hu_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2hu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3hu_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3hu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4hu_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4hu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5hu_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5hu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6hu_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6hu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7hu_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7hu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8hu_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8hu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2hu_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2hu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3hu_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3hu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4hu_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4hu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2hu_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2hu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2hu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2hu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3hu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3hu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4hu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4hu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5hu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5hu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6hu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6hu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7hu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7hu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8hu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8hu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2hu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2hu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3hu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3hu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4hu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4hu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2hu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2hu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2hu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2hu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3hu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3hu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4hu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4hu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5hu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5hu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6hu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6hu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7hu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7hu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8hu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8hu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2hu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2hu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3hu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3hu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4hu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4hu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2hu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2hu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2hu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2hu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3hu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3hu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4hu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4hu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5hu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5hu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6hu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6hu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7hu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7hu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8hu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8hu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2hu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2hu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3hu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3hu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4hu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4hu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2hu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2hu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2hu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2hu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3hu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3hu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4hu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4hu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5hu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5hu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6hu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6hu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7hu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7hu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8hu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8hu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2hu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2hu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3hu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3hu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4hu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4hu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2hu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2hu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2hu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2hu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3hu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3hu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4hu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4hu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5hu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5hu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6hu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6hu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7hu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7hu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8hu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8hu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2hu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2hu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3hu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3hu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4hu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4hu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2hu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2hu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2hu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2hu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3hu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3hu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4hu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4hu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5hu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5hu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6hu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6hu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7hu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7hu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8hu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8hu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2hu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2hu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3hu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3hu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4hu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4hu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2hu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2hu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2hu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2hu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3hu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3hu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4hu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4hu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5hu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5hu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6hu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6hu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7hu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7hu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8hu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8hu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2hu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2hu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3hu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3hu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4hu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4hu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2hu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2hu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2hu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2hu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3hu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3hu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4hu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4hu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5hu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5hu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6hu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6hu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7hu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7hu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8hu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8hu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2hu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2hu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3hu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3hu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4hu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4hu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2hu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2hu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2hu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2hu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3hu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3hu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4hu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4hu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5hu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5hu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6hu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6hu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7hu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7hu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8hu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8hu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2hu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2hu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3hu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3hu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4hu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4hu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2hu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2hu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2hu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2hu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3hu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3hu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4hu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4hu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5hu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5hu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6hu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6hu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7hu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7hu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8hu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8hu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2hu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2hu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3hu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3hu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4hu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4hu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2hu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2hu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2hu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2hu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3hu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3hu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4hu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4hu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5hu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5hu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6hu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6hu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7hu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7hu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8hu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8hu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2hu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2hu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3hu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3hu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4hu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4hu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2hu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2hu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2hu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2hu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3hu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3hu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4hu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4hu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5hu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5hu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6hu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6hu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7hu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7hu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8hu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8hu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2hu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2hu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3hu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3hu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4hu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4hu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2hu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2hu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2w_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2w_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3w_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3w_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4w_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4w_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5w_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5w_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6w_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6w_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7w_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7w_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8w_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8w_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2w_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2w_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3w_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3w_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4w_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4w_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2w_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2w_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2w_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2w_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3w_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3w_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4w_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4w_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5w_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5w_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6w_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6w_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7w_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7w_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8w_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8w_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2w_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2w_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3w_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3w_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4w_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4w_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2w_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2w_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2w_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2w_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3w_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3w_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4w_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4w_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5w_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5w_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6w_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6w_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7w_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7w_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8w_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8w_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2w_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2w_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3w_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3w_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4w_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4w_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2w_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2w_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2w_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2w_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3w_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3w_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4w_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4w_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5w_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5w_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6w_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6w_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7w_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7w_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8w_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8w_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2w_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2w_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3w_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3w_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4w_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4w_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2w_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2w_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2w_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2w_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3w_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3w_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4w_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4w_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5w_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5w_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6w_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6w_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7w_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7w_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8w_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8w_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2w_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2w_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3w_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3w_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4w_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4w_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2w_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2w_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2w_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2w_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3w_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3w_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4w_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4w_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5w_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5w_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6w_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6w_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7w_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7w_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8w_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8w_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2w_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2w_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3w_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3w_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4w_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4w_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2w_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2w_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2w_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2w_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3w_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3w_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4w_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4w_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5w_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5w_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6w_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6w_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7w_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7w_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8w_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8w_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2w_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2w_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3w_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3w_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4w_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4w_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2w_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2w_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2w_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2w_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3w_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3w_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4w_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4w_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5w_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5w_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6w_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6w_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7w_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7w_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8w_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8w_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2w_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2w_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3w_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3w_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4w_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4w_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2w_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2w_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2w_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2w_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3w_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3w_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4w_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4w_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5w_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5w_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6w_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6w_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7w_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7w_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8w_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8w_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2w_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2w_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3w_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3w_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4w_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4w_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2w_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2w_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2w_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2w_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3w_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3w_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4w_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4w_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5w_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5w_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6w_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6w_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7w_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7w_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8w_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8w_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2w_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2w_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3w_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3w_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4w_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4w_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2w_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2w_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2w_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2w_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3w_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3w_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4w_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4w_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5w_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5w_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6w_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6w_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7w_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7w_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8w_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8w_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2w_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2w_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3w_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3w_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4w_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4w_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2w_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2w_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2w_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2w_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3w_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3w_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4w_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4w_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5w_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5w_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6w_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6w_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7w_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7w_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8w_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8w_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2w_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2w_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3w_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3w_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4w_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4w_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2w_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2w_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2w_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2w_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3w_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3w_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4w_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4w_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5w_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5w_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6w_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6w_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7w_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7w_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8w_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8w_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2w_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2w_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3w_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3w_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4w_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4w_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2w_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2w_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2w_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2w_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3w_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3w_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4w_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4w_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5w_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5w_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6w_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6w_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7w_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7w_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8w_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8w_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2w_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2w_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3w_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3w_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4w_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4w_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2w_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2w_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2w_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2w_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3w_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3w_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4w_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4w_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5w_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5w_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6w_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6w_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7w_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7w_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8w_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8w_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2w_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2w_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3w_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3w_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4w_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4w_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2w_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2w_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2w_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2w_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3w_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3w_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4w_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4w_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5w_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5w_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6w_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6w_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7w_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7w_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8w_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8w_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2w_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2w_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3w_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3w_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4w_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4w_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2w_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2w_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2wu_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2wu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3wu_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3wu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4wu_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4wu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5wu_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5wu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6wu_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6wu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7wu_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7wu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8wu_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8wu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2wu_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2wu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3wu_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3wu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4wu_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4wu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2wu_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2wu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2wu_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2wu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3wu_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3wu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4wu_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4wu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5wu_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5wu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6wu_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6wu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7wu_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7wu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8wu_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8wu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2wu_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2wu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3wu_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3wu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4wu_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4wu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2wu_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2wu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2wu_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2wu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3wu_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3wu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4wu_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4wu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5wu_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5wu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6wu_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6wu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7wu_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7wu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8wu_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8wu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2wu_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2wu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3wu_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3wu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4wu_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4wu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2wu_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2wu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2wu_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2wu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3wu_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3wu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4wu_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4wu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5wu_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5wu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6wu_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6wu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7wu_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7wu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8wu_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8wu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2wu_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2wu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3wu_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3wu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4wu_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4wu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2wu_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2wu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2wu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2wu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3wu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3wu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4wu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4wu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5wu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5wu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6wu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6wu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7wu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7wu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8wu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8wu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2wu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2wu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3wu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3wu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4wu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4wu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2wu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2wu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2wu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2wu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3wu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3wu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4wu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4wu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5wu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5wu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6wu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6wu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7wu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7wu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8wu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8wu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2wu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2wu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3wu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3wu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4wu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4wu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2wu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2wu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2wu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2wu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3wu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3wu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4wu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4wu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5wu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5wu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6wu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6wu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7wu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7wu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8wu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8wu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2wu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2wu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3wu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3wu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4wu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4wu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2wu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2wu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2wu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2wu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3wu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3wu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4wu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4wu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5wu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5wu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6wu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6wu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7wu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7wu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8wu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8wu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2wu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2wu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3wu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3wu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4wu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4wu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2wu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2wu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2wu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2wu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3wu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3wu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4wu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4wu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5wu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5wu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6wu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6wu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7wu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7wu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8wu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8wu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2wu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2wu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3wu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3wu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4wu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4wu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2wu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2wu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2wu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2wu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3wu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3wu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4wu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4wu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5wu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5wu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6wu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6wu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7wu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7wu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8wu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8wu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2wu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2wu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3wu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3wu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4wu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4wu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2wu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2wu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2wu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2wu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3wu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3wu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4wu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4wu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5wu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5wu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6wu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6wu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7wu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7wu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8wu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8wu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2wu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2wu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3wu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3wu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4wu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4wu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2wu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2wu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2wu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2wu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3wu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3wu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4wu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4wu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5wu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5wu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6wu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6wu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7wu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7wu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8wu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8wu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2wu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2wu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3wu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3wu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4wu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4wu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2wu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2wu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
vint8m1x2_t __riscv_th_vlsseg2wu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x2_t __riscv_th_vlsseg2wu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x3_t __riscv_th_vlsseg3wu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x3_t __riscv_th_vlsseg3wu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x4_t __riscv_th_vlsseg4wu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x4_t __riscv_th_vlsseg4wu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x5_t __riscv_th_vlsseg5wu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x5_t __riscv_th_vlsseg5wu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x6_t __riscv_th_vlsseg6wu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x6_t __riscv_th_vlsseg6wu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x7_t __riscv_th_vlsseg7wu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x7_t __riscv_th_vlsseg7wu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, size_t stride, size_t vl);
vint8m1x8_t __riscv_th_vlsseg8wu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, size_t stride, size_t vl);
vuint8m1x8_t __riscv_th_vlsseg8wu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x2_t __riscv_th_vlsseg2wu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x2_t __riscv_th_vlsseg2wu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x3_t __riscv_th_vlsseg3wu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x3_t __riscv_th_vlsseg3wu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, size_t stride, size_t vl);
vint8m2x4_t __riscv_th_vlsseg4wu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, size_t stride, size_t vl);
vuint8m2x4_t __riscv_th_vlsseg4wu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, size_t stride, size_t vl);
vint8m4x2_t __riscv_th_vlsseg2wu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, size_t stride, size_t vl);
vuint8m4x2_t __riscv_th_vlsseg2wu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, size_t stride, size_t vl);
vint16m1x2_t __riscv_th_vlsseg2wu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x2_t __riscv_th_vlsseg2wu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x3_t __riscv_th_vlsseg3wu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x3_t __riscv_th_vlsseg3wu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x4_t __riscv_th_vlsseg4wu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x4_t __riscv_th_vlsseg4wu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x5_t __riscv_th_vlsseg5wu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x5_t __riscv_th_vlsseg5wu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x6_t __riscv_th_vlsseg6wu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x6_t __riscv_th_vlsseg6wu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x7_t __riscv_th_vlsseg7wu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x7_t __riscv_th_vlsseg7wu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, size_t stride, size_t vl);
vint16m1x8_t __riscv_th_vlsseg8wu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, size_t stride, size_t vl);
vuint16m1x8_t __riscv_th_vlsseg8wu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x2_t __riscv_th_vlsseg2wu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x2_t __riscv_th_vlsseg2wu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x3_t __riscv_th_vlsseg3wu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x3_t __riscv_th_vlsseg3wu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, size_t stride, size_t vl);
vint16m2x4_t __riscv_th_vlsseg4wu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, size_t stride, size_t vl);
vuint16m2x4_t __riscv_th_vlsseg4wu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, size_t stride, size_t vl);
vint16m4x2_t __riscv_th_vlsseg2wu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, size_t stride, size_t vl);
vuint16m4x2_t __riscv_th_vlsseg2wu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, size_t stride, size_t vl);
vint32m1x2_t __riscv_th_vlsseg2wu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x2_t __riscv_th_vlsseg2wu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x3_t __riscv_th_vlsseg3wu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x3_t __riscv_th_vlsseg3wu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x4_t __riscv_th_vlsseg4wu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x4_t __riscv_th_vlsseg4wu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x5_t __riscv_th_vlsseg5wu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x5_t __riscv_th_vlsseg5wu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x6_t __riscv_th_vlsseg6wu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x6_t __riscv_th_vlsseg6wu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x7_t __riscv_th_vlsseg7wu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x7_t __riscv_th_vlsseg7wu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, size_t stride, size_t vl);
vint32m1x8_t __riscv_th_vlsseg8wu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, size_t stride, size_t vl);
vuint32m1x8_t __riscv_th_vlsseg8wu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x2_t __riscv_th_vlsseg2wu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x2_t __riscv_th_vlsseg2wu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x3_t __riscv_th_vlsseg3wu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x3_t __riscv_th_vlsseg3wu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, size_t stride, size_t vl);
vint32m2x4_t __riscv_th_vlsseg4wu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, size_t stride, size_t vl);
vuint32m2x4_t __riscv_th_vlsseg4wu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, size_t stride, size_t vl);
vint32m4x2_t __riscv_th_vlsseg2wu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, size_t stride, size_t vl);
vuint32m4x2_t __riscv_th_vlsseg2wu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, size_t stride, size_t vl);
vint64m1x2_t __riscv_th_vlsseg2wu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x2_t __riscv_th_vlsseg2wu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x3_t __riscv_th_vlsseg3wu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x3_t __riscv_th_vlsseg3wu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x4_t __riscv_th_vlsseg4wu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x4_t __riscv_th_vlsseg4wu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x5_t __riscv_th_vlsseg5wu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x5_t __riscv_th_vlsseg5wu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x6_t __riscv_th_vlsseg6wu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x6_t __riscv_th_vlsseg6wu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x7_t __riscv_th_vlsseg7wu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x7_t __riscv_th_vlsseg7wu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, size_t stride, size_t vl);
vint64m1x8_t __riscv_th_vlsseg8wu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, size_t stride, size_t vl);
vuint64m1x8_t __riscv_th_vlsseg8wu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x2_t __riscv_th_vlsseg2wu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x2_t __riscv_th_vlsseg2wu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x3_t __riscv_th_vlsseg3wu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x3_t __riscv_th_vlsseg3wu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, size_t stride, size_t vl);
vint64m2x4_t __riscv_th_vlsseg4wu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, size_t stride, size_t vl);
vuint64m2x4_t __riscv_th_vlsseg4wu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, size_t stride, size_t vl);
vint64m4x2_t __riscv_th_vlsseg2wu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, size_t stride, size_t vl);
vuint64m4x2_t __riscv_th_vlsseg2wu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, size_t stride, size_t vl);
----

[[xtheadvector-strided-segment-store]]
===== XTheadVector Strided Segment Store Intrinsics

[,c]
----
void __riscv_th_vssseg2b_v_i8m1x2 (int8_t *a, size_t stride, vint8m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u8m1x2 (uint8_t *a, size_t stride, vuint8m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i8m1x3 (int8_t *a, size_t stride, vint8m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u8m1x3 (uint8_t *a, size_t stride, vuint8m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i8m1x4 (int8_t *a, size_t stride, vint8m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u8m1x4 (uint8_t *a, size_t stride, vuint8m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i8m1x5 (int8_t *a, size_t stride, vint8m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u8m1x5 (uint8_t *a, size_t stride, vuint8m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i8m1x6 (int8_t *a, size_t stride, vint8m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u8m1x6 (uint8_t *a, size_t stride, vuint8m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i8m1x7 (int8_t *a, size_t stride, vint8m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u8m1x7 (uint8_t *a, size_t stride, vuint8m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i8m1x8 (int8_t *a, size_t stride, vint8m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u8m1x8 (uint8_t *a, size_t stride, vuint8m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i8m2x2 (int8_t *a, size_t stride, vint8m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u8m2x2 (uint8_t *a, size_t stride, vuint8m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i8m2x3 (int8_t *a, size_t stride, vint8m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u8m2x3 (uint8_t *a, size_t stride, vuint8m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i8m2x4 (int8_t *a, size_t stride, vint8m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u8m2x4 (uint8_t *a, size_t stride, vuint8m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i8m4x2 (int8_t *a, size_t stride, vint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u8m4x2 (uint8_t *a, size_t stride, vuint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_i16m1x2 (int16_t *a, size_t stride, vint16m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u16m1x2 (uint16_t *a, size_t stride, vuint16m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i16m1x3 (int16_t *a, size_t stride, vint16m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u16m1x3 (uint16_t *a, size_t stride, vuint16m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i16m1x4 (int16_t *a, size_t stride, vint16m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u16m1x4 (uint16_t *a, size_t stride, vuint16m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i16m1x5 (int16_t *a, size_t stride, vint16m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u16m1x5 (uint16_t *a, size_t stride, vuint16m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i16m1x6 (int16_t *a, size_t stride, vint16m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u16m1x6 (uint16_t *a, size_t stride, vuint16m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i16m1x7 (int16_t *a, size_t stride, vint16m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u16m1x7 (uint16_t *a, size_t stride, vuint16m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i16m1x8 (int16_t *a, size_t stride, vint16m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u16m1x8 (uint16_t *a, size_t stride, vuint16m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i16m2x2 (int16_t *a, size_t stride, vint16m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u16m2x2 (uint16_t *a, size_t stride, vuint16m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i16m2x3 (int16_t *a, size_t stride, vint16m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u16m2x3 (uint16_t *a, size_t stride, vuint16m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i16m2x4 (int16_t *a, size_t stride, vint16m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u16m2x4 (uint16_t *a, size_t stride, vuint16m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i16m4x2 (int16_t *a, size_t stride, vint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u16m4x2 (uint16_t *a, size_t stride, vuint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_i32m1x2 (int32_t *a, size_t stride, vint32m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u32m1x2 (uint32_t *a, size_t stride, vuint32m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i32m1x3 (int32_t *a, size_t stride, vint32m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u32m1x3 (uint32_t *a, size_t stride, vuint32m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i32m1x4 (int32_t *a, size_t stride, vint32m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u32m1x4 (uint32_t *a, size_t stride, vuint32m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i32m1x5 (int32_t *a, size_t stride, vint32m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u32m1x5 (uint32_t *a, size_t stride, vuint32m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i32m1x6 (int32_t *a, size_t stride, vint32m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u32m1x6 (uint32_t *a, size_t stride, vuint32m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i32m1x7 (int32_t *a, size_t stride, vint32m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u32m1x7 (uint32_t *a, size_t stride, vuint32m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i32m1x8 (int32_t *a, size_t stride, vint32m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u32m1x8 (uint32_t *a, size_t stride, vuint32m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i32m2x2 (int32_t *a, size_t stride, vint32m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u32m2x2 (uint32_t *a, size_t stride, vuint32m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i32m2x3 (int32_t *a, size_t stride, vint32m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u32m2x3 (uint32_t *a, size_t stride, vuint32m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i32m2x4 (int32_t *a, size_t stride, vint32m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u32m2x4 (uint32_t *a, size_t stride, vuint32m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i32m4x2 (int32_t *a, size_t stride, vint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u32m4x2 (uint32_t *a, size_t stride, vuint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_i64m1x2 (int64_t *a, size_t stride, vint64m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u64m1x2 (uint64_t *a, size_t stride, vuint64m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i64m1x3 (int64_t *a, size_t stride, vint64m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u64m1x3 (uint64_t *a, size_t stride, vuint64m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i64m1x4 (int64_t *a, size_t stride, vint64m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u64m1x4 (uint64_t *a, size_t stride, vuint64m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i64m1x5 (int64_t *a, size_t stride, vint64m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u64m1x5 (uint64_t *a, size_t stride, vuint64m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i64m1x6 (int64_t *a, size_t stride, vint64m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u64m1x6 (uint64_t *a, size_t stride, vuint64m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i64m1x7 (int64_t *a, size_t stride, vint64m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u64m1x7 (uint64_t *a, size_t stride, vuint64m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i64m1x8 (int64_t *a, size_t stride, vint64m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u64m1x8 (uint64_t *a, size_t stride, vuint64m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i64m2x2 (int64_t *a, size_t stride, vint64m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u64m2x2 (uint64_t *a, size_t stride, vuint64m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i64m2x3 (int64_t *a, size_t stride, vint64m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u64m2x3 (uint64_t *a, size_t stride, vuint64m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i64m2x4 (int64_t *a, size_t stride, vint64m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u64m2x4 (uint64_t *a, size_t stride, vuint64m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i64m4x2 (int64_t *a, size_t stride, vint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u64m4x2 (uint64_t *a, size_t stride, vuint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i8m1x2 (int8_t *a, size_t stride, vint8m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u8m1x2 (uint8_t *a, size_t stride, vuint8m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i8m1x3 (int8_t *a, size_t stride, vint8m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u8m1x3 (uint8_t *a, size_t stride, vuint8m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i8m1x4 (int8_t *a, size_t stride, vint8m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u8m1x4 (uint8_t *a, size_t stride, vuint8m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i8m1x5 (int8_t *a, size_t stride, vint8m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u8m1x5 (uint8_t *a, size_t stride, vuint8m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i8m1x6 (int8_t *a, size_t stride, vint8m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u8m1x6 (uint8_t *a, size_t stride, vuint8m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i8m1x7 (int8_t *a, size_t stride, vint8m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u8m1x7 (uint8_t *a, size_t stride, vuint8m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i8m1x8 (int8_t *a, size_t stride, vint8m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u8m1x8 (uint8_t *a, size_t stride, vuint8m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i8m2x2 (int8_t *a, size_t stride, vint8m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u8m2x2 (uint8_t *a, size_t stride, vuint8m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i8m2x3 (int8_t *a, size_t stride, vint8m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u8m2x3 (uint8_t *a, size_t stride, vuint8m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i8m2x4 (int8_t *a, size_t stride, vint8m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u8m2x4 (uint8_t *a, size_t stride, vuint8m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i8m4x2 (int8_t *a, size_t stride, vint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u8m4x2 (uint8_t *a, size_t stride, vuint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i16m1x2 (int16_t *a, size_t stride, vint16m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u16m1x2 (uint16_t *a, size_t stride, vuint16m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i16m1x3 (int16_t *a, size_t stride, vint16m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u16m1x3 (uint16_t *a, size_t stride, vuint16m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i16m1x4 (int16_t *a, size_t stride, vint16m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u16m1x4 (uint16_t *a, size_t stride, vuint16m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i16m1x5 (int16_t *a, size_t stride, vint16m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u16m1x5 (uint16_t *a, size_t stride, vuint16m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i16m1x6 (int16_t *a, size_t stride, vint16m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u16m1x6 (uint16_t *a, size_t stride, vuint16m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i16m1x7 (int16_t *a, size_t stride, vint16m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u16m1x7 (uint16_t *a, size_t stride, vuint16m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i16m1x8 (int16_t *a, size_t stride, vint16m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u16m1x8 (uint16_t *a, size_t stride, vuint16m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i16m2x2 (int16_t *a, size_t stride, vint16m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u16m2x2 (uint16_t *a, size_t stride, vuint16m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i16m2x3 (int16_t *a, size_t stride, vint16m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u16m2x3 (uint16_t *a, size_t stride, vuint16m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i16m2x4 (int16_t *a, size_t stride, vint16m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u16m2x4 (uint16_t *a, size_t stride, vuint16m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i16m4x2 (int16_t *a, size_t stride, vint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u16m4x2 (uint16_t *a, size_t stride, vuint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i32m1x2 (int32_t *a, size_t stride, vint32m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u32m1x2 (uint32_t *a, size_t stride, vuint32m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i32m1x3 (int32_t *a, size_t stride, vint32m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u32m1x3 (uint32_t *a, size_t stride, vuint32m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i32m1x4 (int32_t *a, size_t stride, vint32m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u32m1x4 (uint32_t *a, size_t stride, vuint32m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i32m1x5 (int32_t *a, size_t stride, vint32m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u32m1x5 (uint32_t *a, size_t stride, vuint32m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i32m1x6 (int32_t *a, size_t stride, vint32m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u32m1x6 (uint32_t *a, size_t stride, vuint32m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i32m1x7 (int32_t *a, size_t stride, vint32m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u32m1x7 (uint32_t *a, size_t stride, vuint32m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i32m1x8 (int32_t *a, size_t stride, vint32m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u32m1x8 (uint32_t *a, size_t stride, vuint32m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i32m2x2 (int32_t *a, size_t stride, vint32m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u32m2x2 (uint32_t *a, size_t stride, vuint32m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i32m2x3 (int32_t *a, size_t stride, vint32m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u32m2x3 (uint32_t *a, size_t stride, vuint32m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i32m2x4 (int32_t *a, size_t stride, vint32m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u32m2x4 (uint32_t *a, size_t stride, vuint32m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i32m4x2 (int32_t *a, size_t stride, vint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u32m4x2 (uint32_t *a, size_t stride, vuint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i64m1x2 (int64_t *a, size_t stride, vint64m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u64m1x2 (uint64_t *a, size_t stride, vuint64m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i64m1x3 (int64_t *a, size_t stride, vint64m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u64m1x3 (uint64_t *a, size_t stride, vuint64m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i64m1x4 (int64_t *a, size_t stride, vint64m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u64m1x4 (uint64_t *a, size_t stride, vuint64m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i64m1x5 (int64_t *a, size_t stride, vint64m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u64m1x5 (uint64_t *a, size_t stride, vuint64m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i64m1x6 (int64_t *a, size_t stride, vint64m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u64m1x6 (uint64_t *a, size_t stride, vuint64m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i64m1x7 (int64_t *a, size_t stride, vint64m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u64m1x7 (uint64_t *a, size_t stride, vuint64m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i64m1x8 (int64_t *a, size_t stride, vint64m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u64m1x8 (uint64_t *a, size_t stride, vuint64m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i64m2x2 (int64_t *a, size_t stride, vint64m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u64m2x2 (uint64_t *a, size_t stride, vuint64m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i64m2x3 (int64_t *a, size_t stride, vint64m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u64m2x3 (uint64_t *a, size_t stride, vuint64m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i64m2x4 (int64_t *a, size_t stride, vint64m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u64m2x4 (uint64_t *a, size_t stride, vuint64m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i64m4x2 (int64_t *a, size_t stride, vint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u64m4x2 (uint64_t *a, size_t stride, vuint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i8m1x2 (int8_t *a, size_t stride, vint8m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u8m1x2 (uint8_t *a, size_t stride, vuint8m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i8m1x3 (int8_t *a, size_t stride, vint8m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u8m1x3 (uint8_t *a, size_t stride, vuint8m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i8m1x4 (int8_t *a, size_t stride, vint8m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u8m1x4 (uint8_t *a, size_t stride, vuint8m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i8m1x5 (int8_t *a, size_t stride, vint8m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u8m1x5 (uint8_t *a, size_t stride, vuint8m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i8m1x6 (int8_t *a, size_t stride, vint8m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u8m1x6 (uint8_t *a, size_t stride, vuint8m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i8m1x7 (int8_t *a, size_t stride, vint8m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u8m1x7 (uint8_t *a, size_t stride, vuint8m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i8m1x8 (int8_t *a, size_t stride, vint8m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u8m1x8 (uint8_t *a, size_t stride, vuint8m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i8m2x2 (int8_t *a, size_t stride, vint8m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u8m2x2 (uint8_t *a, size_t stride, vuint8m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i8m2x3 (int8_t *a, size_t stride, vint8m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u8m2x3 (uint8_t *a, size_t stride, vuint8m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i8m2x4 (int8_t *a, size_t stride, vint8m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u8m2x4 (uint8_t *a, size_t stride, vuint8m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i8m4x2 (int8_t *a, size_t stride, vint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u8m4x2 (uint8_t *a, size_t stride, vuint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i16m1x2 (int16_t *a, size_t stride, vint16m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u16m1x2 (uint16_t *a, size_t stride, vuint16m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i16m1x3 (int16_t *a, size_t stride, vint16m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u16m1x3 (uint16_t *a, size_t stride, vuint16m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i16m1x4 (int16_t *a, size_t stride, vint16m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u16m1x4 (uint16_t *a, size_t stride, vuint16m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i16m1x5 (int16_t *a, size_t stride, vint16m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u16m1x5 (uint16_t *a, size_t stride, vuint16m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i16m1x6 (int16_t *a, size_t stride, vint16m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u16m1x6 (uint16_t *a, size_t stride, vuint16m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i16m1x7 (int16_t *a, size_t stride, vint16m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u16m1x7 (uint16_t *a, size_t stride, vuint16m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i16m1x8 (int16_t *a, size_t stride, vint16m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u16m1x8 (uint16_t *a, size_t stride, vuint16m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i16m2x2 (int16_t *a, size_t stride, vint16m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u16m2x2 (uint16_t *a, size_t stride, vuint16m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i16m2x3 (int16_t *a, size_t stride, vint16m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u16m2x3 (uint16_t *a, size_t stride, vuint16m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i16m2x4 (int16_t *a, size_t stride, vint16m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u16m2x4 (uint16_t *a, size_t stride, vuint16m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i16m4x2 (int16_t *a, size_t stride, vint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u16m4x2 (uint16_t *a, size_t stride, vuint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i32m1x2 (int32_t *a, size_t stride, vint32m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u32m1x2 (uint32_t *a, size_t stride, vuint32m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i32m1x3 (int32_t *a, size_t stride, vint32m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u32m1x3 (uint32_t *a, size_t stride, vuint32m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i32m1x4 (int32_t *a, size_t stride, vint32m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u32m1x4 (uint32_t *a, size_t stride, vuint32m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i32m1x5 (int32_t *a, size_t stride, vint32m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u32m1x5 (uint32_t *a, size_t stride, vuint32m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i32m1x6 (int32_t *a, size_t stride, vint32m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u32m1x6 (uint32_t *a, size_t stride, vuint32m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i32m1x7 (int32_t *a, size_t stride, vint32m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u32m1x7 (uint32_t *a, size_t stride, vuint32m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i32m1x8 (int32_t *a, size_t stride, vint32m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u32m1x8 (uint32_t *a, size_t stride, vuint32m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i32m2x2 (int32_t *a, size_t stride, vint32m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u32m2x2 (uint32_t *a, size_t stride, vuint32m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i32m2x3 (int32_t *a, size_t stride, vint32m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u32m2x3 (uint32_t *a, size_t stride, vuint32m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i32m2x4 (int32_t *a, size_t stride, vint32m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u32m2x4 (uint32_t *a, size_t stride, vuint32m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i32m4x2 (int32_t *a, size_t stride, vint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u32m4x2 (uint32_t *a, size_t stride, vuint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i64m1x2 (int64_t *a, size_t stride, vint64m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u64m1x2 (uint64_t *a, size_t stride, vuint64m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i64m1x3 (int64_t *a, size_t stride, vint64m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u64m1x3 (uint64_t *a, size_t stride, vuint64m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i64m1x4 (int64_t *a, size_t stride, vint64m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u64m1x4 (uint64_t *a, size_t stride, vuint64m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i64m1x5 (int64_t *a, size_t stride, vint64m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u64m1x5 (uint64_t *a, size_t stride, vuint64m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i64m1x6 (int64_t *a, size_t stride, vint64m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u64m1x6 (uint64_t *a, size_t stride, vuint64m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i64m1x7 (int64_t *a, size_t stride, vint64m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u64m1x7 (uint64_t *a, size_t stride, vuint64m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i64m1x8 (int64_t *a, size_t stride, vint64m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u64m1x8 (uint64_t *a, size_t stride, vuint64m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i64m2x2 (int64_t *a, size_t stride, vint64m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u64m2x2 (uint64_t *a, size_t stride, vuint64m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i64m2x3 (int64_t *a, size_t stride, vint64m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u64m2x3 (uint64_t *a, size_t stride, vuint64m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i64m2x4 (int64_t *a, size_t stride, vint64m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u64m2x4 (uint64_t *a, size_t stride, vuint64m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i64m4x2 (int64_t *a, size_t stride, vint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u64m4x2 (uint64_t *a, size_t stride, vuint64m4x2_t b, size_t vl);
// masked functions
void __riscv_th_vssseg2b_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, vint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, vuint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, vint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, vuint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, vint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, vuint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x4_t b, size_t vl);
void __riscv_th_vssseg5b_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x5_t b, size_t vl);
void __riscv_th_vssseg5b_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x5_t b, size_t vl);
void __riscv_th_vssseg6b_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x6_t b, size_t vl);
void __riscv_th_vssseg6b_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x6_t b, size_t vl);
void __riscv_th_vssseg7b_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x7_t b, size_t vl);
void __riscv_th_vssseg7b_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x7_t b, size_t vl);
void __riscv_th_vssseg8b_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x8_t b, size_t vl);
void __riscv_th_vssseg8b_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x8_t b, size_t vl);
void __riscv_th_vssseg2b_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x2_t b, size_t vl);
void __riscv_th_vssseg3b_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x3_t b, size_t vl);
void __riscv_th_vssseg3b_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x3_t b, size_t vl);
void __riscv_th_vssseg4b_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x4_t b, size_t vl);
void __riscv_th_vssseg4b_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x4_t b, size_t vl);
void __riscv_th_vssseg2b_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, vint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2b_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, vuint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, vint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, vuint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, vint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, vuint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, vint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, vuint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x4_t b, size_t vl);
void __riscv_th_vssseg5h_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x5_t b, size_t vl);
void __riscv_th_vssseg5h_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x5_t b, size_t vl);
void __riscv_th_vssseg6h_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x6_t b, size_t vl);
void __riscv_th_vssseg6h_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x6_t b, size_t vl);
void __riscv_th_vssseg7h_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x7_t b, size_t vl);
void __riscv_th_vssseg7h_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x7_t b, size_t vl);
void __riscv_th_vssseg8h_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x8_t b, size_t vl);
void __riscv_th_vssseg8h_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x8_t b, size_t vl);
void __riscv_th_vssseg2h_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x2_t b, size_t vl);
void __riscv_th_vssseg3h_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x3_t b, size_t vl);
void __riscv_th_vssseg3h_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x3_t b, size_t vl);
void __riscv_th_vssseg4h_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x4_t b, size_t vl);
void __riscv_th_vssseg4h_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x4_t b, size_t vl);
void __riscv_th_vssseg2h_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, vint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2h_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, vuint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i8m1x2_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u8m1x2_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i8m1x3_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u8m1x3_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i8m1x4_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u8m1x4_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i8m1x5_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u8m1x5_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i8m1x6_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u8m1x6_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i8m1x7_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u8m1x7_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i8m1x8_m (vbool8_t mask, int8_t *a, size_t stride, vint8m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u8m1x8_m (vbool8_t mask, uint8_t *a, size_t stride, vuint8m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i8m2x2_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u8m2x2_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i8m2x3_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u8m2x3_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i8m2x4_m (vbool4_t mask, int8_t *a, size_t stride, vint8m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u8m2x4_m (vbool4_t mask, uint8_t *a, size_t stride, vuint8m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i8m4x2_m (vbool2_t mask, int8_t *a, size_t stride, vint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u8m4x2_m (vbool2_t mask, uint8_t *a, size_t stride, vuint8m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i16m1x2_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u16m1x2_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i16m1x3_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u16m1x3_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i16m1x4_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u16m1x4_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i16m1x5_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u16m1x5_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i16m1x6_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u16m1x6_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i16m1x7_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u16m1x7_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i16m1x8_m (vbool16_t mask, int16_t *a, size_t stride, vint16m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u16m1x8_m (vbool16_t mask, uint16_t *a, size_t stride, vuint16m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i16m2x2_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u16m2x2_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i16m2x3_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u16m2x3_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i16m2x4_m (vbool8_t mask, int16_t *a, size_t stride, vint16m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u16m2x4_m (vbool8_t mask, uint16_t *a, size_t stride, vuint16m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i16m4x2_m (vbool4_t mask, int16_t *a, size_t stride, vint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u16m4x2_m (vbool4_t mask, uint16_t *a, size_t stride, vuint16m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i32m1x2_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u32m1x2_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i32m1x3_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u32m1x3_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i32m1x4_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u32m1x4_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i32m1x5_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u32m1x5_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i32m1x6_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u32m1x6_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i32m1x7_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u32m1x7_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i32m1x8_m (vbool32_t mask, int32_t *a, size_t stride, vint32m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u32m1x8_m (vbool32_t mask, uint32_t *a, size_t stride, vuint32m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i32m2x2_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u32m2x2_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i32m2x3_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u32m2x3_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i32m2x4_m (vbool16_t mask, int32_t *a, size_t stride, vint32m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u32m2x4_m (vbool16_t mask, uint32_t *a, size_t stride, vuint32m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i32m4x2_m (vbool8_t mask, int32_t *a, size_t stride, vint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u32m4x2_m (vbool8_t mask, uint32_t *a, size_t stride, vuint32m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_i64m1x2_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u64m1x2_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i64m1x3_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u64m1x3_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i64m1x4_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u64m1x4_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x4_t b, size_t vl);
void __riscv_th_vssseg5w_v_i64m1x5_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x5_t b, size_t vl);
void __riscv_th_vssseg5w_v_u64m1x5_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x5_t b, size_t vl);
void __riscv_th_vssseg6w_v_i64m1x6_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x6_t b, size_t vl);
void __riscv_th_vssseg6w_v_u64m1x6_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x6_t b, size_t vl);
void __riscv_th_vssseg7w_v_i64m1x7_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x7_t b, size_t vl);
void __riscv_th_vssseg7w_v_u64m1x7_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x7_t b, size_t vl);
void __riscv_th_vssseg8w_v_i64m1x8_m (vbool64_t mask, int64_t *a, size_t stride, vint64m1x8_t b, size_t vl);
void __riscv_th_vssseg8w_v_u64m1x8_m (vbool64_t mask, uint64_t *a, size_t stride, vuint64m1x8_t b, size_t vl);
void __riscv_th_vssseg2w_v_i64m2x2_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u64m2x2_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x2_t b, size_t vl);
void __riscv_th_vssseg3w_v_i64m2x3_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x3_t b, size_t vl);
void __riscv_th_vssseg3w_v_u64m2x3_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x3_t b, size_t vl);
void __riscv_th_vssseg4w_v_i64m2x4_m (vbool32_t mask, int64_t *a, size_t stride, vint64m2x4_t b, size_t vl);
void __riscv_th_vssseg4w_v_u64m2x4_m (vbool32_t mask, uint64_t *a, size_t stride, vuint64m2x4_t b, size_t vl);
void __riscv_th_vssseg2w_v_i64m4x2_m (vbool16_t mask, int64_t *a, size_t stride, vint64m4x2_t b, size_t vl);
void __riscv_th_vssseg2w_v_u64m4x2_m (vbool16_t mask, uint64_t *a, size_t stride, vuint64m4x2_t b, size_t vl);
----

[[xtheadvector-indexed-segment-load]]
===== XTheadVector Indexed Segment Load Intrinsics

[,c]
----
vint8m1x2_t __riscv_th_vlxseg2b_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2b_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3b_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3b_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4b_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4b_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5b_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5b_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6b_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6b_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7b_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7b_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8b_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8b_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2b_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2b_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3b_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3b_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4b_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4b_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2b_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2b_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2b_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2b_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3b_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3b_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4b_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4b_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5b_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5b_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6b_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6b_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7b_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7b_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8b_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8b_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2b_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2b_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3b_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3b_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4b_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4b_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2b_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2b_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2b_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2b_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3b_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3b_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4b_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4b_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5b_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5b_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6b_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6b_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7b_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7b_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8b_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8b_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2b_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2b_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3b_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3b_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4b_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4b_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2b_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2b_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2b_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2b_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3b_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3b_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4b_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4b_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5b_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5b_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6b_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6b_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7b_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7b_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8b_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8b_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2b_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2b_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3b_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3b_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4b_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4b_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2b_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2b_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2b_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2b_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3b_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3b_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4b_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4b_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5b_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5b_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6b_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6b_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7b_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7b_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8b_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8b_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2b_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2b_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3b_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3b_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4b_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4b_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2b_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2b_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2b_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2b_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3b_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3b_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4b_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4b_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5b_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5b_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6b_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6b_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7b_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7b_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8b_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8b_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2b_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2b_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3b_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3b_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4b_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4b_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2b_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2b_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2b_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2b_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3b_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3b_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4b_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4b_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5b_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5b_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6b_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6b_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7b_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7b_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8b_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8b_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2b_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2b_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3b_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3b_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4b_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4b_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2b_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2b_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2b_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2b_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3b_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3b_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4b_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4b_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5b_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5b_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6b_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6b_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7b_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7b_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8b_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8b_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2b_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2b_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3b_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3b_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4b_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4b_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2b_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2b_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2bu_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2bu_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3bu_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3bu_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4bu_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4bu_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5bu_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5bu_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6bu_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6bu_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7bu_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7bu_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8bu_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8bu_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2bu_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2bu_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3bu_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3bu_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4bu_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4bu_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2bu_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2bu_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2bu_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2bu_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3bu_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3bu_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4bu_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4bu_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5bu_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5bu_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6bu_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6bu_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7bu_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7bu_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8bu_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8bu_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2bu_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2bu_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3bu_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3bu_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4bu_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4bu_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2bu_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2bu_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2bu_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2bu_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3bu_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3bu_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4bu_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4bu_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5bu_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5bu_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6bu_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6bu_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7bu_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7bu_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8bu_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8bu_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2bu_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2bu_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3bu_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3bu_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4bu_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4bu_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2bu_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2bu_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2bu_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2bu_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3bu_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3bu_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4bu_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4bu_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5bu_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5bu_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6bu_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6bu_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7bu_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7bu_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8bu_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8bu_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2bu_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2bu_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3bu_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3bu_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4bu_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4bu_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2bu_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2bu_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2bu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2bu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3bu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3bu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4bu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4bu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5bu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5bu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6bu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6bu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7bu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7bu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8bu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8bu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2bu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2bu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3bu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3bu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4bu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4bu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2bu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2bu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2bu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2bu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3bu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3bu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4bu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4bu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5bu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5bu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6bu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6bu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7bu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7bu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8bu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8bu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2bu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2bu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3bu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3bu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4bu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4bu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2bu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2bu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2bu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2bu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3bu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3bu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4bu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4bu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5bu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5bu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6bu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6bu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7bu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7bu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8bu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8bu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2bu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2bu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3bu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3bu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4bu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4bu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2bu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2bu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2bu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2bu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3bu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3bu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4bu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4bu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5bu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5bu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6bu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6bu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7bu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7bu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8bu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8bu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2bu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2bu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3bu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3bu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4bu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4bu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2bu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2bu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2h_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2h_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3h_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3h_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4h_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4h_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5h_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5h_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6h_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6h_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7h_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7h_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8h_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8h_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2h_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2h_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3h_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3h_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4h_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4h_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2h_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2h_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2h_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2h_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3h_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3h_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4h_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4h_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5h_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5h_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6h_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6h_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7h_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7h_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8h_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8h_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2h_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2h_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3h_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3h_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4h_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4h_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2h_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2h_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2h_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2h_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3h_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3h_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4h_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4h_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5h_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5h_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6h_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6h_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7h_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7h_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8h_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8h_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2h_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2h_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3h_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3h_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4h_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4h_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2h_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2h_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2h_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2h_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3h_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3h_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4h_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4h_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5h_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5h_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6h_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6h_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7h_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7h_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8h_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8h_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2h_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2h_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3h_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3h_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4h_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4h_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2h_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2h_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2h_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2h_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3h_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3h_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4h_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4h_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5h_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5h_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6h_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6h_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7h_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7h_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8h_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8h_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2h_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2h_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3h_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3h_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4h_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4h_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2h_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2h_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2h_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2h_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3h_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3h_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4h_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4h_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5h_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5h_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6h_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6h_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7h_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7h_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8h_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8h_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2h_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2h_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3h_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3h_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4h_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4h_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2h_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2h_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2h_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2h_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3h_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3h_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4h_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4h_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5h_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5h_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6h_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6h_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7h_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7h_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8h_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8h_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2h_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2h_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3h_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3h_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4h_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4h_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2h_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2h_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2h_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2h_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3h_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3h_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4h_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4h_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5h_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5h_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6h_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6h_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7h_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7h_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8h_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8h_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2h_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2h_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3h_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3h_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4h_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4h_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2h_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2h_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2hu_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2hu_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3hu_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3hu_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4hu_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4hu_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5hu_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5hu_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6hu_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6hu_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7hu_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7hu_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8hu_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8hu_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2hu_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2hu_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3hu_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3hu_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4hu_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4hu_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2hu_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2hu_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2hu_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2hu_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3hu_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3hu_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4hu_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4hu_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5hu_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5hu_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6hu_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6hu_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7hu_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7hu_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8hu_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8hu_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2hu_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2hu_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3hu_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3hu_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4hu_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4hu_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2hu_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2hu_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2hu_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2hu_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3hu_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3hu_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4hu_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4hu_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5hu_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5hu_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6hu_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6hu_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7hu_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7hu_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8hu_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8hu_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2hu_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2hu_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3hu_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3hu_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4hu_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4hu_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2hu_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2hu_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2hu_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2hu_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3hu_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3hu_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4hu_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4hu_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5hu_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5hu_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6hu_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6hu_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7hu_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7hu_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8hu_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8hu_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2hu_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2hu_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3hu_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3hu_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4hu_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4hu_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2hu_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2hu_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2hu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2hu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3hu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3hu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4hu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4hu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5hu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5hu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6hu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6hu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7hu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7hu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8hu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8hu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2hu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2hu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3hu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3hu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4hu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4hu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2hu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2hu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2hu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2hu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3hu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3hu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4hu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4hu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5hu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5hu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6hu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6hu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7hu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7hu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8hu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8hu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2hu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2hu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3hu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3hu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4hu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4hu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2hu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2hu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2hu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2hu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3hu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3hu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4hu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4hu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5hu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5hu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6hu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6hu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7hu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7hu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8hu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8hu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2hu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2hu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3hu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3hu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4hu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4hu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2hu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2hu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2hu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2hu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3hu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3hu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4hu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4hu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5hu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5hu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6hu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6hu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7hu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7hu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8hu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8hu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2hu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2hu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3hu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3hu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4hu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4hu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2hu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2hu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2w_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2w_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3w_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3w_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4w_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4w_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5w_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5w_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6w_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6w_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7w_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7w_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8w_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8w_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2w_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2w_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3w_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3w_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4w_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4w_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2w_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2w_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2w_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2w_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3w_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3w_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4w_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4w_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5w_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5w_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6w_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6w_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7w_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7w_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8w_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8w_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2w_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2w_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3w_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3w_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4w_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4w_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2w_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2w_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2w_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2w_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3w_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3w_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4w_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4w_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5w_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5w_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6w_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6w_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7w_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7w_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8w_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8w_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2w_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2w_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3w_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3w_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4w_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4w_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2w_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2w_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2w_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2w_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3w_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3w_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4w_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4w_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5w_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5w_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6w_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6w_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7w_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7w_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8w_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8w_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2w_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2w_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3w_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3w_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4w_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4w_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2w_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2w_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2w_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2w_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3w_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3w_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4w_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4w_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5w_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5w_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6w_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6w_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7w_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7w_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8w_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8w_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2w_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2w_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3w_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3w_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4w_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4w_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2w_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2w_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2w_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2w_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3w_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3w_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4w_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4w_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5w_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5w_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6w_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6w_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7w_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7w_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8w_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8w_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2w_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2w_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3w_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3w_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4w_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4w_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2w_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2w_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2w_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2w_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3w_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3w_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4w_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4w_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5w_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5w_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6w_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6w_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7w_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7w_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8w_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8w_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2w_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2w_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3w_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3w_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4w_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4w_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2w_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2w_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2w_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2w_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3w_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3w_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4w_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4w_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5w_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5w_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6w_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6w_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7w_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7w_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8w_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8w_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2w_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2w_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3w_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3w_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4w_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4w_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2w_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2w_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2wu_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2wu_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3wu_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3wu_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4wu_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4wu_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5wu_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5wu_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6wu_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6wu_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7wu_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7wu_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8wu_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8wu_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2wu_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2wu_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3wu_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3wu_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4wu_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4wu_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2wu_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2wu_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2wu_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2wu_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3wu_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3wu_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4wu_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4wu_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5wu_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5wu_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6wu_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6wu_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7wu_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7wu_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8wu_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8wu_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2wu_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2wu_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3wu_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3wu_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4wu_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4wu_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2wu_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2wu_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2wu_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2wu_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3wu_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3wu_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4wu_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4wu_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5wu_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5wu_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6wu_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6wu_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7wu_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7wu_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8wu_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8wu_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2wu_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2wu_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3wu_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3wu_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4wu_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4wu_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2wu_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2wu_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2wu_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2wu_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3wu_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3wu_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4wu_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4wu_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5wu_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5wu_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6wu_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6wu_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7wu_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7wu_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8wu_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8wu_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2wu_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2wu_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3wu_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3wu_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4wu_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4wu_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2wu_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2wu_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2wu_v_i8m1x2_tu (vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2wu_v_u8m1x2_tu (vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3wu_v_i8m1x3_tu (vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3wu_v_u8m1x3_tu (vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4wu_v_i8m1x4_tu (vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4wu_v_u8m1x4_tu (vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5wu_v_i8m1x5_tu (vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5wu_v_u8m1x5_tu (vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6wu_v_i8m1x6_tu (vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6wu_v_u8m1x6_tu (vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7wu_v_i8m1x7_tu (vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7wu_v_u8m1x7_tu (vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8wu_v_i8m1x8_tu (vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8wu_v_u8m1x8_tu (vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2wu_v_i8m2x2_tu (vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2wu_v_u8m2x2_tu (vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3wu_v_i8m2x3_tu (vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3wu_v_u8m2x3_tu (vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4wu_v_i8m2x4_tu (vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4wu_v_u8m2x4_tu (vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2wu_v_i8m4x2_tu (vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2wu_v_u8m4x2_tu (vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2wu_v_i16m1x2_tu (vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2wu_v_u16m1x2_tu (vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3wu_v_i16m1x3_tu (vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3wu_v_u16m1x3_tu (vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4wu_v_i16m1x4_tu (vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4wu_v_u16m1x4_tu (vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5wu_v_i16m1x5_tu (vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5wu_v_u16m1x5_tu (vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6wu_v_i16m1x6_tu (vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6wu_v_u16m1x6_tu (vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7wu_v_i16m1x7_tu (vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7wu_v_u16m1x7_tu (vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8wu_v_i16m1x8_tu (vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8wu_v_u16m1x8_tu (vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2wu_v_i16m2x2_tu (vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2wu_v_u16m2x2_tu (vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3wu_v_i16m2x3_tu (vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3wu_v_u16m2x3_tu (vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4wu_v_i16m2x4_tu (vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4wu_v_u16m2x4_tu (vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2wu_v_i16m4x2_tu (vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2wu_v_u16m4x2_tu (vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2wu_v_i32m1x2_tu (vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2wu_v_u32m1x2_tu (vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3wu_v_i32m1x3_tu (vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3wu_v_u32m1x3_tu (vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4wu_v_i32m1x4_tu (vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4wu_v_u32m1x4_tu (vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5wu_v_i32m1x5_tu (vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5wu_v_u32m1x5_tu (vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6wu_v_i32m1x6_tu (vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6wu_v_u32m1x6_tu (vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7wu_v_i32m1x7_tu (vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7wu_v_u32m1x7_tu (vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8wu_v_i32m1x8_tu (vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8wu_v_u32m1x8_tu (vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2wu_v_i32m2x2_tu (vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2wu_v_u32m2x2_tu (vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3wu_v_i32m2x3_tu (vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3wu_v_u32m2x3_tu (vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4wu_v_i32m2x4_tu (vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4wu_v_u32m2x4_tu (vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2wu_v_i32m4x2_tu (vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2wu_v_u32m4x2_tu (vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2wu_v_i64m1x2_tu (vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2wu_v_u64m1x2_tu (vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3wu_v_i64m1x3_tu (vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3wu_v_u64m1x3_tu (vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4wu_v_i64m1x4_tu (vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4wu_v_u64m1x4_tu (vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5wu_v_i64m1x5_tu (vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5wu_v_u64m1x5_tu (vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6wu_v_i64m1x6_tu (vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6wu_v_u64m1x6_tu (vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7wu_v_i64m1x7_tu (vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7wu_v_u64m1x7_tu (vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8wu_v_i64m1x8_tu (vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8wu_v_u64m1x8_tu (vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2wu_v_i64m2x2_tu (vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2wu_v_u64m2x2_tu (vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3wu_v_i64m2x3_tu (vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3wu_v_u64m2x3_tu (vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4wu_v_i64m2x4_tu (vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4wu_v_u64m2x4_tu (vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2wu_v_i64m4x2_tu (vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2wu_v_u64m4x2_tu (vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
// masked functions
vint8m1x2_t __riscv_th_vlxseg2b_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2b_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3b_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3b_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4b_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4b_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5b_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5b_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6b_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6b_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7b_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7b_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8b_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8b_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2b_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2b_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3b_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3b_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4b_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4b_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2b_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2b_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2b_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2b_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3b_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3b_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4b_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4b_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5b_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5b_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6b_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6b_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7b_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7b_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8b_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8b_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2b_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2b_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3b_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3b_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4b_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4b_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2b_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2b_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2b_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2b_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3b_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3b_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4b_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4b_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5b_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5b_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6b_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6b_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7b_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7b_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8b_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8b_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2b_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2b_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3b_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3b_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4b_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4b_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2b_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2b_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2b_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2b_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3b_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3b_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4b_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4b_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5b_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5b_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6b_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6b_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7b_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7b_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8b_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8b_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2b_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2b_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3b_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3b_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4b_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4b_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2b_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2b_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2b_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2b_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3b_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3b_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4b_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4b_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5b_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5b_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6b_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6b_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7b_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7b_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8b_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8b_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2b_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2b_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3b_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3b_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4b_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4b_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2b_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2b_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2b_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2b_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3b_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3b_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4b_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4b_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5b_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5b_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6b_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6b_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7b_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7b_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8b_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8b_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2b_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2b_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3b_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3b_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4b_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4b_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2b_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2b_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2b_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2b_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3b_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3b_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4b_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4b_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5b_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5b_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6b_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6b_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7b_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7b_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8b_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8b_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2b_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2b_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3b_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3b_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4b_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4b_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2b_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2b_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2b_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2b_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3b_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3b_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4b_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4b_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5b_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5b_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6b_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6b_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7b_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7b_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8b_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8b_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2b_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2b_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3b_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3b_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4b_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4b_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2b_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2b_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2b_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2b_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3b_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3b_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4b_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4b_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5b_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5b_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6b_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6b_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7b_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7b_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8b_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8b_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2b_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2b_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3b_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3b_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4b_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4b_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2b_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2b_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2b_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2b_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3b_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3b_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4b_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4b_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5b_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5b_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6b_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6b_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7b_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7b_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8b_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8b_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2b_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2b_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3b_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3b_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4b_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4b_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2b_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2b_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2b_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2b_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3b_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3b_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4b_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4b_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5b_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5b_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6b_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6b_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7b_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7b_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8b_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8b_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2b_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2b_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3b_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3b_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4b_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4b_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2b_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2b_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2b_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2b_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3b_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3b_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4b_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4b_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5b_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5b_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6b_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6b_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7b_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7b_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8b_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8b_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2b_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2b_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3b_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3b_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4b_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4b_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2b_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2b_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2b_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2b_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3b_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3b_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4b_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4b_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5b_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5b_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6b_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6b_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7b_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7b_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8b_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8b_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2b_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2b_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3b_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3b_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4b_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4b_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2b_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2b_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2b_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2b_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3b_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3b_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4b_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4b_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5b_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5b_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6b_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6b_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7b_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7b_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8b_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8b_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2b_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2b_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3b_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3b_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4b_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4b_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2b_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2b_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2b_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2b_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3b_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3b_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4b_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4b_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5b_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5b_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6b_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6b_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7b_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7b_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8b_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8b_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2b_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2b_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3b_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3b_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4b_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4b_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2b_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2b_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2b_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2b_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3b_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3b_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4b_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4b_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5b_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5b_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6b_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6b_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7b_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7b_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8b_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8b_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2b_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2b_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3b_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3b_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4b_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4b_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2b_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2b_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2bu_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2bu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3bu_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3bu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4bu_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4bu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5bu_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5bu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6bu_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6bu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7bu_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7bu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8bu_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8bu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2bu_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2bu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3bu_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3bu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4bu_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4bu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2bu_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2bu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2bu_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2bu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3bu_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3bu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4bu_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4bu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5bu_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5bu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6bu_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6bu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7bu_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7bu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8bu_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8bu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2bu_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2bu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3bu_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3bu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4bu_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4bu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2bu_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2bu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2bu_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2bu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3bu_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3bu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4bu_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4bu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5bu_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5bu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6bu_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6bu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7bu_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7bu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8bu_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8bu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2bu_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2bu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3bu_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3bu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4bu_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4bu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2bu_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2bu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2bu_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2bu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3bu_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3bu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4bu_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4bu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5bu_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5bu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6bu_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6bu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7bu_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7bu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8bu_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8bu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2bu_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2bu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3bu_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3bu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4bu_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4bu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2bu_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2bu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2bu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2bu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3bu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3bu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4bu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4bu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5bu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5bu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6bu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6bu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7bu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7bu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8bu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8bu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2bu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2bu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3bu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3bu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4bu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4bu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2bu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2bu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2bu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2bu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3bu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3bu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4bu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4bu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5bu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5bu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6bu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6bu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7bu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7bu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8bu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8bu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2bu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2bu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3bu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3bu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4bu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4bu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2bu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2bu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2bu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2bu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3bu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3bu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4bu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4bu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5bu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5bu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6bu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6bu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7bu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7bu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8bu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8bu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2bu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2bu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3bu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3bu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4bu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4bu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2bu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2bu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2bu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2bu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3bu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3bu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4bu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4bu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5bu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5bu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6bu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6bu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7bu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7bu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8bu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8bu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2bu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2bu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3bu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3bu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4bu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4bu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2bu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2bu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2bu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2bu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3bu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3bu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4bu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4bu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5bu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5bu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6bu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6bu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7bu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7bu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8bu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8bu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2bu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2bu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3bu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3bu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4bu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4bu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2bu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2bu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2bu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2bu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3bu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3bu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4bu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4bu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5bu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5bu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6bu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6bu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7bu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7bu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8bu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8bu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2bu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2bu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3bu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3bu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4bu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4bu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2bu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2bu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2bu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2bu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3bu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3bu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4bu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4bu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5bu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5bu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6bu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6bu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7bu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7bu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8bu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8bu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2bu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2bu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3bu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3bu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4bu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4bu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2bu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2bu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2bu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2bu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3bu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3bu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4bu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4bu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5bu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5bu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6bu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6bu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7bu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7bu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8bu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8bu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2bu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2bu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3bu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3bu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4bu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4bu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2bu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2bu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2bu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2bu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3bu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3bu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4bu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4bu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5bu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5bu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6bu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6bu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7bu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7bu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8bu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8bu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2bu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2bu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3bu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3bu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4bu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4bu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2bu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2bu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2bu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2bu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3bu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3bu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4bu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4bu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5bu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5bu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6bu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6bu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7bu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7bu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8bu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8bu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2bu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2bu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3bu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3bu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4bu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4bu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2bu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2bu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2bu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2bu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3bu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3bu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4bu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4bu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5bu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5bu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6bu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6bu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7bu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7bu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8bu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8bu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2bu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2bu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3bu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3bu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4bu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4bu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2bu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2bu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2bu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2bu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3bu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3bu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4bu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4bu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5bu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5bu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6bu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6bu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7bu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7bu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8bu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8bu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2bu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2bu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3bu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3bu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4bu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4bu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2bu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2bu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2h_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2h_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3h_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3h_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4h_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4h_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5h_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5h_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6h_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6h_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7h_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7h_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8h_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8h_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2h_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2h_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3h_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3h_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4h_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4h_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2h_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2h_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2h_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2h_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3h_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3h_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4h_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4h_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5h_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5h_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6h_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6h_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7h_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7h_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8h_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8h_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2h_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2h_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3h_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3h_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4h_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4h_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2h_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2h_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2h_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2h_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3h_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3h_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4h_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4h_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5h_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5h_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6h_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6h_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7h_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7h_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8h_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8h_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2h_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2h_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3h_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3h_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4h_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4h_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2h_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2h_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2h_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2h_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3h_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3h_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4h_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4h_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5h_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5h_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6h_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6h_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7h_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7h_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8h_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8h_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2h_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2h_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3h_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3h_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4h_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4h_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2h_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2h_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2h_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2h_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3h_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3h_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4h_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4h_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5h_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5h_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6h_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6h_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7h_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7h_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8h_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8h_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2h_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2h_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3h_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3h_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4h_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4h_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2h_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2h_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2h_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2h_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3h_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3h_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4h_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4h_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5h_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5h_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6h_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6h_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7h_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7h_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8h_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8h_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2h_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2h_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3h_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3h_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4h_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4h_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2h_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2h_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2h_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2h_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3h_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3h_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4h_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4h_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5h_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5h_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6h_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6h_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7h_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7h_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8h_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8h_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2h_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2h_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3h_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3h_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4h_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4h_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2h_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2h_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2h_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2h_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3h_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3h_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4h_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4h_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5h_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5h_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6h_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6h_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7h_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7h_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8h_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8h_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2h_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2h_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3h_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3h_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4h_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4h_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2h_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2h_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2h_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2h_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3h_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3h_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4h_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4h_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5h_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5h_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6h_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6h_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7h_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7h_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8h_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8h_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2h_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2h_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3h_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3h_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4h_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4h_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2h_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2h_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2h_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2h_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3h_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3h_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4h_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4h_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5h_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5h_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6h_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6h_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7h_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7h_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8h_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8h_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2h_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2h_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3h_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3h_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4h_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4h_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2h_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2h_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2h_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2h_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3h_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3h_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4h_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4h_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5h_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5h_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6h_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6h_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7h_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7h_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8h_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8h_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2h_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2h_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3h_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3h_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4h_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4h_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2h_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2h_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2h_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2h_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3h_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3h_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4h_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4h_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5h_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5h_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6h_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6h_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7h_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7h_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8h_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8h_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2h_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2h_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3h_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3h_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4h_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4h_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2h_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2h_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2h_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2h_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3h_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3h_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4h_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4h_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5h_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5h_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6h_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6h_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7h_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7h_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8h_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8h_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2h_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2h_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3h_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3h_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4h_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4h_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2h_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2h_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2h_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2h_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3h_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3h_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4h_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4h_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5h_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5h_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6h_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6h_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7h_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7h_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8h_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8h_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2h_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2h_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3h_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3h_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4h_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4h_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2h_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2h_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2h_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2h_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3h_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3h_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4h_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4h_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5h_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5h_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6h_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6h_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7h_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7h_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8h_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8h_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2h_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2h_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3h_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3h_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4h_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4h_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2h_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2h_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2h_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2h_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3h_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3h_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4h_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4h_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5h_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5h_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6h_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6h_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7h_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7h_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8h_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8h_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2h_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2h_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3h_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3h_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4h_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4h_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2h_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2h_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2hu_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2hu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3hu_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3hu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4hu_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4hu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5hu_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5hu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6hu_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6hu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7hu_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7hu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8hu_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8hu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2hu_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2hu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3hu_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3hu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4hu_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4hu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2hu_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2hu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2hu_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2hu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3hu_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3hu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4hu_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4hu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5hu_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5hu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6hu_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6hu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7hu_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7hu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8hu_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8hu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2hu_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2hu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3hu_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3hu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4hu_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4hu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2hu_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2hu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2hu_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2hu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3hu_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3hu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4hu_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4hu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5hu_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5hu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6hu_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6hu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7hu_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7hu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8hu_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8hu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2hu_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2hu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3hu_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3hu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4hu_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4hu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2hu_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2hu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2hu_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2hu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3hu_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3hu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4hu_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4hu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5hu_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5hu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6hu_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6hu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7hu_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7hu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8hu_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8hu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2hu_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2hu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3hu_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3hu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4hu_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4hu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2hu_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2hu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2hu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2hu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3hu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3hu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4hu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4hu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5hu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5hu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6hu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6hu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7hu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7hu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8hu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8hu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2hu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2hu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3hu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3hu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4hu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4hu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2hu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2hu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2hu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2hu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3hu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3hu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4hu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4hu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5hu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5hu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6hu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6hu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7hu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7hu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8hu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8hu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2hu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2hu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3hu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3hu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4hu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4hu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2hu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2hu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2hu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2hu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3hu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3hu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4hu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4hu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5hu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5hu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6hu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6hu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7hu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7hu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8hu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8hu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2hu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2hu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3hu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3hu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4hu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4hu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2hu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2hu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2hu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2hu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3hu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3hu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4hu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4hu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5hu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5hu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6hu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6hu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7hu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7hu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8hu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8hu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2hu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2hu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3hu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3hu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4hu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4hu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2hu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2hu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2hu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2hu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3hu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3hu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4hu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4hu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5hu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5hu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6hu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6hu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7hu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7hu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8hu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8hu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2hu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2hu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3hu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3hu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4hu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4hu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2hu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2hu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2hu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2hu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3hu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3hu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4hu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4hu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5hu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5hu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6hu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6hu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7hu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7hu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8hu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8hu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2hu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2hu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3hu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3hu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4hu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4hu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2hu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2hu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2hu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2hu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3hu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3hu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4hu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4hu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5hu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5hu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6hu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6hu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7hu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7hu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8hu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8hu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2hu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2hu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3hu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3hu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4hu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4hu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2hu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2hu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2hu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2hu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3hu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3hu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4hu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4hu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5hu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5hu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6hu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6hu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7hu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7hu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8hu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8hu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2hu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2hu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3hu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3hu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4hu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4hu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2hu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2hu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2hu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2hu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3hu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3hu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4hu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4hu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5hu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5hu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6hu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6hu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7hu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7hu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8hu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8hu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2hu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2hu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3hu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3hu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4hu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4hu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2hu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2hu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2hu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2hu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3hu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3hu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4hu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4hu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5hu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5hu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6hu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6hu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7hu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7hu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8hu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8hu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2hu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2hu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3hu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3hu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4hu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4hu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2hu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2hu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2hu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2hu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3hu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3hu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4hu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4hu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5hu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5hu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6hu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6hu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7hu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7hu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8hu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8hu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2hu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2hu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3hu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3hu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4hu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4hu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2hu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2hu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2hu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2hu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3hu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3hu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4hu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4hu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5hu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5hu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6hu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6hu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7hu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7hu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8hu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8hu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2hu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2hu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3hu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3hu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4hu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4hu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2hu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2hu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2w_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2w_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3w_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3w_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4w_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4w_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5w_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5w_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6w_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6w_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7w_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7w_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8w_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8w_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2w_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2w_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3w_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3w_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4w_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4w_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2w_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2w_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2w_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2w_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3w_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3w_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4w_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4w_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5w_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5w_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6w_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6w_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7w_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7w_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8w_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8w_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2w_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2w_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3w_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3w_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4w_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4w_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2w_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2w_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2w_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2w_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3w_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3w_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4w_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4w_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5w_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5w_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6w_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6w_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7w_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7w_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8w_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8w_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2w_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2w_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3w_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3w_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4w_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4w_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2w_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2w_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2w_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2w_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3w_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3w_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4w_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4w_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5w_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5w_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6w_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6w_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7w_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7w_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8w_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8w_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2w_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2w_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3w_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3w_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4w_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4w_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2w_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2w_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2w_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2w_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3w_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3w_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4w_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4w_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5w_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5w_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6w_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6w_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7w_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7w_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8w_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8w_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2w_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2w_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3w_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3w_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4w_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4w_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2w_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2w_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2w_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2w_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3w_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3w_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4w_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4w_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5w_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5w_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6w_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6w_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7w_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7w_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8w_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8w_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2w_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2w_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3w_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3w_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4w_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4w_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2w_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2w_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2w_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2w_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3w_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3w_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4w_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4w_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5w_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5w_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6w_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6w_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7w_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7w_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8w_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8w_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2w_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2w_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3w_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3w_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4w_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4w_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2w_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2w_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2w_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2w_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3w_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3w_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4w_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4w_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5w_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5w_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6w_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6w_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7w_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7w_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8w_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8w_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2w_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2w_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3w_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3w_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4w_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4w_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2w_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2w_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2w_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2w_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3w_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3w_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4w_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4w_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5w_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5w_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6w_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6w_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7w_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7w_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8w_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8w_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2w_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2w_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3w_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3w_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4w_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4w_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2w_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2w_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2w_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2w_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3w_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3w_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4w_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4w_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5w_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5w_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6w_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6w_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7w_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7w_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8w_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8w_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2w_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2w_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3w_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3w_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4w_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4w_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2w_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2w_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2w_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2w_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3w_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3w_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4w_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4w_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5w_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5w_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6w_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6w_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7w_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7w_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8w_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8w_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2w_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2w_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3w_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3w_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4w_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4w_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2w_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2w_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2w_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2w_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3w_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3w_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4w_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4w_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5w_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5w_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6w_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6w_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7w_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7w_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8w_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8w_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2w_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2w_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3w_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3w_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4w_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4w_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2w_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2w_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2w_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2w_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3w_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3w_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4w_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4w_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5w_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5w_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6w_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6w_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7w_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7w_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8w_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8w_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2w_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2w_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3w_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3w_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4w_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4w_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2w_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2w_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2w_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2w_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3w_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3w_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4w_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4w_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5w_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5w_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6w_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6w_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7w_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7w_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8w_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8w_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2w_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2w_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3w_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3w_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4w_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4w_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2w_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2w_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2w_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2w_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3w_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3w_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4w_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4w_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5w_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5w_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6w_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6w_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7w_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7w_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8w_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8w_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2w_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2w_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3w_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3w_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4w_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4w_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2w_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2w_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2w_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2w_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3w_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3w_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4w_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4w_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5w_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5w_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6w_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6w_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7w_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7w_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8w_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8w_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2w_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2w_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3w_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3w_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4w_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4w_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2w_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2w_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2wu_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2wu_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3wu_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3wu_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4wu_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4wu_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5wu_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5wu_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6wu_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6wu_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7wu_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7wu_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8wu_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8wu_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2wu_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2wu_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3wu_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3wu_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4wu_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4wu_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2wu_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2wu_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2wu_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2wu_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3wu_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3wu_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4wu_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4wu_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5wu_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5wu_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6wu_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6wu_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7wu_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7wu_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8wu_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8wu_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2wu_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2wu_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3wu_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3wu_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4wu_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4wu_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2wu_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2wu_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2wu_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2wu_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3wu_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3wu_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4wu_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4wu_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5wu_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5wu_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6wu_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6wu_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7wu_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7wu_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8wu_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8wu_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2wu_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2wu_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3wu_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3wu_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4wu_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4wu_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2wu_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2wu_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2wu_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2wu_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3wu_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3wu_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4wu_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4wu_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5wu_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5wu_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6wu_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6wu_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7wu_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7wu_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8wu_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8wu_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2wu_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2wu_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3wu_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3wu_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4wu_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4wu_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2wu_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2wu_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2wu_v_i8m1x2_tum (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2wu_v_u8m1x2_tum (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3wu_v_i8m1x3_tum (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3wu_v_u8m1x3_tum (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4wu_v_i8m1x4_tum (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4wu_v_u8m1x4_tum (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5wu_v_i8m1x5_tum (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5wu_v_u8m1x5_tum (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6wu_v_i8m1x6_tum (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6wu_v_u8m1x6_tum (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7wu_v_i8m1x7_tum (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7wu_v_u8m1x7_tum (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8wu_v_i8m1x8_tum (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8wu_v_u8m1x8_tum (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2wu_v_i8m2x2_tum (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2wu_v_u8m2x2_tum (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3wu_v_i8m2x3_tum (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3wu_v_u8m2x3_tum (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4wu_v_i8m2x4_tum (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4wu_v_u8m2x4_tum (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2wu_v_i8m4x2_tum (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2wu_v_u8m4x2_tum (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2wu_v_i16m1x2_tum (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2wu_v_u16m1x2_tum (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3wu_v_i16m1x3_tum (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3wu_v_u16m1x3_tum (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4wu_v_i16m1x4_tum (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4wu_v_u16m1x4_tum (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5wu_v_i16m1x5_tum (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5wu_v_u16m1x5_tum (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6wu_v_i16m1x6_tum (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6wu_v_u16m1x6_tum (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7wu_v_i16m1x7_tum (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7wu_v_u16m1x7_tum (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8wu_v_i16m1x8_tum (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8wu_v_u16m1x8_tum (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2wu_v_i16m2x2_tum (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2wu_v_u16m2x2_tum (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3wu_v_i16m2x3_tum (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3wu_v_u16m2x3_tum (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4wu_v_i16m2x4_tum (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4wu_v_u16m2x4_tum (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2wu_v_i16m4x2_tum (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2wu_v_u16m4x2_tum (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2wu_v_i32m1x2_tum (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2wu_v_u32m1x2_tum (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3wu_v_i32m1x3_tum (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3wu_v_u32m1x3_tum (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4wu_v_i32m1x4_tum (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4wu_v_u32m1x4_tum (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5wu_v_i32m1x5_tum (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5wu_v_u32m1x5_tum (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6wu_v_i32m1x6_tum (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6wu_v_u32m1x6_tum (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7wu_v_i32m1x7_tum (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7wu_v_u32m1x7_tum (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8wu_v_i32m1x8_tum (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8wu_v_u32m1x8_tum (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2wu_v_i32m2x2_tum (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2wu_v_u32m2x2_tum (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3wu_v_i32m2x3_tum (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3wu_v_u32m2x3_tum (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4wu_v_i32m2x4_tum (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4wu_v_u32m2x4_tum (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2wu_v_i32m4x2_tum (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2wu_v_u32m4x2_tum (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2wu_v_i64m1x2_tum (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2wu_v_u64m1x2_tum (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3wu_v_i64m1x3_tum (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3wu_v_u64m1x3_tum (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4wu_v_i64m1x4_tum (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4wu_v_u64m1x4_tum (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5wu_v_i64m1x5_tum (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5wu_v_u64m1x5_tum (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6wu_v_i64m1x6_tum (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6wu_v_u64m1x6_tum (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7wu_v_i64m1x7_tum (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7wu_v_u64m1x7_tum (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8wu_v_i64m1x8_tum (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8wu_v_u64m1x8_tum (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2wu_v_i64m2x2_tum (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2wu_v_u64m2x2_tum (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3wu_v_i64m2x3_tum (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3wu_v_u64m2x3_tum (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4wu_v_i64m2x4_tum (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4wu_v_u64m2x4_tum (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2wu_v_i64m4x2_tum (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2wu_v_u64m4x2_tum (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2wu_v_i8m1x2_tumu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2wu_v_u8m1x2_tumu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3wu_v_i8m1x3_tumu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3wu_v_u8m1x3_tumu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4wu_v_i8m1x4_tumu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4wu_v_u8m1x4_tumu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5wu_v_i8m1x5_tumu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5wu_v_u8m1x5_tumu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6wu_v_i8m1x6_tumu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6wu_v_u8m1x6_tumu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7wu_v_i8m1x7_tumu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7wu_v_u8m1x7_tumu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8wu_v_i8m1x8_tumu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8wu_v_u8m1x8_tumu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2wu_v_i8m2x2_tumu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2wu_v_u8m2x2_tumu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3wu_v_i8m2x3_tumu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3wu_v_u8m2x3_tumu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4wu_v_i8m2x4_tumu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4wu_v_u8m2x4_tumu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2wu_v_i8m4x2_tumu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2wu_v_u8m4x2_tumu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2wu_v_i16m1x2_tumu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2wu_v_u16m1x2_tumu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3wu_v_i16m1x3_tumu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3wu_v_u16m1x3_tumu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4wu_v_i16m1x4_tumu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4wu_v_u16m1x4_tumu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5wu_v_i16m1x5_tumu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5wu_v_u16m1x5_tumu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6wu_v_i16m1x6_tumu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6wu_v_u16m1x6_tumu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7wu_v_i16m1x7_tumu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7wu_v_u16m1x7_tumu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8wu_v_i16m1x8_tumu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8wu_v_u16m1x8_tumu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2wu_v_i16m2x2_tumu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2wu_v_u16m2x2_tumu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3wu_v_i16m2x3_tumu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3wu_v_u16m2x3_tumu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4wu_v_i16m2x4_tumu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4wu_v_u16m2x4_tumu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2wu_v_i16m4x2_tumu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2wu_v_u16m4x2_tumu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2wu_v_i32m1x2_tumu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2wu_v_u32m1x2_tumu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3wu_v_i32m1x3_tumu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3wu_v_u32m1x3_tumu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4wu_v_i32m1x4_tumu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4wu_v_u32m1x4_tumu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5wu_v_i32m1x5_tumu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5wu_v_u32m1x5_tumu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6wu_v_i32m1x6_tumu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6wu_v_u32m1x6_tumu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7wu_v_i32m1x7_tumu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7wu_v_u32m1x7_tumu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8wu_v_i32m1x8_tumu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8wu_v_u32m1x8_tumu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2wu_v_i32m2x2_tumu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2wu_v_u32m2x2_tumu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3wu_v_i32m2x3_tumu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3wu_v_u32m2x3_tumu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4wu_v_i32m2x4_tumu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4wu_v_u32m2x4_tumu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2wu_v_i32m4x2_tumu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2wu_v_u32m4x2_tumu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2wu_v_i64m1x2_tumu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2wu_v_u64m1x2_tumu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3wu_v_i64m1x3_tumu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3wu_v_u64m1x3_tumu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4wu_v_i64m1x4_tumu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4wu_v_u64m1x4_tumu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5wu_v_i64m1x5_tumu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5wu_v_u64m1x5_tumu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6wu_v_i64m1x6_tumu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6wu_v_u64m1x6_tumu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7wu_v_i64m1x7_tumu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7wu_v_u64m1x7_tumu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8wu_v_i64m1x8_tumu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8wu_v_u64m1x8_tumu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2wu_v_i64m2x2_tumu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2wu_v_u64m2x2_tumu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3wu_v_i64m2x3_tumu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3wu_v_u64m2x3_tumu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4wu_v_i64m2x4_tumu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4wu_v_u64m2x4_tumu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2wu_v_i64m4x2_tumu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2wu_v_u64m4x2_tumu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
vint8m1x2_t __riscv_th_vlxseg2wu_v_i8m1x2_mu (vbool8_t mask, vint8m1x2_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x2_t __riscv_th_vlxseg2wu_v_u8m1x2_mu (vbool8_t mask, vuint8m1x2_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x3_t __riscv_th_vlxseg3wu_v_i8m1x3_mu (vbool8_t mask, vint8m1x3_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x3_t __riscv_th_vlxseg3wu_v_u8m1x3_mu (vbool8_t mask, vuint8m1x3_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x4_t __riscv_th_vlxseg4wu_v_i8m1x4_mu (vbool8_t mask, vint8m1x4_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x4_t __riscv_th_vlxseg4wu_v_u8m1x4_mu (vbool8_t mask, vuint8m1x4_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x5_t __riscv_th_vlxseg5wu_v_i8m1x5_mu (vbool8_t mask, vint8m1x5_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x5_t __riscv_th_vlxseg5wu_v_u8m1x5_mu (vbool8_t mask, vuint8m1x5_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x6_t __riscv_th_vlxseg6wu_v_i8m1x6_mu (vbool8_t mask, vint8m1x6_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x6_t __riscv_th_vlxseg6wu_v_u8m1x6_mu (vbool8_t mask, vuint8m1x6_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x7_t __riscv_th_vlxseg7wu_v_i8m1x7_mu (vbool8_t mask, vint8m1x7_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x7_t __riscv_th_vlxseg7wu_v_u8m1x7_mu (vbool8_t mask, vuint8m1x7_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m1x8_t __riscv_th_vlxseg8wu_v_i8m1x8_mu (vbool8_t mask, vint8m1x8_t a, int8_t *b, vuint8m1_t indexed, size_t vl);
vuint8m1x8_t __riscv_th_vlxseg8wu_v_u8m1x8_mu (vbool8_t mask, vuint8m1x8_t a, uint8_t *b, vuint8m1_t indexed, size_t vl);
vint8m2x2_t __riscv_th_vlxseg2wu_v_i8m2x2_mu (vbool4_t mask, vint8m2x2_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x2_t __riscv_th_vlxseg2wu_v_u8m2x2_mu (vbool4_t mask, vuint8m2x2_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x3_t __riscv_th_vlxseg3wu_v_i8m2x3_mu (vbool4_t mask, vint8m2x3_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x3_t __riscv_th_vlxseg3wu_v_u8m2x3_mu (vbool4_t mask, vuint8m2x3_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m2x4_t __riscv_th_vlxseg4wu_v_i8m2x4_mu (vbool4_t mask, vint8m2x4_t a, int8_t *b, vuint8m2_t indexed, size_t vl);
vuint8m2x4_t __riscv_th_vlxseg4wu_v_u8m2x4_mu (vbool4_t mask, vuint8m2x4_t a, uint8_t *b, vuint8m2_t indexed, size_t vl);
vint8m4x2_t __riscv_th_vlxseg2wu_v_i8m4x2_mu (vbool2_t mask, vint8m4x2_t a, int8_t *b, vuint8m4_t indexed, size_t vl);
vuint8m4x2_t __riscv_th_vlxseg2wu_v_u8m4x2_mu (vbool2_t mask, vuint8m4x2_t a, uint8_t *b, vuint8m4_t indexed, size_t vl);
vint16m1x2_t __riscv_th_vlxseg2wu_v_i16m1x2_mu (vbool16_t mask, vint16m1x2_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x2_t __riscv_th_vlxseg2wu_v_u16m1x2_mu (vbool16_t mask, vuint16m1x2_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x3_t __riscv_th_vlxseg3wu_v_i16m1x3_mu (vbool16_t mask, vint16m1x3_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x3_t __riscv_th_vlxseg3wu_v_u16m1x3_mu (vbool16_t mask, vuint16m1x3_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x4_t __riscv_th_vlxseg4wu_v_i16m1x4_mu (vbool16_t mask, vint16m1x4_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x4_t __riscv_th_vlxseg4wu_v_u16m1x4_mu (vbool16_t mask, vuint16m1x4_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x5_t __riscv_th_vlxseg5wu_v_i16m1x5_mu (vbool16_t mask, vint16m1x5_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x5_t __riscv_th_vlxseg5wu_v_u16m1x5_mu (vbool16_t mask, vuint16m1x5_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x6_t __riscv_th_vlxseg6wu_v_i16m1x6_mu (vbool16_t mask, vint16m1x6_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x6_t __riscv_th_vlxseg6wu_v_u16m1x6_mu (vbool16_t mask, vuint16m1x6_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x7_t __riscv_th_vlxseg7wu_v_i16m1x7_mu (vbool16_t mask, vint16m1x7_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x7_t __riscv_th_vlxseg7wu_v_u16m1x7_mu (vbool16_t mask, vuint16m1x7_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m1x8_t __riscv_th_vlxseg8wu_v_i16m1x8_mu (vbool16_t mask, vint16m1x8_t a, int16_t *b, vuint16m1_t indexed, size_t vl);
vuint16m1x8_t __riscv_th_vlxseg8wu_v_u16m1x8_mu (vbool16_t mask, vuint16m1x8_t a, uint16_t *b, vuint16m1_t indexed, size_t vl);
vint16m2x2_t __riscv_th_vlxseg2wu_v_i16m2x2_mu (vbool8_t mask, vint16m2x2_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x2_t __riscv_th_vlxseg2wu_v_u16m2x2_mu (vbool8_t mask, vuint16m2x2_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x3_t __riscv_th_vlxseg3wu_v_i16m2x3_mu (vbool8_t mask, vint16m2x3_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x3_t __riscv_th_vlxseg3wu_v_u16m2x3_mu (vbool8_t mask, vuint16m2x3_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m2x4_t __riscv_th_vlxseg4wu_v_i16m2x4_mu (vbool8_t mask, vint16m2x4_t a, int16_t *b, vuint16m2_t indexed, size_t vl);
vuint16m2x4_t __riscv_th_vlxseg4wu_v_u16m2x4_mu (vbool8_t mask, vuint16m2x4_t a, uint16_t *b, vuint16m2_t indexed, size_t vl);
vint16m4x2_t __riscv_th_vlxseg2wu_v_i16m4x2_mu (vbool4_t mask, vint16m4x2_t a, int16_t *b, vuint16m4_t indexed, size_t vl);
vuint16m4x2_t __riscv_th_vlxseg2wu_v_u16m4x2_mu (vbool4_t mask, vuint16m4x2_t a, uint16_t *b, vuint16m4_t indexed, size_t vl);
vint32m1x2_t __riscv_th_vlxseg2wu_v_i32m1x2_mu (vbool32_t mask, vint32m1x2_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x2_t __riscv_th_vlxseg2wu_v_u32m1x2_mu (vbool32_t mask, vuint32m1x2_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x3_t __riscv_th_vlxseg3wu_v_i32m1x3_mu (vbool32_t mask, vint32m1x3_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x3_t __riscv_th_vlxseg3wu_v_u32m1x3_mu (vbool32_t mask, vuint32m1x3_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x4_t __riscv_th_vlxseg4wu_v_i32m1x4_mu (vbool32_t mask, vint32m1x4_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x4_t __riscv_th_vlxseg4wu_v_u32m1x4_mu (vbool32_t mask, vuint32m1x4_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x5_t __riscv_th_vlxseg5wu_v_i32m1x5_mu (vbool32_t mask, vint32m1x5_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x5_t __riscv_th_vlxseg5wu_v_u32m1x5_mu (vbool32_t mask, vuint32m1x5_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x6_t __riscv_th_vlxseg6wu_v_i32m1x6_mu (vbool32_t mask, vint32m1x6_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x6_t __riscv_th_vlxseg6wu_v_u32m1x6_mu (vbool32_t mask, vuint32m1x6_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x7_t __riscv_th_vlxseg7wu_v_i32m1x7_mu (vbool32_t mask, vint32m1x7_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x7_t __riscv_th_vlxseg7wu_v_u32m1x7_mu (vbool32_t mask, vuint32m1x7_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m1x8_t __riscv_th_vlxseg8wu_v_i32m1x8_mu (vbool32_t mask, vint32m1x8_t a, int32_t *b, vuint32m1_t indexed, size_t vl);
vuint32m1x8_t __riscv_th_vlxseg8wu_v_u32m1x8_mu (vbool32_t mask, vuint32m1x8_t a, uint32_t *b, vuint32m1_t indexed, size_t vl);
vint32m2x2_t __riscv_th_vlxseg2wu_v_i32m2x2_mu (vbool16_t mask, vint32m2x2_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x2_t __riscv_th_vlxseg2wu_v_u32m2x2_mu (vbool16_t mask, vuint32m2x2_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x3_t __riscv_th_vlxseg3wu_v_i32m2x3_mu (vbool16_t mask, vint32m2x3_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x3_t __riscv_th_vlxseg3wu_v_u32m2x3_mu (vbool16_t mask, vuint32m2x3_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m2x4_t __riscv_th_vlxseg4wu_v_i32m2x4_mu (vbool16_t mask, vint32m2x4_t a, int32_t *b, vuint32m2_t indexed, size_t vl);
vuint32m2x4_t __riscv_th_vlxseg4wu_v_u32m2x4_mu (vbool16_t mask, vuint32m2x4_t a, uint32_t *b, vuint32m2_t indexed, size_t vl);
vint32m4x2_t __riscv_th_vlxseg2wu_v_i32m4x2_mu (vbool8_t mask, vint32m4x2_t a, int32_t *b, vuint32m4_t indexed, size_t vl);
vuint32m4x2_t __riscv_th_vlxseg2wu_v_u32m4x2_mu (vbool8_t mask, vuint32m4x2_t a, uint32_t *b, vuint32m4_t indexed, size_t vl);
vint64m1x2_t __riscv_th_vlxseg2wu_v_i64m1x2_mu (vbool64_t mask, vint64m1x2_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x2_t __riscv_th_vlxseg2wu_v_u64m1x2_mu (vbool64_t mask, vuint64m1x2_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x3_t __riscv_th_vlxseg3wu_v_i64m1x3_mu (vbool64_t mask, vint64m1x3_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x3_t __riscv_th_vlxseg3wu_v_u64m1x3_mu (vbool64_t mask, vuint64m1x3_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x4_t __riscv_th_vlxseg4wu_v_i64m1x4_mu (vbool64_t mask, vint64m1x4_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x4_t __riscv_th_vlxseg4wu_v_u64m1x4_mu (vbool64_t mask, vuint64m1x4_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x5_t __riscv_th_vlxseg5wu_v_i64m1x5_mu (vbool64_t mask, vint64m1x5_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x5_t __riscv_th_vlxseg5wu_v_u64m1x5_mu (vbool64_t mask, vuint64m1x5_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x6_t __riscv_th_vlxseg6wu_v_i64m1x6_mu (vbool64_t mask, vint64m1x6_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x6_t __riscv_th_vlxseg6wu_v_u64m1x6_mu (vbool64_t mask, vuint64m1x6_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x7_t __riscv_th_vlxseg7wu_v_i64m1x7_mu (vbool64_t mask, vint64m1x7_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x7_t __riscv_th_vlxseg7wu_v_u64m1x7_mu (vbool64_t mask, vuint64m1x7_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m1x8_t __riscv_th_vlxseg8wu_v_i64m1x8_mu (vbool64_t mask, vint64m1x8_t a, int64_t *b, vuint64m1_t indexed, size_t vl);
vuint64m1x8_t __riscv_th_vlxseg8wu_v_u64m1x8_mu (vbool64_t mask, vuint64m1x8_t a, uint64_t *b, vuint64m1_t indexed, size_t vl);
vint64m2x2_t __riscv_th_vlxseg2wu_v_i64m2x2_mu (vbool32_t mask, vint64m2x2_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x2_t __riscv_th_vlxseg2wu_v_u64m2x2_mu (vbool32_t mask, vuint64m2x2_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x3_t __riscv_th_vlxseg3wu_v_i64m2x3_mu (vbool32_t mask, vint64m2x3_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x3_t __riscv_th_vlxseg3wu_v_u64m2x3_mu (vbool32_t mask, vuint64m2x3_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m2x4_t __riscv_th_vlxseg4wu_v_i64m2x4_mu (vbool32_t mask, vint64m2x4_t a, int64_t *b, vuint64m2_t indexed, size_t vl);
vuint64m2x4_t __riscv_th_vlxseg4wu_v_u64m2x4_mu (vbool32_t mask, vuint64m2x4_t a, uint64_t *b, vuint64m2_t indexed, size_t vl);
vint64m4x2_t __riscv_th_vlxseg2wu_v_i64m4x2_mu (vbool16_t mask, vint64m4x2_t a, int64_t *b, vuint64m4_t indexed, size_t vl);
vuint64m4x2_t __riscv_th_vlxseg2wu_v_u64m4x2_mu (vbool16_t mask, vuint64m4x2_t a, uint64_t *b, vuint64m4_t indexed, size_t vl);
----

[[xtheadvector-indexed-segment-store]]
===== XTheadVector Indexed Segment Store Intrinsics

[,c]
----
void __riscv_th_vsxseg2b_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, vint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, vint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, vint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, vint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, vint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, vint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, vint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, vint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, vint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, vint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, vint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, vint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, vint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, vint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, vint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, vint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, vint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, vint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, vint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, vint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, vint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, vint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, vint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, vint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, vint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, vint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, vint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, vint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, vint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, vint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, vint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, vint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, vint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, vint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, vint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, vint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, vint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, vint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, vint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, vint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, vint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, vint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, vint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, vint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, vint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, vint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, vint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, vint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, vint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, vint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, vint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, vint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, vint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, vint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, vint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, vint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, vint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, vint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, vint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, vint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, vint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, vint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, vint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, vint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, vint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, vint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, vint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, vint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, vint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, vint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, vint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, vint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, vint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, vint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, vint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, vint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, vint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, vint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, vint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, vint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, vint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, vint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, vint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, vint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, vint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, vint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, vint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, vint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i8m1x2 (int8_t *a, vuint8m1_t indexed, vint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u8m1x2 (uint8_t *a, vuint8m1_t indexed, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i8m1x3 (int8_t *a, vuint8m1_t indexed, vint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u8m1x3 (uint8_t *a, vuint8m1_t indexed, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i8m1x4 (int8_t *a, vuint8m1_t indexed, vint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u8m1x4 (uint8_t *a, vuint8m1_t indexed, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i8m1x5 (int8_t *a, vuint8m1_t indexed, vint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u8m1x5 (uint8_t *a, vuint8m1_t indexed, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i8m1x6 (int8_t *a, vuint8m1_t indexed, vint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u8m1x6 (uint8_t *a, vuint8m1_t indexed, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i8m1x7 (int8_t *a, vuint8m1_t indexed, vint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u8m1x7 (uint8_t *a, vuint8m1_t indexed, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i8m1x8 (int8_t *a, vuint8m1_t indexed, vint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u8m1x8 (uint8_t *a, vuint8m1_t indexed, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i8m2x2 (int8_t *a, vuint8m2_t indexed, vint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u8m2x2 (uint8_t *a, vuint8m2_t indexed, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i8m2x3 (int8_t *a, vuint8m2_t indexed, vint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u8m2x3 (uint8_t *a, vuint8m2_t indexed, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i8m2x4 (int8_t *a, vuint8m2_t indexed, vint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u8m2x4 (uint8_t *a, vuint8m2_t indexed, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i8m4x2 (int8_t *a, vuint8m4_t indexed, vint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u8m4x2 (uint8_t *a, vuint8m4_t indexed, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i16m1x2 (int16_t *a, vuint16m1_t indexed, vint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u16m1x2 (uint16_t *a, vuint16m1_t indexed, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i16m1x3 (int16_t *a, vuint16m1_t indexed, vint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u16m1x3 (uint16_t *a, vuint16m1_t indexed, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i16m1x4 (int16_t *a, vuint16m1_t indexed, vint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u16m1x4 (uint16_t *a, vuint16m1_t indexed, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i16m1x5 (int16_t *a, vuint16m1_t indexed, vint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u16m1x5 (uint16_t *a, vuint16m1_t indexed, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i16m1x6 (int16_t *a, vuint16m1_t indexed, vint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u16m1x6 (uint16_t *a, vuint16m1_t indexed, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i16m1x7 (int16_t *a, vuint16m1_t indexed, vint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u16m1x7 (uint16_t *a, vuint16m1_t indexed, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i16m1x8 (int16_t *a, vuint16m1_t indexed, vint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u16m1x8 (uint16_t *a, vuint16m1_t indexed, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i16m2x2 (int16_t *a, vuint16m2_t indexed, vint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u16m2x2 (uint16_t *a, vuint16m2_t indexed, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i16m2x3 (int16_t *a, vuint16m2_t indexed, vint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u16m2x3 (uint16_t *a, vuint16m2_t indexed, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i16m2x4 (int16_t *a, vuint16m2_t indexed, vint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u16m2x4 (uint16_t *a, vuint16m2_t indexed, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i16m4x2 (int16_t *a, vuint16m4_t indexed, vint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u16m4x2 (uint16_t *a, vuint16m4_t indexed, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i32m1x2 (int32_t *a, vuint32m1_t indexed, vint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u32m1x2 (uint32_t *a, vuint32m1_t indexed, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i32m1x3 (int32_t *a, vuint32m1_t indexed, vint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u32m1x3 (uint32_t *a, vuint32m1_t indexed, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i32m1x4 (int32_t *a, vuint32m1_t indexed, vint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u32m1x4 (uint32_t *a, vuint32m1_t indexed, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i32m1x5 (int32_t *a, vuint32m1_t indexed, vint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u32m1x5 (uint32_t *a, vuint32m1_t indexed, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i32m1x6 (int32_t *a, vuint32m1_t indexed, vint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u32m1x6 (uint32_t *a, vuint32m1_t indexed, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i32m1x7 (int32_t *a, vuint32m1_t indexed, vint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u32m1x7 (uint32_t *a, vuint32m1_t indexed, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i32m1x8 (int32_t *a, vuint32m1_t indexed, vint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u32m1x8 (uint32_t *a, vuint32m1_t indexed, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i32m2x2 (int32_t *a, vuint32m2_t indexed, vint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u32m2x2 (uint32_t *a, vuint32m2_t indexed, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i32m2x3 (int32_t *a, vuint32m2_t indexed, vint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u32m2x3 (uint32_t *a, vuint32m2_t indexed, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i32m2x4 (int32_t *a, vuint32m2_t indexed, vint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u32m2x4 (uint32_t *a, vuint32m2_t indexed, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i32m4x2 (int32_t *a, vuint32m4_t indexed, vint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u32m4x2 (uint32_t *a, vuint32m4_t indexed, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i64m1x2 (int64_t *a, vuint64m1_t indexed, vint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u64m1x2 (uint64_t *a, vuint64m1_t indexed, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i64m1x3 (int64_t *a, vuint64m1_t indexed, vint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u64m1x3 (uint64_t *a, vuint64m1_t indexed, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i64m1x4 (int64_t *a, vuint64m1_t indexed, vint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u64m1x4 (uint64_t *a, vuint64m1_t indexed, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i64m1x5 (int64_t *a, vuint64m1_t indexed, vint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u64m1x5 (uint64_t *a, vuint64m1_t indexed, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i64m1x6 (int64_t *a, vuint64m1_t indexed, vint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u64m1x6 (uint64_t *a, vuint64m1_t indexed, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i64m1x7 (int64_t *a, vuint64m1_t indexed, vint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u64m1x7 (uint64_t *a, vuint64m1_t indexed, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i64m1x8 (int64_t *a, vuint64m1_t indexed, vint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u64m1x8 (uint64_t *a, vuint64m1_t indexed, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i64m2x2 (int64_t *a, vuint64m2_t indexed, vint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u64m2x2 (uint64_t *a, vuint64m2_t indexed, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i64m2x3 (int64_t *a, vuint64m2_t indexed, vint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u64m2x3 (uint64_t *a, vuint64m2_t indexed, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i64m2x4 (int64_t *a, vuint64m2_t indexed, vint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u64m2x4 (uint64_t *a, vuint64m2_t indexed, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i64m4x2 (int64_t *a, vuint64m4_t indexed, vint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u64m4x2 (uint64_t *a, vuint64m4_t indexed, vuint64m4x2_t b, size_t vl);
// masked functions
void __riscv_th_vsxseg2b_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, vint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg5b_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg5b_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg6b_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg6b_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg7b_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg7b_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg8b_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg8b_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg3b_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg3b_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg4b_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg4b_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg2b_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, vint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2b_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, vint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg5h_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg5h_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg6h_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg6h_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg7h_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg7h_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg8h_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg8h_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg3h_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg3h_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg4h_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg4h_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg2h_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, vint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2h_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, vuint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i8m1x2_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u8m1x2_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i8m1x3_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u8m1x3_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i8m1x4_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u8m1x4_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i8m1x5_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u8m1x5_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i8m1x6_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u8m1x6_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i8m1x7_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u8m1x7_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i8m1x8_m (vbool8_t mask, int8_t *a, vuint8m1_t indexed, vint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u8m1x8_m (vbool8_t mask, uint8_t *a, vuint8m1_t indexed, vuint8m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i8m2x2_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u8m2x2_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i8m2x3_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u8m2x3_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i8m2x4_m (vbool4_t mask, int8_t *a, vuint8m2_t indexed, vint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u8m2x4_m (vbool4_t mask, uint8_t *a, vuint8m2_t indexed, vuint8m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i8m4x2_m (vbool2_t mask, int8_t *a, vuint8m4_t indexed, vint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u8m4x2_m (vbool2_t mask, uint8_t *a, vuint8m4_t indexed, vuint8m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i16m1x2_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u16m1x2_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i16m1x3_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u16m1x3_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i16m1x4_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u16m1x4_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i16m1x5_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u16m1x5_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i16m1x6_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u16m1x6_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i16m1x7_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u16m1x7_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i16m1x8_m (vbool16_t mask, int16_t *a, vuint16m1_t indexed, vint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u16m1x8_m (vbool16_t mask, uint16_t *a, vuint16m1_t indexed, vuint16m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i16m2x2_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u16m2x2_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i16m2x3_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u16m2x3_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i16m2x4_m (vbool8_t mask, int16_t *a, vuint16m2_t indexed, vint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u16m2x4_m (vbool8_t mask, uint16_t *a, vuint16m2_t indexed, vuint16m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i16m4x2_m (vbool4_t mask, int16_t *a, vuint16m4_t indexed, vint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u16m4x2_m (vbool4_t mask, uint16_t *a, vuint16m4_t indexed, vuint16m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i32m1x2_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u32m1x2_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i32m1x3_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u32m1x3_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i32m1x4_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u32m1x4_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i32m1x5_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u32m1x5_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i32m1x6_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u32m1x6_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i32m1x7_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u32m1x7_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i32m1x8_m (vbool32_t mask, int32_t *a, vuint32m1_t indexed, vint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u32m1x8_m (vbool32_t mask, uint32_t *a, vuint32m1_t indexed, vuint32m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i32m2x2_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u32m2x2_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i32m2x3_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u32m2x3_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i32m2x4_m (vbool16_t mask, int32_t *a, vuint32m2_t indexed, vint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u32m2x4_m (vbool16_t mask, uint32_t *a, vuint32m2_t indexed, vuint32m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i32m4x2_m (vbool8_t mask, int32_t *a, vuint32m4_t indexed, vint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u32m4x2_m (vbool8_t mask, uint32_t *a, vuint32m4_t indexed, vuint32m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i64m1x2_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u64m1x2_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i64m1x3_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u64m1x3_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i64m1x4_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u64m1x4_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x4_t b, size_t vl);
void __riscv_th_vsxseg5w_v_i64m1x5_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg5w_v_u64m1x5_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x5_t b, size_t vl);
void __riscv_th_vsxseg6w_v_i64m1x6_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg6w_v_u64m1x6_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x6_t b, size_t vl);
void __riscv_th_vsxseg7w_v_i64m1x7_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg7w_v_u64m1x7_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x7_t b, size_t vl);
void __riscv_th_vsxseg8w_v_i64m1x8_m (vbool64_t mask, int64_t *a, vuint64m1_t indexed, vint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg8w_v_u64m1x8_m (vbool64_t mask, uint64_t *a, vuint64m1_t indexed, vuint64m1x8_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i64m2x2_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u64m2x2_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x2_t b, size_t vl);
void __riscv_th_vsxseg3w_v_i64m2x3_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg3w_v_u64m2x3_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x3_t b, size_t vl);
void __riscv_th_vsxseg4w_v_i64m2x4_m (vbool32_t mask, int64_t *a, vuint64m2_t indexed, vint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg4w_v_u64m2x4_m (vbool32_t mask, uint64_t *a, vuint64m2_t indexed, vuint64m2x4_t b, size_t vl);
void __riscv_th_vsxseg2w_v_i64m4x2_m (vbool16_t mask, int64_t *a, vuint64m4_t indexed, vint64m4x2_t b, size_t vl);
void __riscv_th_vsxseg2w_v_u64m4x2_m (vbool16_t mask, uint64_t *a, vuint64m4_t indexed, vuint64m4x2_t b, size_t vl);
